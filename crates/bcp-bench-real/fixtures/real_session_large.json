{
  "blocks": [
    {
      "content": "[package]\nname = \"bcp-bench-real\"\nversion = \"0.1.0\"\nedition = \"2024\"\npublish = false\n\n# Real-world token benchmark harness for BCP.\n# Measures actual BPE token savings using tiktoken-rs (cl100k_base)\n# against realistic markdown baselines.\n\n[[bin]]\nname = \"bench_tokens\"\npath = \"src/bin/bench_tokens.rs\"\n\n[[bin]]\nname = \"capture_session\"\npath = \"src/bin/capture_session.rs\"\n\n[[bench]]\nname = \"tiktoken\"\nharness = false\n\n[dependencies]\nbcp-encoder = { path = \"../bcp-encoder\" }\nbcp-decoder = { path = \"../bcp-decoder\" }\nbcp-driver  = { path = \"../bcp-driver\" }\nbcp-types   = { path = \"../bcp-types\" }\nbcp-wire    = { path = \"../bcp-wire\" }\ntiktoken-rs = \"0.6\"\nserde       = { version = \"1\", features = [\"derive\"] }\nserde_json  = { workspace = true }\nanyhow      = { workspace = true }\nclap        = { workspace = true }\ncriterion   = { workspace = true }\n",
      "language": "toml",
      "path": "bcp-bench-real/Cargo.toml",
      "type": "code"
    },
    {
      "content": "use std::path::Path;\n\nuse bcp_bench_real::fixture::encode_fixture;\nuse bcp_bench_real::markdown::{build_naive_markdown, build_realistic_markdown};\nuse bcp_bench_real::token_counter::TokenCounter;\nuse bcp_decoder::BcpDecoder;\nuse bcp_driver::{BcpDriver, DefaultDriver, DriverConfig, OutputMode};\nuse criterion::{BenchmarkId, Criterion, criterion_group, criterion_main};\n\nfn bench_token_counting(c: &mut Criterion) {\n    let counter = TokenCounter::new().unwrap();\n    let fixture_path = find_fixture(\"real_session_medium.json\");\n    let payload = encode_fixture(&fixture_path).unwrap();\n    let decoded = BcpDecoder::decode(&payload).unwrap();\n    let config = DriverConfig {\n        mode: OutputMode::Minimal,\n        ..Default::default()\n    };\n    let rendered = DefaultDriver.render(&decoded.blocks, &config).unwrap();\n\n    c.bench_function(\"tiktoken_count_medium\", |b| {\n        b.iter(|| counter.count(&rendered));\n    });\n}\n\nfn bench_full_pipeline(c: &mut Criterion) {\n    let mut group = c.benchmark_group(\"full_pipeline\");\n\n    for fixture_name in [\n        \"real_session_small.json\",\n        \"real_session_medium.json\",\n        \"real_session_large.json\",\n    ] {\n        let path = find_fixture(fixture_name);\n        if !path.exists() {\n            continue;\n        }\n        group.bench_with_input(\n            BenchmarkId::from_parameter(fixture_name),\n            &path,\n            |b, path| {\n                b.iter(|| {\n                    let payload = encode_fixture(path).unwrap();\n                    let decoded = BcpDecoder::decode(&payload).unwrap();\n                    let config = DriverConfig {\n                        mode: OutputMode::Minimal,\n                        ..Default::default()\n                    };\n                    let rendered = DefaultDriver.render(&decoded.blocks, &config).unwrap();\n                    let counter = TokenCounter::new().unwrap();\n                    counter.count(&rendered)\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\nfn bench_savings_by_mode(c: &mut Criterion) {\n    let counter = TokenCounter::new().unwrap();\n    let fixture_path = find_fixture(\"real_session_medium.json\");\n    let payload = encode_fixture(&fixture_path).unwrap();\n    let decoded = BcpDecoder::decode(&payload).unwrap();\n\n    let mut group = c.benchmark_group(\"savings_by_mode\");\n\n    for (label, mode) in [\n        (\"xml\", OutputMode::Xml),\n        (\"markdown\", OutputMode::Markdown),\n        (\"minimal\", OutputMode::Minimal),\n    ] {\n        let config = DriverConfig {\n            mode,\n            ..Default::default()\n        };\n        let rendered = DefaultDriver.render(&decoded.blocks, &config).unwrap();\n\n        group.bench_with_input(BenchmarkId::from_parameter(label), &rendered, |b, text| {\n            b.iter(|| counter.count(text));\n        });\n    }\n\n    // Also bench the baseline markdown construction\n    let naive = build_naive_markdown(&decoded.blocks);\n    group.bench_with_input(\n        BenchmarkId::from_parameter(\"naive_md\"),\n        &naive,\n        |b, text| {\n            b.iter(|| counter.count(text));\n        },\n    );\n\n    let realistic = build_realistic_markdown(&decoded.blocks);\n    group.bench_with_input(\n        BenchmarkId::from_parameter(\"agent_md\"),\n        &realistic,\n        |b, text| {\n            b.iter(|| counter.count(text));\n        },\n    );\n\n    group.finish();\n}\n\nfn find_fixture(name: &str) -> std::path::PathBuf {\n    let crate_dir = Path::new(env!(\"CARGO_MANIFEST_DIR\"));\n    crate_dir.join(\"fixtures\").join(name)\n}\n\ncriterion_group!(\n    benches,\n    bench_token_counting,\n    bench_full_pipeline,\n    bench_savings_by_mode\n);\ncriterion_main!(benches);\n",
      "language": "rust",
      "path": "bcp-bench-real/benches/tiktoken.rs",
      "type": "code"
    },
    {
      "content": "{\n  \"blocks\": [\n    {\n      \"content\": \"/// TLV field serializer for block bodies.\\n///\\n/// `BlockWriter` accumulates tag-length-value encoded fields into an\\n/// internal byte buffer. It mirrors the field encoding convention from\\n/// `bcp_types::fields` but wraps it in a stateful builder that tracks\\n/// the buffer and provides a clean `finish()` hand-off.\\n///\\n/// This is an internal implementation detail of the encoder — it is\\n/// not part of the public API. Each block type's serialization calls\\n/// into `BlockWriter` to produce the body bytes that get framed by\\n/// [`BlockFrame`](bcp_wire::block_frame::BlockFrame).\\n///\\n/// Wire format per field:\\n///\\n/// ```text\\n/// ┌─────────────────┬──────────────────┬────────────────────────┐\\n/// │ field_id (varint)│ wire_type (varint)│ payload (varies)      │\\n/// ├─────────────────┼──────────────────┼────────────────────────┤\\n/// │                 │ 0 (Varint)       │ value (varint)         │\\n/// │                 │ 1 (Bytes)        │ length (varint) + data │\\n/// │                 │ 2 (Nested)       │ length (varint) + data │\\n/// └─────────────────┴──────────────────┴────────────────────────┘\\n/// ```\\npub struct BlockWriter {\\n    buf: Vec<u8>,\\n}\\n\\nimpl BlockWriter {\\n    /// Create a new writer with an empty buffer.\\n    #[must_use]\\n    pub fn new() -> Self {\\n        Self { buf: Vec::new() }\\n    }\\n\\n    /// Create a new writer with a pre-allocated buffer capacity.\\n    ///\\n    /// Use this when you can estimate the final body size to avoid\\n    /// intermediate reallocations.\\n    #[must_use]\\n    pub fn with_capacity(capacity: usize) -> Self {\\n        Self {\\n            buf: Vec::with_capacity(capacity),\\n        }\\n    }\\n\\n    /// Write a varint field (wire type 0).\\n    ///\\n    /// Encodes: `field_id (varint) | 0 (varint) | value (varint)`\\n    pub fn write_varint_field(&mut self, field_id: u64, value: u64) {\\n        bcp_types::fields::encode_varint_field(&mut self.buf, field_id, value);\\n    }\\n\\n    /// Write a bytes field (wire type 1).\\n    ///\\n    /// Encodes: `field_id (varint) | 1 (varint) | length (varint) | data [length]`\\n    ///\\n    /// Strings are encoded as bytes fields with UTF-8 content — there is\\n    /// no distinct string wire type.\\n    pub fn write_bytes_field(&mut self, field_id: u64, value: &[u8]) {\\n        bcp_types::fields::encode_bytes_field(&mut self.buf, field_id, value);\\n    }\\n\\n    /// Write a nested field (wire type 2).\\n    ///\\n    /// Encodes: `field_id (varint) | 2 (varint) | length (varint) | nested [length]`\\n    ///\\n    /// The `nested` bytes are themselves a sequence of TLV-encoded fields,\\n    /// pre-serialized by the caller. This enables recursive structures like\\n    /// `FileEntry` children and `DiffHunk` sequences.\\n    pub fn write_nested_field(&mut self, field_id: u64, nested: &[u8]) {\\n        bcp_types::fields::encode_nested_field(&mut self.buf, field_id, nested);\\n    }\\n\\n    /// Consume the writer and return the accumulated bytes.\\n    ///\\n    /// After calling `finish()`, the writer is consumed. The returned\\n    /// `Vec<u8>` is the complete TLV-encoded body ready to be wrapped\\n    /// in a `BlockFrame`.\\n    #[must_use]\\n    pub fn finish(self) -> Vec<u8> {\\n        self.buf\\n    }\\n}\\n\\nimpl Default for BlockWriter {\\n    fn default() -> Self {\\n        Self::new()\\n    }\\n}\\n\\n#[cfg(test)]\\nmod tests {\\n    use super::*;\\n\\n    #[test]\\n    fn empty_writer_produces_empty_bytes() {\\n        let writer = BlockWriter::new();\\n        assert!(writer.finish().is_empty());\\n    }\\n\\n    #[test]\\n    fn single_varint_field() {\\n        let mut writer = BlockWriter::new();\\n        writer.write_varint_field(1, 42);\\n        let bytes = writer.finish();\\n        assert!(!bytes.is_empty());\\n\\n        // Verify via bcp-types decode path\\n        let (header, n) = bcp_types::fields::decode_field_header(&bytes).unwrap();\\n        assert_eq!(header.field_id, 1);\\n        assert_eq!(header.wire_type, bcp_types::fields::FieldWireType::Varint);\\n        let (val, m) = bcp_types::fields::decode_varint_value(&bytes[n..]).unwrap();\\n        assert_eq!(val, 42);\\n        assert_eq!(n + m, bytes.len());\\n    }\\n\\n    #[test]\\n    fn single_bytes_field() {\\n        let mut writer = BlockWriter::new();\\n        writer.write_bytes_field(2, b\\\"hello\\\");\\n        let bytes = writer.finish();\\n\\n        let (header, n) = bcp_types::fields::decode_field_header(&bytes).unwrap();\\n        assert_eq!(header.field_id, 2);\\n        assert_eq!(header.wire_type, bcp_types::fields::FieldWireType::Bytes);\\n        let (data, m) = bcp_types::fields::decode_bytes_value(&bytes[n..]).unwrap();\\n        assert_eq!(data, b\\\"hello\\\");\\n        assert_eq!(n + m, bytes.len());\\n    }\\n\\n    #[test]\\n    fn nested_field_roundtrip() {\\n        let mut inner = BlockWriter::new();\\n        inner.write_varint_field(1, 99);\\n        let inner_bytes = inner.finish();\\n\\n        let mut outer = BlockWriter::new();\\n        outer.write_nested_field(3, &inner_bytes);\\n        let bytes = outer.finish();\\n\\n        let (header, n) = bcp_types::fields::decode_field_header(&bytes).unwrap();\\n        assert_eq!(header.field_id, 3);\\n        assert_eq!(header.wire_type, bcp_types::fields::FieldWireType::Nested);\\n        let (nested, m) = bcp_types::fields::decode_bytes_value(&bytes[n..]).unwrap();\\n        assert_eq!(n + m, bytes.len());\\n\\n        // Decode the nested content\\n        let (inner_header, k) = bcp_types::fields::decode_field_header(nested).unwrap();\\n        assert_eq!(inner_header.field_id, 1);\\n        let (val, _) = bcp_types::fields::decode_varint_value(&nested[k..]).unwrap();\\n        assert_eq!(val, 99);\\n    }\\n\\n    #[test]\\n    fn multiple_fields_sequential() {\\n        let mut writer = BlockWriter::new();\\n        writer.write_varint_field(1, 7);\\n        writer.write_bytes_field(2, b\\\"world\\\");\\n        writer.write_varint_field(3, 256);\\n        let bytes = writer.finish();\\n\\n        // Should be decodable as 3 sequential fields\\n        let mut cursor = 0;\\n\\n        let (h, n) = bcp_types::fields::decode_field_header(&bytes[cursor..]).unwrap();\\n        cursor += n;\\n        assert_eq!(h.field_id, 1);\\n        let (v, n) = bcp_types::fields::decode_varint_value(&bytes[cursor..]).unwrap();\\n        cursor += n;\\n        assert_eq!(v, 7);\\n\\n        let (h, n) = bcp_types::fields::decode_field_header(&bytes[cursor..]).unwrap();\\n        cursor += n;\\n        assert_eq!(h.field_id, 2);\\n        let (data, n) = bcp_types::fields::decode_bytes_value(&bytes[cursor..]).unwrap();\\n        cursor += n;\\n        assert_eq!(data, b\\\"world\\\");\\n\\n        let (h, n) = bcp_types::fields::decode_field_header(&bytes[cursor..]).unwrap();\\n        cursor += n;\\n        assert_eq!(h.field_id, 3);\\n        let (v, n) = bcp_types::fields::decode_varint_value(&bytes[cursor..]).unwrap();\\n        cursor += n;\\n        assert_eq!(v, 256);\\n\\n        assert_eq!(cursor, bytes.len());\\n    }\\n}\\n\",\n      \"language\": \"rust\",\n      \"path\": \"block_writer.rs\",\n      \"type\": \"code\"\n    },\n    {\n      \"content\": \"use std::io::Cursor;\\n\\nuse crate::error::CompressionError;\\n\\n/// Minimum block body size (in bytes) before per-block compression\\n/// is attempted.\\n///\\n/// Blocks smaller than this threshold are always stored uncompressed\\n/// because zstd framing overhead (~13 bytes for the frame header)\\n/// outweighs any savings on very small inputs.\\n///\\n/// Default: 256 bytes.\\npub const COMPRESSION_THRESHOLD: usize = 256;\\n\\n/// Default zstd compression level (1–22 scale).\\n///\\n/// Level 3 provides a good balance of speed and compression ratio\\n/// for typical code/text context blocks (RFC §4.6). Higher levels\\n/// yield diminishing returns for the latency cost.\\nconst DEFAULT_COMPRESSION_LEVEL: i32 = 3;\\n\\n/// Compress a byte slice with zstd.\\n///\\n/// Returns `Some(compressed)` if compression reduced the size, or\\n/// `None` if the compressed output is >= the input size. This\\n/// ensures compression is never harmful — the caller should store\\n/// the block uncompressed when `None` is returned.\\n///\\n/// Uses the default compression level (3).\\n///\\n/// # Example\\n///\\n/// ```rust\\n/// use bcp_encoder::compression::compress;\\n///\\n/// let data = \\\"fn main() { }\\\\n\\\".repeat(100);\\n/// match compress(data.as_bytes()) {\\n///     Some(compressed) => assert!(compressed.len() < data.len()),\\n///     None => { /* data was incompressible */ }\\n/// }\\n/// ```\\npub fn compress(data: &[u8]) -> Option<Vec<u8>> {\\n    let compressed = zstd::encode_all(Cursor::new(data), DEFAULT_COMPRESSION_LEVEL).ok()?;\\n    if compressed.len() < data.len() {\\n        Some(compressed)\\n    } else {\\n        None\\n    }\\n}\\n\\n/// Decompress a zstd-compressed byte slice.\\n///\\n/// The `max_size` parameter provides an upper bound on the\\n/// decompressed output to prevent decompression bombs — if the\\n/// decompressed data exceeds this limit, an error is returned\\n/// without completing decompression.\\n///\\n/// # Errors\\n///\\n/// - [`CompressionError::DecompressFailed`] if zstd cannot decode\\n///   the input (invalid frame, truncated data, etc.).\\n/// - [`CompressionError::DecompressionBomb`] if the decompressed\\n///   size exceeds `max_size`.\\npub fn decompress(data: &[u8], max_size: usize) -> Result<Vec<u8>, CompressionError> {\\n    let decompressed = zstd::decode_all(Cursor::new(data))\\n        .map_err(|e| CompressionError::DecompressFailed(e.to_string()))?;\\n    if decompressed.len() > max_size {\\n        return Err(CompressionError::DecompressionBomb {\\n            actual: decompressed.len(),\\n            limit: max_size,\\n        });\\n    }\\n    Ok(decompressed)\\n}\\n\\n#[cfg(test)]\\nmod tests {\\n    use super::*;\\n\\n    #[test]\\n    fn compress_returns_none_for_small_incompressible_data() {\\n        let data = b\\\"abc123\\\";\\n        assert!(compress(data).is_none());\\n    }\\n\\n    #[test]\\n    fn compress_reduces_repetitive_data() {\\n        let data = \\\"fn main() { }\\\\n\\\".repeat(100);\\n        let result = compress(data.as_bytes());\\n        assert!(result.is_some());\\n        let compressed = result.unwrap();\\n        assert!(compressed.len() < data.len());\\n    }\\n\\n    #[test]\\n    fn compress_decompress_roundtrip() {\\n        let data = \\\"pub fn hello() -> &'static str { \\\\\\\"world\\\\\\\" }\\\\n\\\".repeat(50);\\n        let compressed = compress(data.as_bytes()).expect(\\\"should compress\\\");\\n        let decompressed = decompress(&compressed, 1024 * 1024).expect(\\\"should decompress\\\");\\n        assert_eq!(decompressed, data.as_bytes());\\n    }\\n\\n    #[test]\\n    fn decompress_rejects_bomb() {\\n        let data = \\\"x\\\".repeat(10_000);\\n        let compressed = compress(data.as_bytes()).expect(\\\"should compress\\\");\\n        let result = decompress(&compressed, 100);\\n        assert!(matches!(\\n            result,\\n            Err(CompressionError::DecompressionBomb { .. })\\n        ));\\n    }\\n\\n    #[test]\\n    fn decompress_rejects_invalid_data() {\\n        let garbage = b\\\"this is not zstd data\\\";\\n        let result = decompress(garbage, 1024 * 1024);\\n        assert!(matches!(result, Err(CompressionError::DecompressFailed(_))));\\n    }\\n\\n    #[test]\\n    fn compression_threshold_is_256() {\\n        assert_eq!(COMPRESSION_THRESHOLD, 256);\\n    }\\n}\\n\",\n      \"language\": \"rust\",\n      \"path\": \"compression.rs\",\n      \"type\": \"code\"\n    },\n    {\n      \"content\": \"use std::collections::HashMap;\\nuse std::sync::RwLock;\\n\\nuse bcp_types::content_store::ContentStore;\\n\\n/// In-memory content store backed by a `HashMap`.\\n///\\n/// Suitable for the PoC and testing. Not persisted across runs.\\n/// Uses [`RwLock`] for interior mutability so that the\\n/// [`ContentStore`] trait methods (which take `&self`) can mutate\\n/// the internal map safely across threads.\\n///\\n/// # Concurrency\\n///\\n/// Read operations (`get`, `contains`) acquire a read lock.\\n/// Write operations (`put`) acquire a write lock. Multiple\\n/// concurrent readers are allowed; writers are exclusive.\\n///\\n/// # Example\\n///\\n/// ```rust\\n/// use bcp_encoder::MemoryContentStore;\\n/// use bcp_types::ContentStore;\\n///\\n/// let store = MemoryContentStore::new();\\n/// let data = b\\\"fn main() {}\\\";\\n/// let hash = store.put(data);\\n/// assert_eq!(store.get(&hash).unwrap(), data);\\n/// assert!(store.contains(&hash));\\n/// assert_eq!(store.len(), 1);\\n/// ```\\npub struct MemoryContentStore {\\n    store: RwLock<HashMap<[u8; 32], Vec<u8>>>,\\n}\\n\\nimpl MemoryContentStore {\\n    /// Create an empty in-memory content store.\\n    #[must_use]\\n    pub fn new() -> Self {\\n        Self {\\n            store: RwLock::new(HashMap::new()),\\n        }\\n    }\\n\\n    /// Return the number of unique entries in the store.\\n    #[must_use]\\n    pub fn len(&self) -> usize {\\n        self.store\\n            .read()\\n            .expect(\\\"content store lock poisoned\\\")\\n            .len()\\n    }\\n\\n    /// Return `true` if the store contains no entries.\\n    #[must_use]\\n    pub fn is_empty(&self) -> bool {\\n        self.len() == 0\\n    }\\n\\n    /// Return the total bytes stored across all entries.\\n    ///\\n    /// This counts only the content bytes, not the 32-byte hash\\n    /// keys or `HashMap` overhead.\\n    #[must_use]\\n    pub fn total_bytes(&self) -> usize {\\n        self.store\\n            .read()\\n            .expect(\\\"content store lock poisoned\\\")\\n            .values()\\n            .map(Vec::len)\\n            .sum()\\n    }\\n}\\n\\nimpl Default for MemoryContentStore {\\n    fn default() -> Self {\\n        Self::new()\\n    }\\n}\\n\\nimpl ContentStore for MemoryContentStore {\\n    fn get(&self, hash: &[u8; 32]) -> Option<Vec<u8>> {\\n        self.store\\n            .read()\\n            .expect(\\\"content store lock poisoned\\\")\\n            .get(hash)\\n            .cloned()\\n    }\\n\\n    fn put(&self, content: &[u8]) -> [u8; 32] {\\n        let hash: [u8; 32] = blake3::hash(content).into();\\n        let mut store = self.store.write().expect(\\\"content store lock poisoned\\\");\\n        store.entry(hash).or_insert_with(|| content.to_vec());\\n        hash\\n    }\\n\\n    fn contains(&self, hash: &[u8; 32]) -> bool {\\n        self.store\\n            .read()\\n            .expect(\\\"content store lock poisoned\\\")\\n            .contains_key(hash)\\n    }\\n}\\n\\n#[cfg(test)]\\nmod tests {\\n    use super::*;\\n\\n    #[test]\\n    fn put_get_roundtrip() {\\n        let store = MemoryContentStore::new();\\n        let data = b\\\"fn main() { println!(\\\\\\\"hello\\\\\\\"); }\\\";\\n        let hash = store.put(data);\\n        let retrieved = store.get(&hash).expect(\\\"should find stored content\\\");\\n        assert_eq!(retrieved, data);\\n    }\\n\\n    #[test]\\n    fn put_returns_deterministic_hash() {\\n        let store = MemoryContentStore::new();\\n        let data = b\\\"deterministic content\\\";\\n        let hash1 = store.put(data);\\n        let hash2 = store.put(data);\\n        assert_eq!(hash1, hash2);\\n    }\\n\\n    #[test]\\n    fn dedup_stores_only_once() {\\n        let store = MemoryContentStore::new();\\n        let data = b\\\"duplicate content\\\";\\n        store.put(data);\\n        store.put(data);\\n        assert_eq!(store.len(), 1);\\n    }\\n\\n    #[test]\\n    fn contains_returns_true_for_stored_hash() {\\n        let store = MemoryContentStore::new();\\n        let data = b\\\"some content\\\";\\n        let hash = store.put(data);\\n        assert!(store.contains(&hash));\\n    }\\n\\n    #[test]\\n    fn contains_returns_false_for_unknown_hash() {\\n        let store = MemoryContentStore::new();\\n        let fake_hash = [0u8; 32];\\n        assert!(!store.contains(&fake_hash));\\n    }\\n\\n    #[test]\\n    fn get_returns_none_for_unknown_hash() {\\n        let store = MemoryContentStore::new();\\n        let fake_hash = [0xFF; 32];\\n        assert!(store.get(&fake_hash).is_none());\\n    }\\n\\n    #[test]\\n    fn len_and_total_bytes() {\\n        let store = MemoryContentStore::new();\\n        assert_eq!(store.len(), 0);\\n        assert!(store.is_empty());\\n        assert_eq!(store.total_bytes(), 0);\\n\\n        store.put(b\\\"hello\\\"); // 5 bytes\\n        store.put(b\\\"world\\\"); // 5 bytes\\n        assert_eq!(store.len(), 2);\\n        assert!(!store.is_empty());\\n        assert_eq!(store.total_bytes(), 10);\\n    }\\n\\n    #[test]\\n    fn blake3_hash_is_32_bytes() {\\n        let store = MemoryContentStore::new();\\n        let hash = store.put(b\\\"test\\\");\\n        assert_eq!(hash.len(), 32);\\n    }\\n\\n    #[test]\\n    fn different_content_produces_different_hashes() {\\n        let store = MemoryContentStore::new();\\n        let hash1 = store.put(b\\\"content A\\\");\\n        let hash2 = store.put(b\\\"content B\\\");\\n        assert_ne!(hash1, hash2);\\n    }\\n}\\n\",\n      \"language\": \"rust\",\n      \"path\": \"content_store.rs\",\n      \"type\": \"code\"\n    },\n    {\n      \"content\": \"use std::sync::Arc;\\n\\nuse bcp_types::BlockContent;\\nuse bcp_types::annotation::AnnotationBlock;\\nuse bcp_types::code::CodeBlock;\\nuse bcp_types::content_store::ContentStore;\\nuse bcp_types::conversation::ConversationBlock;\\nuse bcp_types::diff::{DiffBlock, DiffHunk};\\nuse bcp_types::embedding_ref::EmbeddingRefBlock;\\nuse bcp_types::document::DocumentBlock;\\nuse bcp_types::enums::{\\n    AnnotationKind, DataFormat, FormatHint, Lang, MediaType, Priority, Role, Status,\\n};\\nuse bcp_types::extension::ExtensionBlock;\\nuse bcp_types::file_tree::{FileEntry, FileTreeBlock};\\nuse bcp_types::image::ImageBlock;\\nuse bcp_types::structured_data::StructuredDataBlock;\\nuse bcp_types::summary::Summary;\\nuse bcp_types::tool_result::ToolResultBlock;\\nuse bcp_wire::block_frame::{BlockFlags, BlockFrame, block_type};\\nuse bcp_wire::header::{HEADER_SIZE, HeaderFlags, BcpHeader};\\n\\nuse crate::compression::{self, COMPRESSION_THRESHOLD};\\nuse crate::error::EncodeError;\\n\\n/// Maximum block body size (16 MiB). Blocks exceeding this limit produce\\n/// an [`EncodeError::BlockTooLarge`] during `.encode()`.\\nconst MAX_BLOCK_BODY_SIZE: usize = 16 * 1024 * 1024;\\n\\n/// BCP encoder — constructs a binary payload from structured blocks.\\n///\\n/// The encoder is the tool-facing API that allows agents, MCP servers,\\n/// and other producers to build BCP payloads. It follows the builder\\n/// pattern defined in RFC §5.6: methods like [`add_code`](Self::add_code),\\n/// [`add_conversation`](Self::add_conversation), etc. append typed blocks\\n/// to an internal list, and chainable modifiers like\\n/// [`with_summary`](Self::with_summary) and\\n/// [`with_priority`](Self::with_priority) annotate the most recently\\n/// added block.\\n///\\n/// # Compression (RFC §4.6)\\n///\\n/// Two compression modes are supported, both opt-in:\\n///\\n/// - **Per-block**: call [`with_compression`](Self::with_compression) after\\n///   adding a block, or [`compress_blocks`](Self::compress_blocks) to\\n///   enable compression for all subsequent blocks. Each block body is\\n///   independently zstd-compressed if it exceeds\\n///   [`COMPRESSION_THRESHOLD`](crate::compression::COMPRESSION_THRESHOLD)\\n///   bytes and compression yields a size reduction. The block's\\n///   `COMPRESSED` flag (bit 1) is set when compression is applied.\\n///\\n/// - **Whole-payload**: call [`compress_payload`](Self::compress_payload)\\n///   to zstd-compress all bytes after the 8-byte header. When enabled,\\n///   per-block compression is skipped (whole-payload subsumes it). The\\n///   header's `COMPRESSED` flag (bit 0) is set.\\n///\\n/// # Content Addressing (RFC §4.7)\\n///\\n/// When a [`ContentStore`] is configured via\\n/// [`set_content_store`](Self::set_content_store), blocks can be stored\\n/// by their BLAKE3 hash rather than inline:\\n///\\n/// - **Per-block**: call [`with_content_addressing`](Self::with_content_addressing)\\n///   after adding a block. The body is hashed, stored in the content store,\\n///   and replaced with the 32-byte hash on the wire. The block's\\n///   `IS_REFERENCE` flag (bit 2) is set.\\n///\\n/// - **Auto-dedup**: call [`auto_dedup`](Self::auto_dedup) to automatically\\n///   content-address any block whose body has been seen before. First\\n///   occurrence is stored inline and registered in the store; subsequent\\n///   identical blocks become references.\\n///\\n/// Content addressing runs before compression — a 32-byte hash reference\\n/// is always below the compression threshold, so reference blocks are\\n/// never compressed.\\n///\\n/// # Usage\\n///\\n/// ```rust\\n/// use bcp_encoder::BcpEncoder;\\n/// use bcp_types::enums::{Lang, Role, Status, Priority};\\n///\\n/// # fn main() -> Result<(), Box<dyn std::error::Error>> {\\n/// let payload = BcpEncoder::new()\\n///     .add_code(Lang::Rust, \\\"src/main.rs\\\", b\\\"fn main() {}\\\")\\n///     .with_summary(\\\"Entry point: CLI setup and server startup.\\\")?\\n///     .with_priority(Priority::High)?\\n///     .add_conversation(Role::User, b\\\"Fix the timeout bug.\\\")\\n///     .add_conversation(Role::Assistant, b\\\"I'll examine the pool config...\\\")\\n///     .add_tool_result(\\\"ripgrep\\\", Status::Ok, b\\\"3 matches found.\\\")\\n///     .encode()?;\\n/// # Ok(())\\n/// # }\\n/// ```\\n///\\n/// # Output layout\\n///\\n/// The `.encode()` method serializes all accumulated blocks into a\\n/// self-contained byte sequence:\\n///\\n/// ```text\\n/// ┌──────────────┬──────────────────────────────────────────┐\\n/// │ [8 bytes]    │ File header (magic, version, flags, rsv) │\\n/// │ [N bytes]    │ Block 0 frame (type + flags + len + body)│\\n/// │ [N bytes]    │ Block 1 frame ...                        │\\n/// │ ...          │                                          │\\n/// │ [2-3 bytes]  │ END sentinel (type=0xFF, flags=0, len=0) │\\n/// └──────────────┴──────────────────────────────────────────┘\\n/// ```\\n///\\n/// When whole-payload compression is enabled, the layout becomes:\\n///\\n/// ```text\\n/// ┌──────────────┬──────────────────────────────────────────┐\\n/// │ [8 bytes]    │ Header (flags bit 0 = COMPRESSED)        │\\n/// │ [N bytes]    │ zstd(Block 0 + Block 1 + ... + END)      │\\n/// └──────────────┴──────────────────────────────────────────┘\\n/// ```\\n///\\n/// The payload is ready for storage or transmission — no further\\n/// framing is required.\\npub struct BcpEncoder {\\n    blocks: Vec<PendingBlock>,\\n    flags: HeaderFlags,\\n    /// When `true`, the entire payload after the header is zstd-compressed.\\n    compress_payload: bool,\\n    /// When `true`, all blocks are individually compressed (unless\\n    /// `compress_payload` is also set, which takes precedence).\\n    compress_all_blocks: bool,\\n    /// Content store for BLAKE3 content-addressed deduplication.\\n    /// Required when any block has `content_address = true` or\\n    /// when `auto_dedup` is enabled.\\n    content_store: Option<Arc<dyn ContentStore>>,\\n    /// When `true`, automatically content-address any block whose body\\n    /// has been seen before (hash already exists in the store).\\n    auto_dedup: bool,\\n}\\n\\n/// Internal representation of a block awaiting serialization.\\n///\\n/// Captures the block type tag, the typed content (which knows how to\\n/// serialize its own TLV body via [`BlockContent::encode_body`]), an\\n/// optional summary to prepend, and per-block compression / content\\n/// addressing flags.\\n///\\n/// `PendingBlock` is never exposed publicly. The encoder builds these\\n/// internally as the caller chains `.add_*()` and `.with_*()` methods,\\n/// then consumes them during `.encode()`.\\nstruct PendingBlock {\\n    block_type: u8,\\n    content: BlockContent,\\n    summary: Option<String>,\\n    /// When `true`, this block's body should be zstd-compressed if it\\n    /// exceeds [`COMPRESSION_THRESHOLD`] and compression yields savings.\\n    compress: bool,\\n    /// When `true`, this block's body should be replaced with its\\n    /// 32-byte BLAKE3 hash and stored in the content store.\\n    content_address: bool,\\n}\\n\\nimpl BcpEncoder {\\n    /// Create a new encoder with default settings (version 1.0, no flags).\\n    ///\\n    /// The encoder starts with an empty block list, no compression, and\\n    /// no content store. At least one block must be added before calling\\n    /// `.encode()`, otherwise it returns [`EncodeError::EmptyPayload`].\\n    #[must_use]\\n    pub fn new() -> Self {\\n        Self {\\n            blocks: Vec::new(),\\n            flags: HeaderFlags::NONE,\\n            compress_payload: false,\\n            compress_all_blocks: false,\\n            content_store: None,\\n            auto_dedup: false,\\n        }\\n    }\\n\\n    // ── Block addition methods ──────────────────────────────────────────\\n    //\\n    // Each method constructs the appropriate `BlockContent` variant from\\n    // `bcp-types`, wraps it in a `PendingBlock`, pushes it onto the\\n    // internal list, and returns `&mut Self` for chaining.\\n\\n    /// Add a CODE block.\\n    ///\\n    /// Encodes a source code file or fragment. The `lang` enum identifies\\n    /// the programming language (used by the decoder for syntax-aware\\n    /// rendering), `path` is the file path (UTF-8), and `content` is the\\n    /// raw source bytes.\\n    ///\\n    /// For partial files, use [`add_code_range`](Self::add_code_range)\\n    /// to include line range metadata.\\n    pub fn add_code(&mut self, lang: Lang, path: &str, content: &[u8]) -> &mut Self {\\n        self.push_block(\\n            block_type::CODE,\\n            BlockContent::Code(CodeBlock {\\n                lang,\\n                path: path.to_string(),\\n                content: content.to_vec(),\\n                line_range: None,\\n            }),\\n        )\\n    }\\n\\n    /// Add a CODE block with a line range.\\n    ///\\n    /// Same as [`add_code`](Self::add_code) but includes `line_start` and\\n    /// `line_end` metadata (1-based, inclusive). The decoder can use this\\n    /// to display line numbers or to correlate with diagnostics.\\n    pub fn add_code_range(\\n        &mut self,\\n        lang: Lang,\\n        path: &str,\\n        content: &[u8],\\n        line_start: u32,\\n        line_end: u32,\\n    ) -> &mut Self {\\n        self.push_block(\\n            block_type::CODE,\\n            BlockContent::Code(CodeBlock {\\n                lang,\\n                path: path.to_string(),\\n                content: content.to_vec(),\\n                line_range: Some((line_start, line_end)),\\n            }),\\n        )\\n    }\\n\\n    /// Add a CONVERSATION block.\\n    ///\\n    /// Represents a single chat turn. The `role` identifies the speaker\\n    /// (system, user, assistant, or tool) and `content` is the message\\n    /// body as raw bytes.\\n    pub fn add_conversation(&mut self, role: Role, content: &[u8]) -> &mut Self {\\n        self.push_block(\\n            block_type::CONVERSATION,\\n            BlockContent::Conversation(ConversationBlock {\\n                role,\\n                content: content.to_vec(),\\n                tool_call_id: None,\\n            }),\\n        )\\n    }\\n\\n    /// Add a CONVERSATION block with a tool call ID.\\n    ///\\n    /// Used for tool-role messages that reference a specific tool\\n    /// invocation. The `tool_call_id` links this response back to the\\n    /// tool call that produced it.\\n    pub fn add_conversation_tool(\\n        &mut self,\\n        role: Role,\\n        content: &[u8],\\n        tool_call_id: &str,\\n    ) -> &mut Self {\\n        self.push_block(\\n            block_type::CONVERSATION,\\n            BlockContent::Conversation(ConversationBlock {\\n                role,\\n                content: content.to_vec(),\\n                tool_call_id: Some(tool_call_id.to_string()),\\n            }),\\n        )\\n    }\\n\\n    /// Add a `FILE_TREE` block.\\n    ///\\n    /// Represents a directory structure rooted at `root`. Each entry\\n    /// contains a name, kind (file or directory), size, and optional\\n    /// nested children for recursive directory trees.\\n    pub fn add_file_tree(&mut self, root: &str, entries: Vec<FileEntry>) -> &mut Self {\\n        self.push_block(\\n            block_type::FILE_TREE,\\n            BlockContent::FileTree(FileTreeBlock {\\n                root_path: root.to_string(),\\n                entries,\\n            }),\\n        )\\n    }\\n\\n    /// Add a `TOOL_RESULT` block.\\n    ///\\n    /// Captures the output of an external tool invocation (e.g. ripgrep,\\n    /// LSP diagnostics, test runner). The `status` indicates whether the\\n    /// tool succeeded, failed, or timed out.\\n    pub fn add_tool_result(&mut self, name: &str, status: Status, content: &[u8]) -> &mut Self {\\n        self.push_block(\\n            block_type::TOOL_RESULT,\\n            BlockContent::ToolResult(ToolResultBlock {\\n                tool_name: name.to_string(),\\n                status,\\n                content: content.to_vec(),\\n                schema_hint: None,\\n            }),\\n        )\\n    }\\n\\n    /// Add a DOCUMENT block.\\n    ///\\n    /// Represents prose content — README files, documentation, wiki pages.\\n    /// The `format_hint` tells the decoder how to render the body\\n    /// (markdown, plain text, or HTML).\\n    pub fn add_document(\\n        &mut self,\\n        title: &str,\\n        content: &[u8],\\n        format_hint: FormatHint,\\n    ) -> &mut Self {\\n        self.push_block(\\n            block_type::DOCUMENT,\\n            BlockContent::Document(DocumentBlock {\\n                title: title.to_string(),\\n                content: content.to_vec(),\\n                format_hint,\\n            }),\\n        )\\n    }\\n\\n    /// Add a `STRUCTURED_DATA` block.\\n    ///\\n    /// Encodes tabular or structured content — JSON configs, YAML\\n    /// manifests, TOML files, CSV data. The `format` identifies the\\n    /// serialization format so the decoder can syntax-highlight or\\n    /// parse appropriately.\\n    pub fn add_structured_data(&mut self, format: DataFormat, content: &[u8]) -> &mut Self {\\n        self.push_block(\\n            block_type::STRUCTURED_DATA,\\n            BlockContent::StructuredData(StructuredDataBlock {\\n                format,\\n                content: content.to_vec(),\\n                schema: None,\\n            }),\\n        )\\n    }\\n\\n    /// Add a DIFF block.\\n    ///\\n    /// Represents code changes for a single file — from git diffs, editor\\n    /// changes, or patch files. Each hunk captures a contiguous range of\\n    /// modifications in unified diff format.\\n    pub fn add_diff(&mut self, path: &str, hunks: Vec<DiffHunk>) -> &mut Self {\\n        self.push_block(\\n            block_type::DIFF,\\n            BlockContent::Diff(DiffBlock {\\n                path: path.to_string(),\\n                hunks,\\n            }),\\n        )\\n    }\\n\\n    /// Add an ANNOTATION block.\\n    ///\\n    /// Annotations are metadata overlays that target another block by its\\n    /// zero-based index in the stream. The `kind` determines how the\\n    /// `value` payload is interpreted (priority level, summary text, or\\n    /// tag label).\\n    ///\\n    /// For the common case of attaching a priority to the most recent\\n    /// block, prefer [`with_priority`](Self::with_priority).\\n    pub fn add_annotation(\\n        &mut self,\\n        target_block_id: u32,\\n        kind: AnnotationKind,\\n        value: &[u8],\\n    ) -> &mut Self {\\n        self.push_block(\\n            block_type::ANNOTATION,\\n            BlockContent::Annotation(AnnotationBlock {\\n                target_block_id,\\n                kind,\\n                value: value.to_vec(),\\n            }),\\n        )\\n    }\\n\\n    /// Add an EMBEDDING_REF block.\\n    ///\\n    /// Points to a pre-computed vector embedding stored externally (e.g.\\n    /// in a vector database). The `vector_id` is an opaque byte identifier\\n    /// for the vector in the external store, `source_hash` is the BLAKE3\\n    /// hash of the content that was embedded (32 bytes), and `model` is\\n    /// the name of the embedding model (e.g. `\\\"text-embedding-3-small\\\"`).\\n    ///\\n    /// # Wire type\\n    ///\\n    /// Block type `0x09` (`EMBEDDING_REF`). See RFC §4.4.\\n    pub fn add_embedding_ref(\\n        &mut self,\\n        vector_id: &[u8],\\n        source_hash: &[u8],\\n        model: &str,\\n    ) -> &mut Self {\\n        self.push_block(\\n            block_type::EMBEDDING_REF,\\n            BlockContent::EmbeddingRef(EmbeddingRefBlock {\\n                vector_id: vector_id.to_vec(),\\n                source_hash: source_hash.to_vec(),\\n                model: model.to_string(),\\n            }),\\n        )\\n    }\\n\\n    /// Add an IMAGE block.\\n    ///\\n    /// Encodes an image as inline binary data. The `media_type` identifies\\n    /// the image format (PNG, JPEG, etc.), `alt_text` provides a textual\\n    /// description for accessibility, and `data` is the raw image bytes.\\n    pub fn add_image(&mut self, media_type: MediaType, alt_text: &str, data: &[u8]) -> &mut Self {\\n        self.push_block(\\n            block_type::IMAGE,\\n            BlockContent::Image(ImageBlock {\\n                media_type,\\n                alt_text: alt_text.to_string(),\\n                data: data.to_vec(),\\n            }),\\n        )\\n    }\\n\\n    /// Add an EXTENSION block.\\n    ///\\n    /// User-defined block type for custom payloads. The `namespace` and\\n    /// `type_name` together form a unique identifier for the extension\\n    /// type, preventing collisions across different tools and vendors.\\n    pub fn add_extension(&mut self, namespace: &str, type_name: &str, content: &[u8]) -> &mut Self {\\n        self.push_block(\\n            block_type::EXTENSION,\\n            BlockContent::Extension(ExtensionBlock {\\n                namespace: namespace.to_string(),\\n                type_name: type_name.to_string(),\\n                content: content.to_vec(),\\n            }),\\n        )\\n    }\\n\\n    // ── Modifier methods ────────────────────────────────────────────────\\n    //\\n    // Modifiers act on the most recently added block. They set metadata\\n    // that affects how the block is serialized (summary prefix, flags)\\n    // or append related blocks (priority annotation).\\n\\n    /// Attach a summary to the most recently added block.\\n    ///\\n    /// Sets the `HAS_SUMMARY` flag on the block and prepends the summary\\n    /// sub-block to the body during serialization. The summary is a\\n    /// compact UTF-8 description that the token budget engine can use as\\n    /// a stand-in when the full block content would exceed the budget.\\n    ///\\n    /// # Errors\\n    ///\\n    /// Returns [`EncodeError::NoBlockTarget`] if no blocks have been\\n    /// added yet. Use this immediately after an `.add_*()` call.\\n    pub fn with_summary(&mut self, summary: &str) -> Result<&mut Self, EncodeError> {\\n        let block = self\\n            .blocks\\n            .last_mut()\\n            .ok_or(EncodeError::NoBlockTarget { method: \\\"with_summary\\\" })?;\\n        block.summary = Some(summary.to_string());\\n        Ok(self)\\n    }\\n\\n    /// Attach a priority annotation to the most recently added block.\\n    ///\\n    /// This is a convenience method that appends an ANNOTATION block\\n    /// with `kind=Priority` targeting the last added block's index.\\n    /// The annotation's value is the priority byte (e.g. `0x02` for\\n    /// `Priority::High`).\\n    ///\\n    /// # Errors\\n    ///\\n    /// Returns [`EncodeError::NoBlockTarget`] if no blocks have been\\n    /// added yet.\\n    pub fn with_priority(&mut self, priority: Priority) -> Result<&mut Self, EncodeError> {\\n        let target_index = self\\n            .blocks\\n            .len()\\n            .checked_sub(1)\\n            .ok_or(EncodeError::NoBlockTarget { method: \\\"with_priority\\\" })?;\\n\\n        #[allow(clippy::cast_possible_truncation)]\\n        let target_id = target_index as u32;\\n\\n        self.push_block(\\n            block_type::ANNOTATION,\\n            BlockContent::Annotation(AnnotationBlock {\\n                target_block_id: target_id,\\n                kind: AnnotationKind::Priority,\\n                value: vec![priority.to_wire_byte()],\\n            }),\\n        );\\n        Ok(self)\\n    }\\n\\n    // ── Compression modifiers ────────────────────────────────────────────\\n    //\\n    // These methods control per-block and whole-payload zstd compression.\\n    // Per-block compression is skipped when whole-payload compression is\\n    // enabled — the outer zstd frame subsumes individual block compression.\\n\\n    /// Enable zstd compression for the most recently added block.\\n    ///\\n    /// During `.encode()`, the block body is compressed with zstd if it\\n    /// exceeds [`COMPRESSION_THRESHOLD`] bytes and compression yields a\\n    /// size reduction. If compression doesn't help (output >= input), the\\n    /// body is stored uncompressed and the `COMPRESSED` flag is not set.\\n    ///\\n    /// Has no effect if [`compress_payload`](Self::compress_payload) is\\n    /// also enabled — whole-payload compression takes precedence.\\n    ///\\n    /// # Errors\\n    ///\\n    /// Returns [`EncodeError::NoBlockTarget`] if no blocks have been\\n    /// added yet.\\n    pub fn with_compression(&mut self) -> Result<&mut Self, EncodeError> {\\n        let block = self\\n            .blocks\\n            .last_mut()\\n            .ok_or(EncodeError::NoBlockTarget { method: \\\"with_compression\\\" })?;\\n        block.compress = true;\\n        Ok(self)\\n    }\\n\\n    /// Enable zstd compression for all blocks added so far and all\\n    /// future blocks.\\n    ///\\n    /// Equivalent to calling [`with_compression`](Self::with_compression)\\n    /// on every block. Individual blocks still respect the size threshold\\n    /// and no-savings guard.\\n    pub fn compress_blocks(&mut self) -> &mut Self {\\n        self.compress_all_blocks = true;\\n        for block in &mut self.blocks {\\n            block.compress = true;\\n        }\\n        self\\n    }\\n\\n    /// Enable whole-payload zstd compression.\\n    ///\\n    /// When set, the entire block stream (all frames + END sentinel) is\\n    /// compressed as a single zstd frame. The 8-byte header is written\\n    /// uncompressed with `HeaderFlags::COMPRESSED` set so the decoder\\n    /// can detect compression before reading further.\\n    ///\\n    /// When whole-payload compression is enabled, per-block compression\\n    /// is skipped — compressing within a compressed stream adds overhead\\n    /// without benefit.\\n    ///\\n    /// If compression doesn't reduce the total size, the payload is\\n    /// stored uncompressed and the header flag is not set.\\n    ///\\n    /// **Tradeoff**: Whole-payload compression disables incremental\\n    /// streaming in `StreamingDecoder` — the decoder must buffer and\\n    /// decompress the entire payload before yielding any blocks. If\\n    /// streaming is important, use [`compress_blocks`](Self::compress_blocks)\\n    /// instead.\\n    pub fn compress_payload(&mut self) -> &mut Self {\\n        self.compress_payload = true;\\n        self\\n    }\\n\\n    // ── Content addressing modifiers ────────────────────────────────────\\n    //\\n    // These methods control BLAKE3 content-addressed deduplication.\\n    // A content store must be configured before blocks can be\\n    // content-addressed.\\n\\n    /// Set the content store used for BLAKE3 content addressing.\\n    ///\\n    /// The store is shared via `Arc` so the same store can be passed to\\n    /// both the encoder and decoder for roundtrip workflows. The encoder\\n    /// calls `store.put()` for each content-addressed block; the decoder\\n    /// calls `store.get()` to resolve references.\\n    ///\\n    /// Must be called before `.encode()` if any block has content\\n    /// addressing enabled or if [`auto_dedup`](Self::auto_dedup) is set.\\n    pub fn set_content_store(&mut self, store: Arc<dyn ContentStore>) -> &mut Self {\\n        self.content_store = Some(store);\\n        self\\n    }\\n\\n    /// Enable content addressing for the most recently added block.\\n    ///\\n    /// During `.encode()`, the block body is hashed with BLAKE3,\\n    /// stored in the content store, and replaced with the 32-byte hash\\n    /// on the wire. The block's `IS_REFERENCE` flag (bit 2) is set.\\n    ///\\n    /// Requires a content store — call\\n    /// [`set_content_store`](Self::set_content_store) before `.encode()`.\\n    ///\\n    /// Content addressing runs before compression. Since a 32-byte\\n    /// hash reference is always below [`COMPRESSION_THRESHOLD`],\\n    /// reference blocks are never per-block compressed.\\n    ///\\n    /// # Errors\\n    ///\\n    /// Returns [`EncodeError::NoBlockTarget`] if no blocks have been\\n    /// added yet.\\n    pub fn with_content_addressing(&mut self) -> Result<&mut Self, EncodeError> {\\n        let block = self\\n            .blocks\\n            .last_mut()\\n            .ok_or(EncodeError::NoBlockTarget { method: \\\"with_content_addressing\\\" })?;\\n        block.content_address = true;\\n        Ok(self)\\n    }\\n\\n    /// Enable automatic deduplication across all blocks.\\n    ///\\n    /// When set, the encoder hashes every block body with BLAKE3 during\\n    /// `.encode()`. If the hash already exists in the content store\\n    /// (i.e. a previous block in this or a prior encoding had the same\\n    /// content), the block is automatically replaced with a hash\\n    /// reference. First-occurrence blocks are stored inline and\\n    /// registered in the store for future dedup.\\n    ///\\n    /// Requires a content store — call\\n    /// [`set_content_store`](Self::set_content_store) before `.encode()`.\\n    pub fn auto_dedup(&mut self) -> &mut Self {\\n        self.auto_dedup = true;\\n        self\\n    }\\n\\n    // ── Encode ──────────────────────────────────────────────────────────\\n\\n    /// Serialize all accumulated blocks into a complete BCP payload.\\n    ///\\n    /// The encode pipeline processes each `PendingBlock` through up to\\n    /// three stages:\\n    ///\\n    ///   1. **Serialize** — calls [`BlockContent::encode_body`] to get\\n    ///      the TLV-encoded body bytes. If a summary is present, it is\\n    ///      prepended and the `HAS_SUMMARY` flag is set.\\n    ///\\n    ///   2. **Content address** (optional) — if the block has\\n    ///      `content_address = true` or auto-dedup detects a duplicate,\\n    ///      the body is hashed with BLAKE3, stored in the content store,\\n    ///      and replaced with the 32-byte hash. The `IS_REFERENCE` flag\\n    ///      (bit 2) is set.\\n    ///\\n    ///   3. **Per-block compress** (optional) — if compression is enabled\\n    ///      for this block, whole-payload compression is NOT active, and\\n    ///      the body is not a reference, the body is zstd-compressed if\\n    ///      it exceeds [`COMPRESSION_THRESHOLD`] and compression yields\\n    ///      savings. The `COMPRESSED` flag (bit 1) is set.\\n    ///\\n    /// After all blocks, the END sentinel is appended. If whole-payload\\n    /// compression is enabled, everything after the 8-byte header is\\n    /// compressed as a single zstd frame and the header's `COMPRESSED`\\n    /// flag is set.\\n    ///\\n    /// # Errors\\n    ///\\n    /// - [`EncodeError::EmptyPayload`] if no blocks have been added.\\n    /// - [`EncodeError::BlockTooLarge`] if any block body exceeds 16 MiB.\\n    /// - [`EncodeError::MissingContentStore`] if content addressing is\\n    ///   requested but no store has been configured.\\n    /// - [`EncodeError::Wire`] if the underlying wire serialization fails.\\n    /// - [`EncodeError::Io`] if writing to the output buffer fails.\\n    pub fn encode(&self) -> Result<Vec<u8>, EncodeError> {\\n        if self.blocks.is_empty() {\\n            return Err(EncodeError::EmptyPayload);\\n        }\\n\\n        // Validate: if any block needs content addressing or auto_dedup\\n        // is enabled, a store must be present.\\n        let needs_store = self.auto_dedup || self.blocks.iter().any(|b| b.content_address);\\n        if needs_store && self.content_store.is_none() {\\n            return Err(EncodeError::MissingContentStore);\\n        }\\n\\n        // Pre-allocate: 8 bytes header + estimated block data + END sentinel.\\n        let estimated_size = HEADER_SIZE + self.blocks.len() * 256 + 3;\\n        let mut output = Vec::with_capacity(estimated_size);\\n\\n        // 1. Write a placeholder header (flags may be updated for whole-payload).\\n        output.resize(HEADER_SIZE, 0);\\n\\n        // 2. Serialize each pending block through the encode pipeline.\\n        for pending in &self.blocks {\\n            let mut body = Self::serialize_block_body(pending)?;\\n            let mut flags_raw = 0u8;\\n\\n            if pending.summary.is_some() {\\n                flags_raw |= BlockFlags::HAS_SUMMARY.raw();\\n            }\\n\\n            // Stage 2: Content addressing (runs before compression).\\n            let is_reference = self.apply_content_addressing(pending, &mut body)?;\\n            if is_reference {\\n                flags_raw |= BlockFlags::IS_REFERENCE.raw();\\n            }\\n\\n            // Stage 3: Per-block compression (skipped for references and\\n            // when whole-payload compression is active).\\n            if !is_reference && !self.compress_payload {\\n                let should_compress = pending.compress || self.compress_all_blocks;\\n                if should_compress && body.len() >= COMPRESSION_THRESHOLD {\\n                    if let Some(compressed) = compression::compress(&body) {\\n                        body = compressed;\\n                        flags_raw |= BlockFlags::COMPRESSED.raw();\\n                    }\\n                }\\n            }\\n\\n            let frame = BlockFrame {\\n                block_type: pending.block_type,\\n                flags: BlockFlags::from_raw(flags_raw),\\n                body,\\n            };\\n            frame.write_to(&mut output)?;\\n        }\\n\\n        // 3. Write the END sentinel.\\n        let end_frame = BlockFrame {\\n            block_type: block_type::END,\\n            flags: BlockFlags::NONE,\\n            body: Vec::new(),\\n        };\\n        end_frame.write_to(&mut output)?;\\n\\n        // 4. Whole-payload compression: compress everything after the header.\\n        let header_flags = if self.compress_payload {\\n            let block_data = &output[HEADER_SIZE..];\\n            match compression::compress(block_data) {\\n                Some(compressed) => {\\n                    output.truncate(HEADER_SIZE);\\n                    output.extend_from_slice(&compressed);\\n                    HeaderFlags::from_raw(self.flags.raw() | HeaderFlags::COMPRESSED.raw())\\n                }\\n                None => self.flags,\\n            }\\n        } else {\\n            self.flags\\n        };\\n\\n        // 5. Write the final header with correct flags.\\n        let header = BcpHeader::new(header_flags);\\n        header.write_to(&mut output[..HEADER_SIZE])?;\\n\\n        Ok(output)\\n    }\\n\\n    // ── Internal helpers ────────────────────────────────────────────────\\n\\n    /// Push a new `PendingBlock` onto the internal list.\\n    ///\\n    /// If `compress_all_blocks` is set, the new block inherits\\n    /// `compress = true` automatically.\\n    ///\\n    /// Returns `&mut Self` so callers can chain additional methods.\\n    fn push_block(&mut self, block_type: u8, content: BlockContent) -> &mut Self {\\n        self.blocks.push(PendingBlock {\\n            block_type,\\n            content,\\n            summary: None,\\n            compress: self.compress_all_blocks,\\n            content_address: false,\\n        });\\n        self\\n    }\\n\\n    /// Apply content addressing to a block body if requested.\\n    ///\\n    /// Returns `true` if the body was replaced with a 32-byte hash\\n    /// reference, `false` if the body is unchanged (inline).\\n    ///\\n    /// Two paths trigger content addressing:\\n    /// 1. `pending.content_address == true` — always replace with hash.\\n    /// 2. `self.auto_dedup == true` — replace only if the hash already\\n    ///    exists in the store (i.e. a duplicate). First occurrence is\\n    ///    stored inline and registered for future dedup.\\n    fn apply_content_addressing(\\n        &self,\\n        pending: &PendingBlock,\\n        body: &mut Vec<u8>,\\n    ) -> Result<bool, EncodeError> {\\n        let store = match &self.content_store {\\n            Some(s) => s,\\n            None => return Ok(false),\\n        };\\n\\n        if pending.content_address {\\n            // Explicit content addressing: always replace with hash.\\n            let hash = store.put(body);\\n            *body = hash.to_vec();\\n            return Ok(true);\\n        }\\n\\n        if self.auto_dedup {\\n            // Auto-dedup: check if this body was seen before.\\n            let hash: [u8; 32] = blake3::hash(body).into();\\n            if store.contains(&hash) {\\n                // Duplicate — replace with reference.\\n                *body = hash.to_vec();\\n                return Ok(true);\\n            }\\n            // First occurrence — store for future dedup, keep inline.\\n            store.put(body);\\n        }\\n\\n        Ok(false)\\n    }\\n\\n    /// Serialize a `PendingBlock` into its final body bytes.\\n    ///\\n    /// If the block has a summary, the summary is encoded first (as a\\n    /// length-prefixed UTF-8 string) followed by the TLV body fields.\\n    /// This matches the wire convention: when `HAS_SUMMARY` is set, the\\n    /// summary occupies the front of the body, before any TLV fields.\\n    fn serialize_block_body(pending: &PendingBlock) -> Result<Vec<u8>, EncodeError> {\\n        let tlv_body = pending.content.encode_body();\\n        let mut body = Vec::new();\\n\\n        if let Some(ref summary_text) = pending.summary {\\n            let summary = Summary {\\n                text: summary_text.clone(),\\n            };\\n            summary.encode(&mut body);\\n        }\\n\\n        body.extend_from_slice(&tlv_body);\\n\\n        if body.len() > MAX_BLOCK_BODY_SIZE {\\n            return Err(EncodeError::BlockTooLarge {\\n                size: body.len(),\\n                limit: MAX_BLOCK_BODY_SIZE,\\n            });\\n        }\\n\\n        Ok(body)\\n    }\\n}\\n\\nimpl Default for BcpEncoder {\\n    fn default() -> Self {\\n        Self::new()\\n    }\\n}\\n\\n#[cfg(test)]\\nmod tests {\\n    use super::*;\\n    use bcp_types::file_tree::FileEntryKind;\\n    use bcp_wire::header::BCP_MAGIC;\\n\\n    // ── Helper ──────────────────────────────────────────────────────────\\n\\n    /// Verify that a payload starts with the BCP magic number.\\n    fn assert_starts_with_magic(payload: &[u8]) {\\n        assert!(payload.len() >= HEADER_SIZE, \\\"payload too short for header\\\");\\n        assert_eq!(&payload[..4], &BCP_MAGIC, \\\"missing BCP magic\\\");\\n    }\\n\\n    /// Verify that a payload ends with a valid END sentinel.\\n    ///\\n    /// The END sentinel is: block_type=0xFF as varint (2 bytes: 0xFF 0x01),\\n    /// flags=0x00, content_len=0 as varint (1 byte: 0x00).\\n    fn assert_ends_with_end_sentinel(payload: &[u8]) {\\n        // The END block type 0xFF encodes as varint [0xFF, 0x01],\\n        // followed by flags byte 0x00, followed by content_len varint 0x00.\\n        let tail = &payload[payload.len() - 4..];\\n        assert_eq!(tail, &[0xFF, 0x01, 0x00, 0x00], \\\"missing END sentinel\\\");\\n    }\\n\\n    // ── Acceptance criteria tests ───────────────────────────────────────\\n\\n    #[test]\\n    fn encode_single_code_block_produces_valid_magic() {\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"src/main.rs\\\", b\\\"fn main() {}\\\")\\n            .encode()\\n            .unwrap();\\n\\n        assert_starts_with_magic(&payload);\\n    }\\n\\n    #[test]\\n    fn builder_methods_are_chainable() {\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"src/lib.rs\\\", b\\\"pub fn hello() {}\\\")\\n            .with_summary(\\\"Hello function.\\\").unwrap()\\n            .add_conversation(Role::User, b\\\"What does this do?\\\")\\n            .encode()\\n            .unwrap();\\n\\n        assert_starts_with_magic(&payload);\\n        assert_ends_with_end_sentinel(&payload);\\n    }\\n\\n    #[test]\\n    fn with_summary_sets_has_summary_flag() {\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Python, \\\"main.py\\\", b\\\"print('hi')\\\")\\n            .with_summary(\\\"Prints a greeting.\\\").unwrap()\\n            .encode()\\n            .unwrap();\\n\\n        // Parse: skip the 8-byte header, read the first block frame.\\n        let frame_buf = &payload[HEADER_SIZE..];\\n        let (frame, _) = BlockFrame::read_from(frame_buf).unwrap().unwrap();\\n        assert!(\\n            frame.flags.has_summary(),\\n            \\\"HAS_SUMMARY flag should be set on the code block\\\"\\n        );\\n    }\\n\\n    #[test]\\n    fn with_priority_appends_annotation_block() {\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"lib.rs\\\", b\\\"// code\\\")\\n            .with_priority(Priority::High).unwrap()\\n            .encode()\\n            .unwrap();\\n\\n        // Parse: header + first block (CODE) + second block (ANNOTATION) + END\\n        let mut cursor = HEADER_SIZE;\\n\\n        // Block 0: CODE\\n        let (frame0, n) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\\n        assert_eq!(frame0.block_type, block_type::CODE);\\n        cursor += n;\\n\\n        // Block 1: ANNOTATION (priority)\\n        let (frame1, _) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\\n        assert_eq!(frame1.block_type, block_type::ANNOTATION);\\n\\n        // Decode the annotation body and verify it targets block 0\\n        let annotation = AnnotationBlock::decode_body(&frame1.body).unwrap();\\n        assert_eq!(annotation.target_block_id, 0);\\n        assert_eq!(annotation.kind, AnnotationKind::Priority);\\n        assert_eq!(annotation.value, vec![Priority::High.to_wire_byte()]);\\n    }\\n\\n    #[test]\\n    fn empty_encoder_returns_empty_payload_error() {\\n        let result = BcpEncoder::new().encode();\\n        assert!(matches!(result, Err(EncodeError::EmptyPayload)));\\n    }\\n\\n    #[test]\\n    fn payload_ends_with_end_sentinel() {\\n        let payload = BcpEncoder::new()\\n            .add_conversation(Role::User, b\\\"hello\\\")\\n            .encode()\\n            .unwrap();\\n\\n        assert_ends_with_end_sentinel(&payload);\\n    }\\n\\n    #[test]\\n    fn all_eleven_block_types_encode_without_error() {\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"main.rs\\\", b\\\"fn main() {}\\\")\\n            .add_conversation(Role::User, b\\\"hello\\\")\\n            .add_file_tree(\\n                \\\"/project\\\",\\n                vec![FileEntry {\\n                    name: \\\"lib.rs\\\".to_string(),\\n                    kind: FileEntryKind::File,\\n                    size: 100,\\n                    children: vec![],\\n                }],\\n            )\\n            .add_tool_result(\\\"rg\\\", Status::Ok, b\\\"found 3 matches\\\")\\n            .add_document(\\\"README\\\", b\\\"# Title\\\", FormatHint::Markdown)\\n            .add_structured_data(DataFormat::Json, b\\\"{\\\\\\\"key\\\\\\\": \\\\\\\"value\\\\\\\"}\\\")\\n            .add_diff(\\n                \\\"src/lib.rs\\\",\\n                vec![DiffHunk {\\n                    old_start: 1,\\n                    new_start: 1,\\n                    lines: b\\\"+new line\\\\n\\\".to_vec(),\\n                }],\\n            )\\n            .add_annotation(0, AnnotationKind::Tag, b\\\"important\\\")\\n            .add_embedding_ref(b\\\"vec-001\\\", &[0xAB; 32], \\\"text-embedding-3-small\\\")\\n            .add_image(MediaType::Png, \\\"screenshot\\\", b\\\"\\\\x89PNG\\\\r\\\\n\\\")\\n            .add_extension(\\\"myco\\\", \\\"custom_block\\\", b\\\"custom data\\\")\\n            .encode()\\n            .unwrap();\\n\\n        assert_starts_with_magic(&payload);\\n        assert_ends_with_end_sentinel(&payload);\\n\\n        // Verify we can walk all 12 content blocks (11 semantic + 1 annotation)\\n        let mut cursor = HEADER_SIZE;\\n        let mut block_count = 0;\\n        loop {\\n            match BlockFrame::read_from(&payload[cursor..]).unwrap() {\\n                Some((_, n)) => {\\n                    cursor += n;\\n                    block_count += 1;\\n                }\\n                None => break, // END sentinel\\n            }\\n        }\\n        assert_eq!(block_count, 11, \\\"expected 11 content blocks\\\");\\n    }\\n\\n    #[test]\\n    fn payload_byte_length_matches_calculation() {\\n        let mut enc = BcpEncoder::new();\\n        enc.add_code(Lang::Rust, \\\"x.rs\\\", b\\\"let x = 1;\\\");\\n        enc.add_conversation(Role::User, b\\\"hi\\\");\\n\\n        let payload = enc.encode().unwrap();\\n\\n        // Calculate expected size manually:\\n        // Header: 8 bytes\\n        let mut expected = HEADER_SIZE;\\n\\n        // Walk actual frames to verify\\n        let mut cursor = HEADER_SIZE;\\n        loop {\\n            let remaining = &payload[cursor..];\\n            // Try to read a frame (including END which returns None)\\n            let start = cursor;\\n            match BlockFrame::read_from(remaining).unwrap() {\\n                Some((_, n)) => {\\n                    cursor += n;\\n                    expected += n;\\n                }\\n                None => {\\n                    // END sentinel was consumed — count those bytes too\\n                    let end_bytes = payload.len() - start;\\n                    expected += end_bytes;\\n                    break;\\n                }\\n            }\\n        }\\n\\n        assert_eq!(\\n            payload.len(),\\n            expected,\\n            \\\"payload length should match header + frames + END\\\"\\n        );\\n    }\\n\\n    #[test]\\n    fn optional_fields_omitted_when_none() {\\n        // CODE block without line_range\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"x.rs\\\", b\\\"code\\\")\\n            .encode()\\n            .unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\\n            .unwrap()\\n            .unwrap();\\n\\n        // Decode the body and verify line_range is None\\n        let code = CodeBlock::decode_body(&frame.body).unwrap();\\n        assert!(code.line_range.is_none());\\n\\n        // CONVERSATION block without tool_call_id\\n        let payload = BcpEncoder::new()\\n            .add_conversation(Role::User, b\\\"msg\\\")\\n            .encode()\\n            .unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\\n            .unwrap()\\n            .unwrap();\\n\\n        let conv = ConversationBlock::decode_body(&frame.body).unwrap();\\n        assert!(conv.tool_call_id.is_none());\\n    }\\n\\n    #[test]\\n    fn code_range_includes_line_numbers() {\\n        let payload = BcpEncoder::new()\\n            .add_code_range(Lang::Rust, \\\"src/lib.rs\\\", b\\\"fn foo() {}\\\", 10, 20)\\n            .encode()\\n            .unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\\n            .unwrap()\\n            .unwrap();\\n\\n        let code = CodeBlock::decode_body(&frame.body).unwrap();\\n        assert_eq!(code.line_range, Some((10, 20)));\\n    }\\n\\n    #[test]\\n    fn conversation_tool_includes_tool_call_id() {\\n        let payload = BcpEncoder::new()\\n            .add_conversation_tool(Role::Tool, b\\\"result\\\", \\\"call_123\\\")\\n            .encode()\\n            .unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\\n            .unwrap()\\n            .unwrap();\\n\\n        let conv = ConversationBlock::decode_body(&frame.body).unwrap();\\n        assert_eq!(conv.tool_call_id.as_deref(), Some(\\\"call_123\\\"));\\n    }\\n\\n    #[test]\\n    fn summary_is_decodable_from_block_body() {\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"main.rs\\\", b\\\"fn main() {}\\\")\\n            .with_summary(\\\"Entry point for the application.\\\").unwrap()\\n            .encode()\\n            .unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\\n            .unwrap()\\n            .unwrap();\\n\\n        assert!(frame.flags.has_summary());\\n\\n        // Decode summary from the front of the body\\n        let (summary, consumed) = Summary::decode(&frame.body).unwrap();\\n        assert_eq!(summary.text, \\\"Entry point for the application.\\\");\\n\\n        // Remaining bytes should decode as a valid CodeBlock\\n        let code = CodeBlock::decode_body(&frame.body[consumed..]).unwrap();\\n        assert_eq!(code.path, \\\"main.rs\\\");\\n        assert_eq!(code.content, b\\\"fn main() {}\\\");\\n    }\\n\\n    #[test]\\n    fn rfc_example_encodes_successfully() {\\n        // Reproduces the example from RFC §12.1 / SPEC_03 §1\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"src/main.rs\\\", b\\\"fn main() { todo!() }\\\")\\n            .with_summary(\\\"Entry point: CLI setup and server startup.\\\").unwrap()\\n            .with_priority(Priority::High).unwrap()\\n            .add_conversation(Role::User, b\\\"Fix the timeout bug.\\\")\\n            .add_conversation(Role::Assistant, b\\\"I'll examine the pool config...\\\")\\n            .add_tool_result(\\\"ripgrep\\\", Status::Ok, b\\\"3 matches found.\\\")\\n            .encode()\\n            .unwrap();\\n\\n        assert_starts_with_magic(&payload);\\n        assert_ends_with_end_sentinel(&payload);\\n\\n        // Walk all frames to verify structure\\n        let mut cursor = HEADER_SIZE;\\n        let mut types = Vec::new();\\n        loop {\\n            match BlockFrame::read_from(&payload[cursor..]).unwrap() {\\n                Some((frame, n)) => {\\n                    types.push(frame.block_type);\\n                    cursor += n;\\n                }\\n                None => break,\\n            }\\n        }\\n\\n        assert_eq!(\\n            types,\\n            vec![\\n                block_type::CODE,\\n                block_type::ANNOTATION, // from with_priority\\n                block_type::CONVERSATION,\\n                block_type::CONVERSATION,\\n                block_type::TOOL_RESULT,\\n            ]\\n        );\\n    }\\n\\n    #[test]\\n    fn default_impl_matches_new() {\\n        let from_new = BcpEncoder::new();\\n        let from_default = BcpEncoder::default();\\n        assert!(from_new.blocks.is_empty());\\n        assert!(from_default.blocks.is_empty());\\n    }\\n\\n    // ── Per-block compression tests ─────────────────────────────────────\\n\\n    #[test]\\n    fn per_block_compression_sets_compressed_flag() {\\n        // Create a large, compressible block (exceeds COMPRESSION_THRESHOLD)\\n        let big_content = \\\"fn main() { println!(\\\\\\\"hello world\\\\\\\"); }\\\\n\\\".repeat(50);\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"main.rs\\\", big_content.as_bytes())\\n            .with_compression().unwrap()\\n            .encode()\\n            .unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\\n            .unwrap()\\n            .unwrap();\\n        assert!(\\n            frame.flags.is_compressed(),\\n            \\\"COMPRESSED flag should be set on a large compressible block\\\"\\n        );\\n        assert!(\\n            frame.body.len() < big_content.len(),\\n            \\\"compressed body should be smaller than original\\\"\\n        );\\n    }\\n\\n    #[test]\\n    fn small_block_not_compressed_even_when_requested() {\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"x.rs\\\", b\\\"let x = 1;\\\")\\n            .with_compression().unwrap()\\n            .encode()\\n            .unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\\n            .unwrap()\\n            .unwrap();\\n        assert!(\\n            !frame.flags.is_compressed(),\\n            \\\"small blocks should not be compressed (below threshold)\\\"\\n        );\\n    }\\n\\n    #[test]\\n    fn compress_blocks_applies_to_all() {\\n        let big_content = \\\"use std::io;\\\\n\\\".repeat(100);\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"a.rs\\\", big_content.as_bytes())\\n            .add_code(Lang::Rust, \\\"b.rs\\\", big_content.as_bytes())\\n            .compress_blocks()\\n            .encode()\\n            .unwrap();\\n\\n        let mut cursor = HEADER_SIZE;\\n        for _ in 0..2 {\\n            let (frame, n) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\\n            assert!(\\n                frame.flags.is_compressed(),\\n                \\\"all blocks should be compressed with compress_blocks()\\\"\\n            );\\n            cursor += n;\\n        }\\n    }\\n\\n    // ── Whole-payload compression tests ─────────────────────────────────\\n\\n    #[test]\\n    fn whole_payload_compression_sets_header_flag() {\\n        let big_content = \\\"pub fn hello() -> &'static str { \\\\\\\"world\\\\\\\" }\\\\n\\\".repeat(100);\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"main.rs\\\", big_content.as_bytes())\\n            .compress_payload()\\n            .encode()\\n            .unwrap();\\n\\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\\n        assert!(\\n            header.flags.is_compressed(),\\n            \\\"header COMPRESSED flag should be set for whole-payload compression\\\"\\n        );\\n    }\\n\\n    #[test]\\n    fn whole_payload_skips_per_block_compression() {\\n        // When whole-payload compression is active, individual block\\n        // COMPRESSED flags should NOT be set.\\n        let big_content = \\\"pub fn hello() -> &'static str { \\\\\\\"world\\\\\\\" }\\\\n\\\".repeat(100);\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"main.rs\\\", big_content.as_bytes())\\n            .with_compression().unwrap()\\n            .compress_payload()\\n            .encode()\\n            .unwrap();\\n\\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\\n        assert!(header.flags.is_compressed());\\n\\n        // Decompress the payload to check individual blocks\\n        let decompressed =\\n            crate::compression::decompress(&payload[HEADER_SIZE..], 16 * 1024 * 1024).unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&decompressed).unwrap().unwrap();\\n        assert!(\\n            !frame.flags.is_compressed(),\\n            \\\"per-block COMPRESSED flag should not be set when whole-payload is active\\\"\\n        );\\n    }\\n\\n    #[test]\\n    fn whole_payload_no_savings_stays_uncompressed() {\\n        // Tiny payload — zstd overhead exceeds savings\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"x.rs\\\", b\\\"x\\\")\\n            .compress_payload()\\n            .encode()\\n            .unwrap();\\n\\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\\n        assert!(\\n            !header.flags.is_compressed(),\\n            \\\"header COMPRESSED flag should NOT be set when compression yields no savings\\\"\\n        );\\n    }\\n\\n    // ── Content addressing tests ────────────────────────────────────────\\n\\n    #[test]\\n    fn content_addressing_sets_reference_flag() {\\n        let store = Arc::new(crate::MemoryContentStore::new());\\n        let payload = BcpEncoder::new()\\n            .set_content_store(store.clone())\\n            .add_code(Lang::Rust, \\\"main.rs\\\", b\\\"fn main() {}\\\")\\n            .with_content_addressing().unwrap()\\n            .encode()\\n            .unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\\n            .unwrap()\\n            .unwrap();\\n        assert!(\\n            frame.flags.is_reference(),\\n            \\\"IS_REFERENCE flag should be set on content-addressed block\\\"\\n        );\\n        assert_eq!(\\n            frame.body.len(),\\n            32,\\n            \\\"reference block body should be exactly 32 bytes (BLAKE3 hash)\\\"\\n        );\\n\\n        // The hash should resolve in the store\\n        let hash: [u8; 32] = frame.body.try_into().unwrap();\\n        assert!(store.contains(&hash));\\n    }\\n\\n    #[test]\\n    fn content_addressing_without_store_errors() {\\n        let result = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"main.rs\\\", b\\\"fn main() {}\\\")\\n            .with_content_addressing().unwrap()\\n            .encode();\\n\\n        assert!(\\n            matches!(result, Err(EncodeError::MissingContentStore)),\\n            \\\"should error when content addressing is requested without a store\\\"\\n        );\\n    }\\n\\n    #[test]\\n    fn auto_dedup_detects_duplicate_blocks() {\\n        let store = Arc::new(crate::MemoryContentStore::new());\\n        // Both blocks must have identical serialized TLV bodies (same\\n        // path + content + lang) for auto-dedup to detect a duplicate.\\n        let content = b\\\"fn main() {}\\\";\\n\\n        let payload = BcpEncoder::new()\\n            .set_content_store(store.clone())\\n            .auto_dedup()\\n            .add_code(Lang::Rust, \\\"main.rs\\\", content)\\n            .add_code(Lang::Rust, \\\"main.rs\\\", content) // identical TLV body\\n            .encode()\\n            .unwrap();\\n\\n        let mut cursor = HEADER_SIZE;\\n\\n        // First block: inline (first occurrence)\\n        let (frame0, n) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\\n        assert!(\\n            !frame0.flags.is_reference(),\\n            \\\"first occurrence should be stored inline\\\"\\n        );\\n        cursor += n;\\n\\n        // Second block: reference (duplicate)\\n        let (frame1, _) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\\n        assert!(\\n            frame1.flags.is_reference(),\\n            \\\"duplicate should become a hash reference\\\"\\n        );\\n        assert_eq!(frame1.body.len(), 32);\\n    }\\n\\n    #[test]\\n    fn auto_dedup_without_store_errors() {\\n        let result = BcpEncoder::new()\\n            .auto_dedup()\\n            .add_code(Lang::Rust, \\\"x.rs\\\", b\\\"code\\\")\\n            .encode();\\n\\n        assert!(matches!(result, Err(EncodeError::MissingContentStore)));\\n    }\\n\\n    #[test]\\n    fn reference_block_not_per_block_compressed() {\\n        // Content-addressed blocks produce 32-byte bodies which are\\n        // below the compression threshold — verify no COMPRESSED flag.\\n        let store = Arc::new(crate::MemoryContentStore::new());\\n        let big_content = \\\"fn main() { println!(\\\\\\\"hello\\\\\\\"); }\\\\n\\\".repeat(50);\\n        let payload = BcpEncoder::new()\\n            .set_content_store(store)\\n            .add_code(Lang::Rust, \\\"main.rs\\\", big_content.as_bytes())\\n            .with_content_addressing().unwrap()\\n            .with_compression().unwrap()\\n            .encode()\\n            .unwrap();\\n\\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\\n            .unwrap()\\n            .unwrap();\\n        assert!(frame.flags.is_reference());\\n        assert!(\\n            !frame.flags.is_compressed(),\\n            \\\"reference blocks should not be per-block compressed\\\"\\n        );\\n    }\\n\\n    #[test]\\n    fn content_addressing_with_whole_payload_compression() {\\n        // Reference blocks CAN be wrapped in whole-payload compression.\\n        let store = Arc::new(crate::MemoryContentStore::new());\\n        // Same path + content = identical TLV body = single store entry\\n        let content = \\\"fn main() { println!(\\\\\\\"hello\\\\\\\"); }\\\\n\\\".repeat(50);\\n\\n        let payload = BcpEncoder::new()\\n            .set_content_store(store.clone())\\n            .compress_payload()\\n            .add_code(Lang::Rust, \\\"main.rs\\\", content.as_bytes())\\n            .with_content_addressing().unwrap()\\n            .add_code(Lang::Rust, \\\"main.rs\\\", content.as_bytes())\\n            .with_content_addressing().unwrap()\\n            .encode()\\n            .unwrap();\\n\\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\\n        // The payload might or might not be compressed (two 32-byte hashes\\n        // plus framing may not compress well), but if it is, verify it's valid.\\n        if header.flags.is_compressed() {\\n            let decompressed =\\n                crate::compression::decompress(&payload[HEADER_SIZE..], 16 * 1024 * 1024).unwrap();\\n\\n            let (frame, _) = BlockFrame::read_from(&decompressed).unwrap().unwrap();\\n            assert!(frame.flags.is_reference());\\n            assert_eq!(frame.body.len(), 32);\\n        }\\n\\n        // Both blocks have identical TLV bodies → single store entry\\n        assert_eq!(\\n            store.len(),\\n            1,\\n            \\\"identical blocks should produce one store entry\\\"\\n        );\\n    }\\n\\n    // ── Phase 4: Cross-cutting tests ────────────────────────────────────\\n\\n    #[test]\\n    fn compression_ratio_benchmark() {\\n        // A realistic 50-line Rust file should compress by >= 20%.\\n        let rust_code = r#\\\"use std::collections::HashMap;\\nuse std::sync::Arc;\\n\\npub struct Config {\\n    pub name: String,\\n    pub values: HashMap<String, String>,\\n    pub timeout: u64,\\n}\\n\\nimpl Config {\\n    pub fn new(name: &str) -> Self {\\n        Self {\\n            name: name.to_string(),\\n            values: HashMap::new(),\\n            timeout: 30,\\n        }\\n    }\\n\\n    pub fn set(&mut self, key: &str, value: &str) {\\n        self.values.insert(key.to_string(), value.to_string());\\n    }\\n\\n    pub fn get(&self, key: &str) -> Option<&String> {\\n        self.values.get(key)\\n    }\\n\\n    pub fn timeout(&self) -> u64 {\\n        self.timeout\\n    }\\n}\\n\\nimpl Default for Config {\\n    fn default() -> Self {\\n        Self::new(\\\"default\\\")\\n    }\\n}\\n\\n#[cfg(test)]\\nmod tests {\\n    use super::*;\\n\\n    #[test]\\n    fn test_new_config() {\\n        let config = Config::new(\\\"test\\\");\\n        assert_eq!(config.name, \\\"test\\\");\\n        assert!(config.values.is_empty());\\n        assert_eq!(config.timeout(), 30);\\n    }\\n\\n    #[test]\\n    fn test_set_and_get() {\\n        let mut config = Config::new(\\\"test\\\");\\n        config.set(\\\"key\\\", \\\"value\\\");\\n        assert_eq!(config.get(\\\"key\\\"), Some(&\\\"value\\\".to_string()));\\n    }\\n}\\n\\\"#;\\n\\n        let uncompressed_payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"config.rs\\\", rust_code.as_bytes())\\n            .encode()\\n            .unwrap();\\n\\n        let compressed_payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"config.rs\\\", rust_code.as_bytes())\\n            .with_compression().unwrap()\\n            .encode()\\n            .unwrap();\\n\\n        let savings_pct =\\n            100.0 * (1.0 - compressed_payload.len() as f64 / uncompressed_payload.len() as f64);\\n\\n        assert!(\\n            savings_pct >= 20.0,\\n            \\\"expected >= 20% compression savings on a 50-line Rust file, got {savings_pct:.1}%\\\"\\n        );\\n    }\\n\\n    #[test]\\n    fn whole_payload_wins_over_per_block() {\\n        // When both per-block and whole-payload compression are requested,\\n        // only the header COMPRESSED flag should be set; individual blocks\\n        // should NOT have their COMPRESSED flags set.\\n        let big_content = \\\"pub fn process() -> Result<(), Error> { Ok(()) }\\\\n\\\".repeat(50);\\n        let payload = BcpEncoder::new()\\n            .add_code(Lang::Rust, \\\"a.rs\\\", big_content.as_bytes())\\n            .with_compression().unwrap()\\n            .add_code(Lang::Rust, \\\"b.rs\\\", big_content.as_bytes())\\n            .with_compression().unwrap()\\n            .compress_payload()\\n            .encode()\\n            .unwrap();\\n\\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\\n        assert!(\\n            header.flags.is_compressed(),\\n            \\\"header should have COMPRESSED flag\\\"\\n        );\\n\\n        // Decompress payload to inspect individual blocks\\n        let decompressed =\\n            crate::compression::decompress(&payload[HEADER_SIZE..], 16 * 1024 * 1024).unwrap();\\n\\n        let mut cursor = 0;\\n        while let Some((frame, n)) = BlockFrame::read_from(&decompressed[cursor..]).unwrap() {\\n            assert!(\\n                !frame.flags.is_compressed(),\\n                \\\"individual blocks should NOT be compressed when whole-payload is active\\\"\\n            );\\n            cursor += n;\\n        }\\n    }\\n\\n    #[test]\\n    fn full_pipeline_encode_decode_roundtrip() {\\n        // Exercises all features together: multiple block types,\\n        // summaries, priorities, per-block compression, content\\n        // addressing, and auto-dedup.\\n        let store = Arc::new(crate::MemoryContentStore::new());\\n        let big_code = \\\"fn compute() -> i64 { 42 }\\\\n\\\".repeat(50);\\n\\n        let payload = BcpEncoder::new()\\n            .set_content_store(store.clone())\\n            .auto_dedup()\\n            .add_code(Lang::Rust, \\\"lib.rs\\\", big_code.as_bytes())\\n            .with_summary(\\\"Core computation module.\\\").unwrap()\\n            .with_compression().unwrap()\\n            .add_code(Lang::Rust, \\\"lib.rs\\\", big_code.as_bytes()) // auto-dedup\\n            .add_conversation(Role::User, b\\\"Review this code\\\")\\n            .add_tool_result(\\\"clippy\\\", Status::Ok, b\\\"No warnings\\\")\\n            .encode()\\n            .unwrap();\\n\\n        // Decode with the same store\\n        let decoded = bcp_decoder::BcpDecoder::decode_with_store(&payload, store.as_ref()).unwrap();\\n\\n        assert_eq!(decoded.blocks.len(), 4);\\n        assert_eq!(decoded.blocks[0].block_type, bcp_types::BlockType::Code);\\n        assert_eq!(\\n            decoded.blocks[0].summary.as_ref().unwrap().text,\\n            \\\"Core computation module.\\\"\\n        );\\n        assert_eq!(decoded.blocks[1].block_type, bcp_types::BlockType::Code);\\n        assert_eq!(\\n            decoded.blocks[2].block_type,\\n            bcp_types::BlockType::Conversation\\n        );\\n        assert_eq!(\\n            decoded.blocks[3].block_type,\\n            bcp_types::BlockType::ToolResult\\n        );\\n\\n        // Both code blocks should have the same content\\n        for block in &decoded.blocks[..2] {\\n            match &block.content {\\n                BlockContent::Code(code) => {\\n                    assert_eq!(code.content, big_code.as_bytes());\\n                }\\n                other => panic!(\\\"expected Code, got {other:?}\\\"),\\n            }\\n        }\\n    }\\n}\\n\",\n      \"language\": \"rust\",\n      \"path\": \"encoder.rs\",\n      \"type\": \"code\"\n    },\n    {\n      \"content\": \"use bcp_wire::WireError;\\n\\n/// Errors specific to zstd compression and decompression.\\n///\\n/// These are surfaced when per-block or whole-payload compression\\n/// is enabled and the zstd codec encounters an issue, or when\\n/// decompressed output exceeds safety limits.\\n///\\n/// ```text\\n///   CompressionError\\n///   ├── CompressFailed      ← zstd encoder returned an error\\n///   ├── DecompressFailed    ← zstd decoder returned an error\\n///   └── DecompressionBomb   ← decompressed size exceeds safety limit\\n/// ```\\n#[derive(Debug, thiserror::Error)]\\npub enum CompressionError {\\n    #[error(\\\"zstd compression failed: {0}\\\")]\\n    CompressFailed(String),\\n\\n    #[error(\\\"zstd decompression failed: {0}\\\")]\\n    DecompressFailed(String),\\n\\n    #[error(\\\"decompressed size {actual} exceeds limit {limit}\\\")]\\n    DecompressionBomb { actual: usize, limit: usize },\\n}\\n\\n/// Errors that can occur during BCP payload encoding.\\n///\\n/// The encoder validates structural constraints (non-empty payload,\\n/// block size limits, summary targeting) and propagates lower-level\\n/// wire, I/O, and compression errors from the serialization layer.\\n///\\n/// Error hierarchy:\\n///\\n/// ```text\\n///   EncodeError\\n///   ├── EmptyPayload         ← no blocks were added before .encode()\\n///   ├── BlockTooLarge        ← single block body exceeds size limit\\n///   ├── NoBlockTarget        ← modifier called with no preceding block\\n///   ├── MissingContentStore  ← content addressing enabled without a store\\n///   ├── Compression(…)       ← from zstd compress/decompress\\n///   ├── Wire(WireError)      ← from bcp-wire serialization\\n///   └── Io(std::io::Error)   ← from underlying I/O writes\\n/// ```\\n#[derive(Debug, thiserror::Error)]\\npub enum EncodeError {\\n    #[error(\\\"no blocks have been added to the encoder\\\")]\\n    EmptyPayload,\\n\\n    #[error(\\\"block body exceeds maximum size ({size} bytes, limit {limit})\\\")]\\n    BlockTooLarge { size: usize, limit: usize },\\n\\n    #[error(\\\"{method} called but no blocks have been added yet\\\")]\\n    NoBlockTarget { method: &'static str },\\n\\n    #[error(\\\"content addressing requires a content store (call set_content_store first)\\\")]\\n    MissingContentStore,\\n\\n    #[error(transparent)]\\n    Compression(#[from] CompressionError),\\n\\n    #[error(transparent)]\\n    Wire(#[from] WireError),\\n\\n    #[error(transparent)]\\n    Io(#[from] std::io::Error),\\n}\\n\",\n      \"language\": \"rust\",\n      \"path\": \"error.rs\",\n      \"type\": \"code\"\n    },\n    {\n      \"content\": \"I've loaded 5 source files from ./crates/bcp-encoder/src. Can you analyze the architecture?\",\n      \"role\": \"user\",\n      \"type\": \"conversation\"\n    },\n    {\n      \"content\": \"I'll review the 5 files. Let me start by understanding the module structure and key types.\",\n      \"role\": \"assistant\",\n      \"type\": \"conversation\"\n    },\n    {\n      \"content\": \"Found 5 source files in ./crates/bcp-encoder/src\",\n      \"status\": \"ok\",\n      \"tool_name\": \"ripgrep\",\n      \"type\": \"tool_result\"\n    }\n  ],\n  \"description\": \"Dir scan of ./crates/bcp-encoder/src (5 files)\"\n}",
      "language": "json",
      "path": "bcp-bench-real/fixtures/real_session_medium.json",
      "type": "code"
    },
    {
      "content": "{\n  \"description\": \"Small session: 3 code files, 2 conversation turns, 1 tool result\",\n  \"blocks\": [\n    {\n      \"type\": \"code\",\n      \"language\": \"rust\",\n      \"path\": \"src/main.rs\",\n      \"content\": \"use std::env;\\nuse anyhow::Result;\\n\\nmod config;\\nmod server;\\n\\nfn main() -> Result<()> {\\n    let args: Vec<String> = env::args().collect();\\n    let config = config::Config::from_args(&args)?;\\n    server::run(config)\\n}\\n\",\n      \"summary\": \"Entry point: CLI arg parsing, config loading, server startup.\"\n    },\n    {\n      \"type\": \"code\",\n      \"language\": \"rust\",\n      \"path\": \"src/config.rs\",\n      \"content\": \"use anyhow::{Result, bail};\\n\\npub struct Config {\\n    pub host: String,\\n    pub port: u16,\\n    pub timeout_ms: u64,\\n    pub max_connections: usize,\\n}\\n\\nimpl Config {\\n    pub fn from_args(args: &[String]) -> Result<Self> {\\n        if args.len() < 2 {\\n            bail!(\\\"Usage: server <port>\\\");\\n        }\\n        let port: u16 = args[1].parse()?;\\n        Ok(Self {\\n            host: \\\"0.0.0.0\\\".into(),\\n            port,\\n            timeout_ms: 5000,\\n            max_connections: 100,\\n        })\\n    }\\n}\\n\",\n      \"summary\": \"Config struct with CLI arg parsing.\"\n    },\n    {\n      \"type\": \"code\",\n      \"language\": \"rust\",\n      \"path\": \"src/server.rs\",\n      \"content\": \"use std::net::TcpListener;\\nuse std::io::{Read, Write};\\nuse anyhow::Result;\\n\\nuse crate::config::Config;\\n\\npub fn run(config: Config) -> Result<()> {\\n    let addr = format!(\\\"{}:{}\\\", config.host, config.port);\\n    let listener = TcpListener::bind(&addr)?;\\n    println!(\\\"Listening on {addr}\\\");\\n\\n    for stream in listener.incoming() {\\n        let mut stream = stream?;\\n        let mut buf = [0u8; 1024];\\n        let n = stream.read(&mut buf)?;\\n        let request = String::from_utf8_lossy(&buf[..n]);\\n        println!(\\\"Request: {}\\\", request.lines().next().unwrap_or(\\\"\\\"));\\n\\n        let response = \\\"HTTP/1.1 200 OK\\\\r\\\\nContent-Length: 2\\\\r\\\\n\\\\r\\\\nOK\\\";\\n        stream.write_all(response.as_bytes())?;\\n    }\\n    Ok(())\\n}\\n\"\n    },\n    {\n      \"type\": \"conversation\",\n      \"role\": \"user\",\n      \"content\": \"The server is dropping connections after 5 seconds. I think it's the timeout configuration but I'm not sure where it's applied. Can you trace through the code and find where the timeout_ms config value is actually used?\"\n    },\n    {\n      \"type\": \"conversation\",\n      \"role\": \"assistant\",\n      \"content\": \"Looking at the code, I can see the issue. The `Config` struct defines `timeout_ms: 5000` in `src/config.rs`, but the `server::run()` function in `src/server.rs` never actually reads or applies this timeout value. The TCP listener is using default OS socket timeouts instead.\\n\\nTo fix this, you'll need to set the read/write timeouts on each accepted `TcpStream` using `stream.set_read_timeout()` and `stream.set_write_timeout()` with the configured duration.\"\n    },\n    {\n      \"type\": \"tool_result\",\n      \"tool_name\": \"ripgrep\",\n      \"status\": \"ok\",\n      \"content\": \"src/config.rs:5:    pub timeout_ms: u64,\\nsrc/config.rs:15:            timeout_ms: 5000,\\n\\nNo other references to timeout_ms found in the codebase.\",\n      \"summary\": \"2 matches for timeout_ms, both in config.rs only.\"\n    }\n  ]\n}\n",
      "language": "json",
      "path": "bcp-bench-real/fixtures/real_session_small.json",
      "type": "code"
    },
    {
      "content": "use std::path::PathBuf;\n\nuse anyhow::Result;\nuse bcp_bench_real::fixture::encode_fixture;\nuse bcp_bench_real::markdown::build_realistic_markdown;\nuse bcp_bench_real::token_counter::TokenCounter;\nuse bcp_decoder::BcpDecoder;\nuse bcp_driver::{BcpDriver, DefaultDriver, DriverConfig, OutputMode};\nuse bcp_types::block::Block;\nuse bcp_types::BlockType;\nuse clap::Parser;\n\n#[derive(Parser)]\n#[command(name = \"bench_tokens\", about = \"BCP token savings benchmark (cl100k_base)\")]\nstruct Args {\n    /// Path to a session fixture JSON file.\n    #[arg(default_value = \"fixtures/real_session_medium.json\")]\n    fixture: PathBuf,\n}\n\nfn main() -> Result<()> {\n    let args = Args::parse();\n    let counter = TokenCounter::new()?;\n\n    let payload = encode_fixture(&args.fixture)?;\n    let decoded = BcpDecoder::decode(&payload)?;\n\n    let mut results: Vec<(&str, usize)> = Vec::new();\n\n    for (label, mode) in [\n        (\"BCP XML\", OutputMode::Xml),\n        (\"BCP Markdown\", OutputMode::Markdown),\n        (\"BCP Minimal\", OutputMode::Minimal),\n    ] {\n        let config = DriverConfig {\n            mode,\n            ..Default::default()\n        };\n        let rendered = DefaultDriver.render(&decoded.blocks, &config)?;\n        let tokens = counter.count(&rendered);\n        results.push((label, tokens));\n    }\n\n    let naive_md = build_naive_markdown(&decoded.blocks);\n    let naive_tokens = counter.count(&naive_md);\n    results.push((\"Raw MD (naive)\", naive_tokens));\n\n    let realistic_md = build_realistic_markdown(&decoded.blocks);\n    let realistic_tokens = counter.count(&realistic_md);\n    results.push((\"Raw MD (agent)\", realistic_tokens));\n\n    println!();\n    println!(\n        \"\\u{2554}{:\\u{2550}<22}\\u{2566}{:\\u{2550}<9}\\u{2566}{:\\u{2550}<19}\\u{2566}{:\\u{2550}<19}\\u{2557}\",\n        \"\", \"\", \"\", \"\"\n    );\n    println!(\n        \"\\u{2551} {:<20} \\u{2551} {:>7} \\u{2551} {:>17} \\u{2551} {:>17} \\u{2551}\",\n        \"Format\", \"Tokens\", \"vs Naive MD\", \"vs Agent MD\"\n    );\n    println!(\n        \"\\u{2560}{:\\u{2550}<22}\\u{256C}{:\\u{2550}<9}\\u{256C}{:\\u{2550}<19}\\u{256C}{:\\u{2550}<19}\\u{2563}\",\n        \"\", \"\", \"\", \"\"\n    );\n\n    for (label, tokens) in &results {\n        let is_baseline =\n            *label == \"Raw MD (naive)\" || *label == \"Raw MD (agent)\";\n        let vs_naive = if is_baseline {\n            \"\\u{2014}\".into()\n        } else {\n            let pct = savings_pct(*tokens, naive_tokens);\n            format!(\"{pct:+.1}%\")\n        };\n        let vs_realistic = if is_baseline {\n            \"\\u{2014}\".into()\n        } else {\n            let pct = savings_pct(*tokens, realistic_tokens);\n            format!(\"{pct:+.1}%\")\n        };\n        println!(\n            \"\\u{2551} {label:<20} \\u{2551} {tokens:>7} \\u{2551} {vs_naive:>17} \\u{2551} {vs_realistic:>17} \\u{2551}\"\n        );\n    }\n\n    println!(\n        \"\\u{255A}{:\\u{2550}<22}\\u{2569}{:\\u{2550}<9}\\u{2569}{:\\u{2550}<19}\\u{2569}{:\\u{2550}<19}\\u{255D}\",\n        \"\", \"\", \"\", \"\"\n    );\n\n    println!();\n    print_per_block_breakdown(&decoded.blocks, &counter)?;\n\n    println!();\n    println!(\"BCP wire size:     {} bytes\", payload.len());\n    println!(\"Naive MD size:     {} bytes\", naive_md.len());\n    println!(\"Agent MD size:     {} bytes\", realistic_md.len());\n    println!(\n        \"Wire compression:  {:.1}% vs naive MD bytes\",\n        savings_pct(payload.len(), naive_md.len())\n    );\n\n    println!();\n    print_rfc_table(&decoded.blocks, &counter)?;\n\n    Ok(())\n}\n\nfn savings_pct(actual: usize, baseline: usize) -> f64 {\n    if baseline == 0 {\n        return 0.0;\n    }\n    (1.0 - actual as f64 / baseline as f64) * 100.0\n}\n\nfn print_per_block_breakdown(blocks: &[Block], counter: &TokenCounter) -> Result<()> {\n    println!(\"Per-Block-Type Overhead Breakdown:\");\n    println!(\n        \"\\u{250C}{:\\u{2500}<20}\\u{252C}{:\\u{2500}<12}\\u{252C}{:\\u{2500}<12}\\u{252C}{:\\u{2500}<10}\\u{2510}\",\n        \"\", \"\", \"\", \"\"\n    );\n    println!(\n        \"\\u{2502} {:<18} \\u{2502} {:>10} \\u{2502} {:>10} \\u{2502} {:>8} \\u{2502}\",\n        \"Block Type\", \"BCP Min.\", \"Naive MD\", \"Savings\"\n    );\n    println!(\n        \"\\u{251C}{:\\u{2500}<20}\\u{253C}{:\\u{2500}<12}\\u{253C}{:\\u{2500}<12}\\u{253C}{:\\u{2500}<10}\\u{2524}\",\n        \"\", \"\", \"\", \"\"\n    );\n\n    let type_groups: &[(BlockType, &str)] = &[\n        (BlockType::Code, \"Code\"),\n        (BlockType::Conversation, \"Conversation\"),\n        (BlockType::ToolResult, \"ToolResult\"),\n        (BlockType::FileTree, \"FileTree\"),\n        (BlockType::Document, \"Document\"),\n        (BlockType::StructuredData, \"StructuredData\"),\n        (BlockType::Diff, \"Diff\"),\n    ];\n\n    for (bt, label) in type_groups {\n        let group: Vec<&Block> = blocks\n            .iter()\n            .filter(|b| b.block_type == *bt)\n            .collect();\n        if group.is_empty() {\n            continue;\n        }\n\n        let owned_blocks: Vec<Block> = group.into_iter().cloned().collect();\n\n        let config = DriverConfig {\n            mode: OutputMode::Minimal,\n            ..Default::default()\n        };\n        let bcp_out = DefaultDriver.render(&owned_blocks, &config)?;\n        let bcp_tokens = counter.count(&bcp_out);\n\n        let naive_out = build_naive_markdown(&owned_blocks);\n        let naive_tokens = counter.count(&naive_out);\n\n        let pct = savings_pct(bcp_tokens, naive_tokens);\n        println!(\n            \"\\u{2502} {label:<18} \\u{2502} {bcp_tokens:>10} \\u{2502} {naive_tokens:>10} \\u{2502} {pct:>7.1}% \\u{2502}\"\n        );\n    }\n\n    println!(\n        \"\\u{2514}{:\\u{2500}<20}\\u{2534}{:\\u{2500}<12}\\u{2534}{:\\u{2500}<12}\\u{2534}{:\\u{2500}<10}\\u{2518}\",\n        \"\", \"\", \"\", \"\"\n    );\n    Ok(())\n}\n\nfn build_naive_markdown(blocks: &[Block]) -> String {\n    bcp_bench_real::markdown::build_naive_markdown(blocks)\n}\n\nfn print_rfc_table(blocks: &[Block], counter: &TokenCounter) -> Result<()> {\n    println!(\"RFC \\u{00A7}6 Replacement Data (cl100k_base tokenizer):\");\n    println!();\n    println!(\"   +----------------------------+-----------+-----------+---------+\");\n    println!(\"   | Context Pattern            | Markdown  | BCP Min.  | Savings |\");\n    println!(\"   |                            | Tokens    | Tokens    |         |\");\n    println!(\"   +----------------------------+-----------+-----------+---------+\");\n\n    let type_groups: &[(BlockType, &str)] = &[\n        (BlockType::Code, \"Code blocks\"),\n        (BlockType::Conversation, \"Conversation turns\"),\n        (BlockType::ToolResult, \"Tool results\"),\n        (BlockType::FileTree, \"File trees\"),\n        (BlockType::Document, \"Documents\"),\n        (BlockType::StructuredData, \"Structured data\"),\n        (BlockType::Diff, \"Diffs\"),\n    ];\n\n    let mut total_md: usize = 0;\n    let mut total_bcp: usize = 0;\n\n    for (bt, label) in type_groups {\n        let group: Vec<Block> = blocks\n            .iter()\n            .filter(|b| b.block_type == *bt)\n            .cloned()\n            .collect();\n        if group.is_empty() {\n            continue;\n        }\n\n        let config = DriverConfig {\n            mode: OutputMode::Minimal,\n            ..Default::default()\n        };\n        let bcp_out = DefaultDriver.render(&group, &config)?;\n        let bcp_tokens = counter.count(&bcp_out);\n\n        let naive_out = bcp_bench_real::markdown::build_naive_markdown(&group);\n        let naive_tokens = counter.count(&naive_out);\n\n        total_md += naive_tokens;\n        total_bcp += bcp_tokens;\n\n        let pct = savings_pct(bcp_tokens, naive_tokens);\n        println!(\n            \"   | {label:<26} | {naive_tokens:>9} | {bcp_tokens:>9} | {pct:>6.0}% |\"\n        );\n    }\n\n    println!(\"   +----------------------------+-----------+-----------+---------+\");\n    let total_pct = savings_pct(total_bcp, total_md);\n    println!(\n        \"   | {:<26} | {:>9} | {:>9} | {:>6.1}% |\",\n        \"TOTAL\", total_md, total_bcp, total_pct\n    );\n    println!(\"   +----------------------------+-----------+-----------+---------+\");\n    Ok(())\n}\n",
      "language": "rust",
      "path": "bcp-bench-real/src/bin/bench_tokens.rs",
      "type": "code"
    },
    {
      "content": "use std::path::PathBuf;\n\nuse anyhow::{Context, Result};\nuse clap::{Parser, ValueEnum};\n\n#[derive(Parser)]\n#[command(name = \"capture_session\", about = \"Generate BCP benchmark fixture JSON\")]\nstruct Args {\n    /// Capture mode.\n    #[arg(long, value_enum, default_value = \"dir-scan\")]\n    mode: CaptureMode,\n\n    /// Source path (directory for dir-scan, JSON file for transcript).\n    #[arg(long)]\n    path: PathBuf,\n\n    /// Maximum number of files to include (dir-scan mode only).\n    #[arg(long, default_value = \"15\")]\n    max_files: usize,\n\n    /// Output fixture JSON path.\n    #[arg(long, short)]\n    output: PathBuf,\n}\n\n#[derive(Clone, ValueEnum)]\nenum CaptureMode {\n    DirScan,\n    Transcript,\n}\n\nfn main() -> Result<()> {\n    let args = Args::parse();\n\n    match args.mode {\n        CaptureMode::DirScan => capture_dir_scan(&args.path, args.max_files, &args.output),\n        CaptureMode::Transcript => capture_transcript(&args.path, &args.output),\n    }\n}\n\nfn capture_dir_scan(dir: &PathBuf, max_files: usize, output: &PathBuf) -> Result<()> {\n    let mut blocks = Vec::new();\n    let mut file_count = 0;\n\n    let walker = walkdir(dir, max_files)?;\n\n    for (path, content) in &walker {\n        let lang = detect_lang(path);\n        let rel_path = path\n            .strip_prefix(dir)\n            .unwrap_or(path)\n            .to_string_lossy()\n            .to_string();\n\n        blocks.push(serde_json::json!({\n            \"type\": \"code\",\n            \"language\": lang,\n            \"path\": rel_path,\n            \"content\": content,\n        }));\n        file_count += 1;\n    }\n\n    // Add synthetic conversation turns for realism\n    blocks.push(serde_json::json!({\n        \"type\": \"conversation\",\n        \"role\": \"user\",\n        \"content\": format!(\"I've loaded {file_count} source files from {}. Can you analyze the architecture?\", dir.display()),\n    }));\n    blocks.push(serde_json::json!({\n        \"type\": \"conversation\",\n        \"role\": \"assistant\",\n        \"content\": format!(\"I'll review the {file_count} files. Let me start by understanding the module structure and key types.\"),\n    }));\n\n    // Add a synthetic tool result\n    blocks.push(serde_json::json!({\n        \"type\": \"tool_result\",\n        \"tool_name\": \"ripgrep\",\n        \"status\": \"ok\",\n        \"content\": format!(\"Found {file_count} source files in {}\", dir.display()),\n    }));\n\n    let fixture = serde_json::json!({\n        \"description\": format!(\"Dir scan of {} ({file_count} files)\", dir.display()),\n        \"blocks\": blocks,\n    });\n\n    let json = serde_json::to_string_pretty(&fixture)?;\n    if let Some(parent) = output.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    std::fs::write(output, json)?;\n\n    println!(\"Wrote fixture: {} ({file_count} code blocks + 3 synthetic blocks)\", output.display());\n    Ok(())\n}\n\nfn capture_transcript(input: &PathBuf, output: &PathBuf) -> Result<()> {\n    let json = std::fs::read_to_string(input)\n        .with_context(|| format!(\"reading {}\", input.display()))?;\n\n    // Validate it parses as a fixture\n    let _: serde_json::Value = serde_json::from_str(&json).context(\"invalid JSON\")?;\n\n    if let Some(parent) = output.parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n    std::fs::write(output, &json)?;\n    println!(\"Wrote fixture: {}\", output.display());\n    Ok(())\n}\n\nfn walkdir(dir: &PathBuf, max_files: usize) -> Result<Vec<(PathBuf, String)>> {\n    let mut results = Vec::new();\n    collect_files(dir, &mut results, max_files)?;\n    Ok(results)\n}\n\nfn collect_files(\n    dir: &PathBuf,\n    results: &mut Vec<(PathBuf, String)>,\n    max_files: usize,\n) -> Result<()> {\n    if results.len() >= max_files {\n        return Ok(());\n    }\n\n    let mut entries: Vec<_> = std::fs::read_dir(dir)?\n        .filter_map(|e| e.ok())\n        .collect();\n    entries.sort_by_key(|e| e.file_name());\n\n    for entry in entries {\n        if results.len() >= max_files {\n            break;\n        }\n        let path = entry.path();\n        if path.is_dir() {\n            let name = path.file_name().unwrap_or_default().to_string_lossy();\n            if name.starts_with('.') || name == \"target\" || name == \"node_modules\" {\n                continue;\n            }\n            collect_files(&path, results, max_files)?;\n        } else if is_source_file(&path) {\n            if let Ok(content) = std::fs::read_to_string(&path) {\n                results.push((path, content));\n            }\n        }\n    }\n    Ok(())\n}\n\nfn is_source_file(path: &PathBuf) -> bool {\n    let ext = path\n        .extension()\n        .unwrap_or_default()\n        .to_string_lossy()\n        .to_lowercase();\n    matches!(\n        ext.as_str(),\n        \"rs\" | \"ts\" | \"tsx\" | \"js\" | \"jsx\" | \"py\" | \"go\" | \"java\" | \"c\" | \"cpp\" | \"h\"\n            | \"rb\" | \"sh\" | \"sql\" | \"html\" | \"css\" | \"json\" | \"yaml\" | \"yml\" | \"toml\" | \"md\"\n    )\n}\n\nfn detect_lang(path: &PathBuf) -> &'static str {\n    let ext = path\n        .extension()\n        .unwrap_or_default()\n        .to_string_lossy()\n        .to_lowercase();\n    match ext.as_str() {\n        \"rs\" => \"rust\",\n        \"ts\" | \"tsx\" => \"typescript\",\n        \"js\" | \"jsx\" => \"javascript\",\n        \"py\" => \"python\",\n        \"go\" => \"go\",\n        \"java\" => \"java\",\n        \"c\" | \"h\" => \"c\",\n        \"cpp\" => \"cpp\",\n        \"rb\" => \"ruby\",\n        \"sh\" => \"shell\",\n        \"sql\" => \"sql\",\n        \"html\" => \"html\",\n        \"css\" => \"css\",\n        \"json\" => \"json\",\n        \"yaml\" | \"yml\" => \"yaml\",\n        \"toml\" => \"toml\",\n        \"md\" => \"markdown\",\n        _ => \"text\",\n    }\n}\n",
      "language": "rust",
      "path": "bcp-bench-real/src/bin/capture_session.rs",
      "type": "code"
    },
    {
      "content": "use std::path::Path;\n\nuse anyhow::{Context, Result};\nuse bcp_encoder::BcpEncoder;\nuse bcp_types::enums::{DataFormat, FormatHint, Lang, Role, Status};\nuse bcp_types::file_tree::{FileEntry, FileEntryKind};\nuse serde::Deserialize;\n\n#[derive(Deserialize)]\npub struct SessionFixture {\n    #[allow(dead_code)]\n    pub description: String,\n    pub blocks: Vec<FixtureBlock>,\n}\n\n#[derive(Deserialize)]\n#[serde(tag = \"type\", rename_all = \"snake_case\")]\npub enum FixtureBlock {\n    Code {\n        language: String,\n        path: String,\n        content: String,\n        #[serde(default)]\n        summary: Option<String>,\n    },\n    Conversation {\n        role: String,\n        content: String,\n    },\n    ToolResult {\n        tool_name: String,\n        status: String,\n        content: String,\n        #[serde(default)]\n        summary: Option<String>,\n    },\n    FileTree {\n        root_path: String,\n        entries: Vec<FixtureEntry>,\n    },\n    Document {\n        title: String,\n        content: String,\n        #[serde(default)]\n        format_hint: Option<String>,\n    },\n    StructuredData {\n        format: String,\n        content: String,\n    },\n    Diff {\n        path: String,\n        hunks: Vec<FixtureHunk>,\n    },\n}\n\n#[derive(Deserialize)]\npub struct FixtureEntry {\n    pub name: String,\n    pub kind: String,\n    #[serde(default)]\n    pub size: u64,\n    #[serde(default)]\n    pub children: Vec<FixtureEntry>,\n}\n\n#[derive(Deserialize)]\npub struct FixtureHunk {\n    pub old_start: u32,\n    pub new_start: u32,\n    pub lines: String,\n}\n\npub fn encode_fixture(path: &Path) -> Result<Vec<u8>> {\n    let json = std::fs::read_to_string(path)\n        .with_context(|| format!(\"reading fixture {}\", path.display()))?;\n    let fixture: SessionFixture =\n        serde_json::from_str(&json).context(\"parsing fixture JSON\")?;\n    encode_session(&fixture)\n}\n\npub fn encode_session(fixture: &SessionFixture) -> Result<Vec<u8>> {\n    let mut encoder = BcpEncoder::new();\n\n    for block in &fixture.blocks {\n        match block {\n            FixtureBlock::Code {\n                language,\n                path,\n                content,\n                summary,\n            } => {\n                let lang = parse_lang(language);\n                encoder.add_code(lang, path, content.as_bytes());\n                if let Some(s) = summary {\n                    encoder.with_summary(s)?;\n                }\n            }\n            FixtureBlock::Conversation { role, content } => {\n                let r = parse_role(role);\n                encoder.add_conversation(r, content.as_bytes());\n            }\n            FixtureBlock::ToolResult {\n                tool_name,\n                status,\n                content,\n                summary,\n            } => {\n                let s = parse_status(status);\n                encoder.add_tool_result(tool_name, s, content.as_bytes());\n                if let Some(sm) = summary {\n                    encoder.with_summary(sm)?;\n                }\n            }\n            FixtureBlock::FileTree { root_path, entries } => {\n                let converted = entries.iter().map(convert_entry).collect();\n                encoder.add_file_tree(root_path, converted);\n            }\n            FixtureBlock::Document {\n                title,\n                content,\n                format_hint,\n            } => {\n                let hint = format_hint\n                    .as_deref()\n                    .map_or(FormatHint::Plain, parse_format_hint);\n                encoder.add_document(title, content.as_bytes(), hint);\n            }\n            FixtureBlock::StructuredData { format, content } => {\n                let fmt = parse_data_format(format);\n                encoder.add_structured_data(fmt, content.as_bytes());\n            }\n            FixtureBlock::Diff { path, hunks } => {\n                let converted = hunks\n                    .iter()\n                    .map(|h| bcp_types::diff::DiffHunk {\n                        old_start: h.old_start,\n                        new_start: h.new_start,\n                        lines: h.lines.as_bytes().to_vec(),\n                    })\n                    .collect();\n                encoder.add_diff(path, converted);\n            }\n        }\n    }\n\n    Ok(encoder.encode()?)\n}\n\nfn convert_entry(e: &FixtureEntry) -> FileEntry {\n    let kind = if e.kind == \"dir\" || e.kind == \"directory\" {\n        FileEntryKind::Directory\n    } else {\n        FileEntryKind::File\n    };\n    FileEntry {\n        name: e.name.clone(),\n        kind,\n        size: e.size,\n        children: e.children.iter().map(convert_entry).collect(),\n    }\n}\n\nfn parse_lang(s: &str) -> Lang {\n    match s.to_lowercase().as_str() {\n        \"rust\" | \"rs\" => Lang::Rust,\n        \"typescript\" | \"ts\" => Lang::TypeScript,\n        \"javascript\" | \"js\" => Lang::JavaScript,\n        \"python\" | \"py\" => Lang::Python,\n        \"go\" => Lang::Go,\n        \"java\" => Lang::Java,\n        \"c\" => Lang::C,\n        \"cpp\" | \"c++\" => Lang::Cpp,\n        \"ruby\" | \"rb\" => Lang::Ruby,\n        \"shell\" | \"bash\" | \"sh\" => Lang::Shell,\n        \"sql\" => Lang::Sql,\n        \"html\" => Lang::Html,\n        \"css\" => Lang::Css,\n        \"json\" => Lang::Json,\n        \"yaml\" | \"yml\" => Lang::Yaml,\n        \"toml\" => Lang::Toml,\n        \"markdown\" | \"md\" => Lang::Markdown,\n        _ => Lang::Unknown,\n    }\n}\n\nfn parse_role(s: &str) -> Role {\n    match s.to_lowercase().as_str() {\n        \"system\" => Role::System,\n        \"user\" => Role::User,\n        \"assistant\" => Role::Assistant,\n        \"tool\" => Role::Tool,\n        _ => Role::User,\n    }\n}\n\nfn parse_status(s: &str) -> Status {\n    match s.to_lowercase().as_str() {\n        \"ok\" | \"success\" => Status::Ok,\n        \"error\" | \"fail\" | \"failed\" => Status::Error,\n        \"timeout\" => Status::Timeout,\n        _ => Status::Ok,\n    }\n}\n\nfn parse_format_hint(s: &str) -> FormatHint {\n    match s.to_lowercase().as_str() {\n        \"markdown\" | \"md\" => FormatHint::Markdown,\n        \"html\" => FormatHint::Html,\n        _ => FormatHint::Plain,\n    }\n}\n\nfn parse_data_format(s: &str) -> DataFormat {\n    match s.to_lowercase().as_str() {\n        \"json\" => DataFormat::Json,\n        \"yaml\" | \"yml\" => DataFormat::Yaml,\n        \"toml\" => DataFormat::Toml,\n        \"csv\" => DataFormat::Csv,\n        _ => DataFormat::Json,\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn parse_lang_known_variants() {\n        assert_eq!(parse_lang(\"rust\"), Lang::Rust);\n        assert_eq!(parse_lang(\"TypeScript\"), Lang::TypeScript);\n        assert_eq!(parse_lang(\"py\"), Lang::Python);\n        assert_eq!(parse_lang(\"unknown_lang\"), Lang::Unknown);\n    }\n\n    #[test]\n    fn parse_role_known_variants() {\n        assert_eq!(parse_role(\"user\"), Role::User);\n        assert_eq!(parse_role(\"Assistant\"), Role::Assistant);\n    }\n\n    #[test]\n    fn encode_minimal_fixture() {\n        let fixture = SessionFixture {\n            description: \"test\".into(),\n            blocks: vec![FixtureBlock::Conversation {\n                role: \"user\".into(),\n                content: \"hello\".into(),\n            }],\n        };\n        let payload = encode_session(&fixture).unwrap();\n        assert!(payload.len() > 8);\n    }\n}\n",
      "language": "rust",
      "path": "bcp-bench-real/src/fixture.rs",
      "type": "code"
    },
    {
      "content": "#![warn(clippy::pedantic)]\n\npub mod fixture;\npub mod markdown;\npub mod token_counter;\n",
      "language": "rust",
      "path": "bcp-bench-real/src/lib.rs",
      "type": "code"
    },
    {
      "content": "use bcp_types::block::{Block, BlockContent};\n\n/// Build naive markdown equivalent — triple backticks, ### headers.\n///\n/// Represents how most tools dump context into a prompt:\n/// fenced code blocks, role headers, bullet-list tool results.\n/// This is the \"strawman\" baseline — easy to beat.\npub fn build_naive_markdown(blocks: &[Block]) -> String {\n    let mut out = String::new();\n\n    for block in blocks {\n        match &block.content {\n            BlockContent::Code(c) => {\n                let lang = lang_name(c.lang);\n                out.push_str(&format!(\"```{lang}\\n// {}\\n\", c.path));\n                out.push_str(&lossy(&c.content));\n                out.push_str(\"\\n```\\n\\n\");\n            }\n            BlockContent::Conversation(c) => {\n                let role = role_name(c.role);\n                out.push_str(&format!(\"### {role}:\\n\\n\"));\n                out.push_str(&lossy(&c.content));\n                out.push_str(\"\\n\\n\");\n            }\n            BlockContent::ToolResult(t) => {\n                let name = &t.tool_name;\n                let status = status_name(t.status);\n                out.push_str(&format!(\"### Tool Result ({name}) [{status}]:\\n\\n```\\n\"));\n                out.push_str(&lossy(&t.content));\n                out.push_str(\"\\n```\\n\\n\");\n            }\n            BlockContent::FileTree(ft) => {\n                out.push_str(&format!(\"### File Tree: {}\\n\\n```\\n\", ft.root_path));\n                render_tree_entries_naive(&mut out, &ft.entries, 0);\n                out.push_str(\"```\\n\\n\");\n            }\n            BlockContent::Document(d) => {\n                out.push_str(&format!(\"## {}\\n\\n\", d.title));\n                out.push_str(&lossy(&d.content));\n                out.push_str(\"\\n\\n\");\n            }\n            BlockContent::StructuredData(s) => {\n                let fmt = data_format_name(s.format);\n                out.push_str(&format!(\"```{fmt}\\n\"));\n                out.push_str(&lossy(&s.content));\n                out.push_str(\"\\n```\\n\\n\");\n            }\n            BlockContent::Diff(d) => {\n                out.push_str(&format!(\"### Diff: {}\\n\\n```diff\\n\", d.path));\n                for hunk in &d.hunks {\n                    out.push_str(&format!(\n                        \"@@ -{},{} +{},{} @@\\n\",\n                        hunk.old_start, 0, hunk.new_start, 0\n                    ));\n                    out.push_str(&lossy(&hunk.lines));\n                    if !hunk.lines.ends_with(b\"\\n\") {\n                        out.push('\\n');\n                    }\n                }\n                out.push_str(\"```\\n\\n\");\n            }\n            BlockContent::EmbeddingRef(e) => {\n                out.push_str(&format!(\n                    \"[embedding: model={}, vector_id={} bytes]\\n\\n\",\n                    e.model,\n                    e.vector_id.len()\n                ));\n            }\n            BlockContent::Image(img) => {\n                out.push_str(&format!(\n                    \"![{}](data:{};base64,...{} bytes)\\n\\n\",\n                    img.alt_text,\n                    media_type_name(img.media_type),\n                    img.data.len()\n                ));\n            }\n            BlockContent::Extension(ext) => {\n                out.push_str(&format!(\n                    \"### Extension ({}/{})\\n\\n```\\n\",\n                    ext.namespace, ext.type_name\n                ));\n                out.push_str(&lossy(&ext.content));\n                out.push_str(\"\\n```\\n\\n\");\n            }\n            BlockContent::Annotation(_) | BlockContent::End | BlockContent::Unknown { .. } => {}\n        }\n    }\n\n    out\n}\n\n/// Build realistic agent markdown — mimics Claude Code's actual context format.\n///\n/// This is the fairer comparison because it represents what models actually\n/// receive today. Includes XML-style source tags, JSON tool-call envelopes,\n/// and message wrappers with role attributes.\npub fn build_realistic_markdown(blocks: &[Block]) -> String {\n    let mut out = String::new();\n    out.push_str(\"<context>\\n\");\n\n    for (i, block) in blocks.iter().enumerate() {\n        match &block.content {\n            BlockContent::Code(c) => {\n                let lang = lang_name(c.lang);\n                out.push_str(&format!(\n                    \"<source path=\\\"{}\\\" language=\\\"{lang}\\\">\\n```{lang}\\n\",\n                    c.path\n                ));\n                out.push_str(&lossy(&c.content));\n                out.push_str(\"\\n```\\n</source>\\n\\n\");\n            }\n            BlockContent::Conversation(c) => {\n                let role = role_name(c.role);\n                out.push_str(&format!(\"<message role=\\\"{role}\\\">\\n\"));\n                out.push_str(&lossy(&c.content));\n                out.push_str(\"\\n</message>\\n\\n\");\n            }\n            BlockContent::ToolResult(t) => {\n                let name = &t.tool_name;\n                let status = status_name(t.status);\n                let content_str = lossy(&t.content);\n                let json_content =\n                    serde_json::to_string(&content_str).unwrap_or_else(|_| content_str.clone());\n                out.push_str(&format!(\n                    \"<tool_result>\\n{{\\n  \\\"tool\\\": \\\"{name}\\\",\\n  \\\"status\\\": \\\"{status}\\\",\\n  \\\"output\\\": {json_content}\\n}}\\n</tool_result>\\n\\n\"\n                ));\n            }\n            BlockContent::FileTree(ft) => {\n                out.push_str(&format!(\n                    \"<file_tree root=\\\"{}\\\">\\n```\\n\",\n                    ft.root_path\n                ));\n                render_tree_entries_naive(&mut out, &ft.entries, 0);\n                out.push_str(\"```\\n</file_tree>\\n\\n\");\n            }\n            BlockContent::Document(d) => {\n                out.push_str(&format!(\n                    \"<document title=\\\"{}\\\">\\n\",\n                    d.title\n                ));\n                out.push_str(&lossy(&d.content));\n                out.push_str(\"\\n</document>\\n\\n\");\n            }\n            BlockContent::StructuredData(s) => {\n                let fmt = data_format_name(s.format);\n                out.push_str(&format!(\"<data format=\\\"{fmt}\\\">\\n\"));\n                out.push_str(&lossy(&s.content));\n                out.push_str(\"\\n</data>\\n\\n\");\n            }\n            BlockContent::Diff(d) => {\n                out.push_str(&format!(\"<diff path=\\\"{}\\\">\\n```diff\\n\", d.path));\n                for hunk in &d.hunks {\n                    out.push_str(&format!(\n                        \"@@ -{},{} +{},{} @@\\n\",\n                        hunk.old_start, 0, hunk.new_start, 0\n                    ));\n                    out.push_str(&lossy(&hunk.lines));\n                    if !hunk.lines.ends_with(b\"\\n\") {\n                        out.push('\\n');\n                    }\n                }\n                out.push_str(\"```\\n</diff>\\n\\n\");\n            }\n            BlockContent::Extension(ext) => {\n                out.push_str(&format!(\n                    \"<extension namespace=\\\"{}\\\" type=\\\"{}\\\">\\n\",\n                    ext.namespace, ext.type_name\n                ));\n                out.push_str(&lossy(&ext.content));\n                out.push_str(\"\\n</extension>\\n\\n\");\n            }\n            BlockContent::Annotation(_) | BlockContent::End | BlockContent::Unknown { .. } => {}\n            _ => {\n                out.push_str(&format!(\"<block index=\\\"{i}\\\">\\n\"));\n                out.push_str(\"</block>\\n\\n\");\n            }\n        }\n    }\n\n    out.push_str(\"</context>\\n\");\n    out\n}\n\nfn render_tree_entries_naive(\n    out: &mut String,\n    entries: &[bcp_types::file_tree::FileEntry],\n    depth: usize,\n) {\n    for entry in entries {\n        let indent = \"  \".repeat(depth);\n        let suffix = if entry.kind == bcp_types::file_tree::FileEntryKind::Directory {\n            \"/\"\n        } else {\n            \"\"\n        };\n        out.push_str(&format!(\"{indent}{}{suffix}\\n\", entry.name));\n        if !entry.children.is_empty() {\n            render_tree_entries_naive(out, &entry.children, depth + 1);\n        }\n    }\n}\n\nfn lossy(bytes: &[u8]) -> String {\n    String::from_utf8_lossy(bytes).into_owned()\n}\n\nfn lang_name(lang: bcp_types::enums::Lang) -> &'static str {\n    match lang {\n        bcp_types::enums::Lang::Rust => \"rust\",\n        bcp_types::enums::Lang::TypeScript => \"typescript\",\n        bcp_types::enums::Lang::JavaScript => \"javascript\",\n        bcp_types::enums::Lang::Python => \"python\",\n        bcp_types::enums::Lang::Go => \"go\",\n        bcp_types::enums::Lang::Java => \"java\",\n        bcp_types::enums::Lang::C => \"c\",\n        bcp_types::enums::Lang::Cpp => \"cpp\",\n        bcp_types::enums::Lang::Ruby => \"ruby\",\n        bcp_types::enums::Lang::Shell => \"shell\",\n        bcp_types::enums::Lang::Sql => \"sql\",\n        bcp_types::enums::Lang::Html => \"html\",\n        bcp_types::enums::Lang::Css => \"css\",\n        bcp_types::enums::Lang::Json => \"json\",\n        bcp_types::enums::Lang::Yaml => \"yaml\",\n        bcp_types::enums::Lang::Toml => \"toml\",\n        bcp_types::enums::Lang::Markdown => \"markdown\",\n        bcp_types::enums::Lang::Unknown | bcp_types::enums::Lang::Other(_) => \"text\",\n    }\n}\n\nfn role_name(role: bcp_types::enums::Role) -> &'static str {\n    match role {\n        bcp_types::enums::Role::System => \"system\",\n        bcp_types::enums::Role::User => \"user\",\n        bcp_types::enums::Role::Assistant => \"assistant\",\n        bcp_types::enums::Role::Tool => \"tool\",\n    }\n}\n\nfn status_name(status: bcp_types::enums::Status) -> &'static str {\n    match status {\n        bcp_types::enums::Status::Ok => \"ok\",\n        bcp_types::enums::Status::Error => \"error\",\n        bcp_types::enums::Status::Timeout => \"timeout\",\n    }\n}\n\nfn data_format_name(fmt: bcp_types::enums::DataFormat) -> &'static str {\n    match fmt {\n        bcp_types::enums::DataFormat::Json => \"json\",\n        bcp_types::enums::DataFormat::Yaml => \"yaml\",\n        bcp_types::enums::DataFormat::Toml => \"toml\",\n        bcp_types::enums::DataFormat::Csv => \"csv\",\n    }\n}\n\nfn media_type_name(mt: bcp_types::enums::MediaType) -> &'static str {\n    match mt {\n        bcp_types::enums::MediaType::Png => \"image/png\",\n        bcp_types::enums::MediaType::Jpeg => \"image/jpeg\",\n        bcp_types::enums::MediaType::Gif => \"image/gif\",\n        bcp_types::enums::MediaType::Svg => \"image/svg+xml\",\n        bcp_types::enums::MediaType::Webp => \"image/webp\",\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use bcp_types::block::BlockContent;\n    use bcp_types::code::CodeBlock;\n    use bcp_types::conversation::ConversationBlock;\n    use bcp_types::enums::{Lang, Role};\n    use bcp_wire::block_frame::BlockFlags;\n\n    fn make_block(content: BlockContent) -> Block {\n        Block {\n            block_type: bcp_types::BlockType::Code,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content,\n        }\n    }\n\n    #[test]\n    fn naive_markdown_code_block() {\n        let blocks = vec![make_block(BlockContent::Code(CodeBlock {\n            lang: Lang::Rust,\n            path: \"src/main.rs\".into(),\n            content: b\"fn main() {}\".to_vec(),\n            line_range: None,\n        }))];\n        let md = build_naive_markdown(&blocks);\n        assert!(md.contains(\"```rust\"));\n        assert!(md.contains(\"// src/main.rs\"));\n        assert!(md.contains(\"fn main() {}\"));\n    }\n\n    #[test]\n    fn realistic_markdown_has_context_wrapper() {\n        let blocks = vec![make_block(BlockContent::Conversation(\n            ConversationBlock {\n                role: Role::User,\n                content: b\"hello\".to_vec(),\n                tool_call_id: None,\n            },\n        ))];\n        let md = build_realistic_markdown(&blocks);\n        assert!(md.starts_with(\"<context>\"));\n        assert!(md.contains(\"<message role=\\\"user\\\">\"));\n        assert!(md.ends_with(\"</context>\\n\"));\n    }\n}\n",
      "language": "rust",
      "path": "bcp-bench-real/src/markdown.rs",
      "type": "code"
    },
    {
      "content": "use anyhow::Result;\nuse tiktoken_rs::cl100k_base;\n\npub struct TokenCounter {\n    bpe: tiktoken_rs::CoreBPE,\n}\n\nimpl TokenCounter {\n    pub fn new() -> Result<Self> {\n        let bpe = cl100k_base()?;\n        Ok(Self { bpe })\n    }\n\n    pub fn count(&self, text: &str) -> usize {\n        self.bpe.encode_with_special_tokens(text).len()\n    }\n}\n",
      "language": "rust",
      "path": "bcp-bench-real/src/token_counter.rs",
      "type": "code"
    },
    {
      "content": "[package]\nname = \"bcp-cli\"\nversion = \"0.1.0\"\nedition = \"2024\"\n\n[[bin]]\nname = \"bcp\"\npath = \"src/main.rs\"\n\n[dependencies]\nbcp-decoder = { path = \"../bcp-decoder\" }\nbcp-driver  = { path = \"../bcp-driver\" }\nbcp-encoder = { path = \"../bcp-encoder\" }\nbcp-types   = { path = \"../bcp-types\" }\nanyhow      = { workspace = true }\nclap        = { workspace = true }\nserde       = { version = \"1\", features = [\"derive\"] }\nserde_json  = \"1\"\n",
      "language": "toml",
      "path": "bcp-cli/Cargo.toml",
      "type": "code"
    },
    {
      "content": "/// Implementation of `bcp decode`.\n///\n/// Reads a BCP file, decodes all blocks with `BcpDecoder`, then passes\n/// the block slice to `DefaultDriver::render` for model-ready text\n/// output. The output is written to stdout or to `-o <file>`.\n///\n/// # Output modes\n///\n/// ```text\n/// ┌──────────┬──────────────────────────────────────────────────────────────┐\n/// │ Mode     │ Format                                                       │\n/// ├──────────┼──────────────────────────────────────────────────────────────┤\n/// │ xml      │ <code lang=\"rust\" path=\"...\">...</code>   (default)          │\n/// │ markdown │ ```rust\\n// src/main.rs\\n...\\n```                            │\n/// │ minimal  │ --- src/main.rs [rust] ---\\n...                              │\n/// └──────────┴──────────────────────────────────────────────────────────────┘\n/// ```\n///\n/// # Verbosity and budget\n///\n/// ```text\n/// ┌──────────┬────────────────────────────────────────────────────────────┐\n/// │ Verbosity│ Behaviour                                                  │\n/// ├──────────┼────────────────────────────────────────────────────────────┤\n/// │ full     │ Always render complete block content, ignore budget        │\n/// │ summary  │ Render summaries where available, full content otherwise   │\n/// │ adaptive │ Auto-select per block based on budget + priority (default) │\n/// └──────────┴────────────────────────────────────────────────────────────┘\n/// ```\n///\n/// When `--budget N` is set, the driver tracks tokens spent and falls back\n/// to summaries or placeholders for lower-priority blocks once the budget\n/// is exhausted (RFC §5.5). Without `--budget`, adaptive mode behaves like\n/// `full`.\n///\n/// # Type filtering\n///\n/// `--include code,conversation` limits rendering to those block types.\n/// All other blocks are silently excluded from the output.\nuse std::fs;\nuse std::io::{self, Write as _};\n\nuse anyhow::{Context, Result, anyhow};\nuse bcp_decoder::BcpDecoder;\nuse bcp_driver::{DefaultDriver, DriverConfig, BcpDriver, OutputMode, Verbosity};\nuse bcp_types::block_type::BlockType;\n\nuse crate::DecodeArgs;\n\n/// Run the `bcp decode` command.\n///\n/// Decodes the BCP file, applies the mode / verbosity / budget / include\n/// configuration, renders the blocks via [`DefaultDriver`], and writes the\n/// result to stdout or an output file.\n///\n/// # Errors\n///\n/// Returns an error if the file cannot be read, the BCP payload is\n/// structurally invalid, any CLI flag value is unrecognised, or the\n/// driver fails to render.\npub fn run(args: &DecodeArgs) -> Result<()> {\n    let bytes =\n        fs::read(&args.file).with_context(|| format!(\"cannot read {}\", args.file.display()))?;\n\n    let decoded = BcpDecoder::decode(&bytes)\n        .with_context(|| format!(\"failed to decode {}\", args.file.display()))?;\n\n    let mode = parse_output_mode(&args.mode)?;\n    let verbosity = parse_verbosity(&args.verbosity)?;\n    let include_types = args\n        .include\n        .as_deref()\n        .map(parse_include_types)\n        .transpose()?;\n\n    let config = DriverConfig {\n        mode,\n        verbosity,\n        token_budget: args.budget,\n        include_types,\n        target_model: None,\n    };\n\n    let driver = DefaultDriver;\n    let rendered = driver\n        .render(&decoded.blocks, &config)\n        .with_context(|| \"driver render failed\")?;\n\n    if let Some(path) = &args.output {\n        fs::write(path, rendered.as_bytes())\n            .with_context(|| format!(\"cannot write {}\", path.display()))?;\n    } else {\n        let stdout = io::stdout();\n        let mut handle = stdout.lock();\n        handle\n            .write_all(rendered.as_bytes())\n            .context(\"cannot write to stdout\")?;\n        if !rendered.ends_with('\\n') {\n            handle.write_all(b\"\\n\").context(\"cannot write to stdout\")?;\n        }\n    }\n\n    Ok(())\n}\n\n// ── Flag parsers ──────────────────────────────────────────────────────────────\n\n/// Parses the `--mode` string to an [`OutputMode`].\n///\n/// # Errors\n///\n/// Returns an error for unrecognised mode names.\nfn parse_output_mode(s: &str) -> Result<OutputMode> {\n    match s.to_lowercase().as_str() {\n        \"xml\" => Ok(OutputMode::Xml),\n        \"markdown\" | \"md\" => Ok(OutputMode::Markdown),\n        \"minimal\" => Ok(OutputMode::Minimal),\n        _ => Err(anyhow!(\n            \"unknown mode {s:?} — expected xml|markdown|minimal\"\n        )),\n    }\n}\n\n/// Parses the `--verbosity` string to a [`Verbosity`].\n///\n/// # Errors\n///\n/// Returns an error for unrecognised verbosity names.\nfn parse_verbosity(s: &str) -> Result<Verbosity> {\n    match s.to_lowercase().as_str() {\n        \"full\" => Ok(Verbosity::Full),\n        \"summary\" => Ok(Verbosity::Summary),\n        \"adaptive\" => Ok(Verbosity::Adaptive),\n        _ => Err(anyhow!(\n            \"unknown verbosity {s:?} — expected full|summary|adaptive\"\n        )),\n    }\n}\n\n/// Parses a comma-separated `--include` string to a list of [`BlockType`]s.\n///\n/// # Errors\n///\n/// Returns an error if any token is not a recognised block type name.\nfn parse_include_types(s: &str) -> Result<Vec<BlockType>> {\n    s.split(',')\n        .map(str::trim)\n        .filter(|t| !t.is_empty())\n        .map(|token| parse_block_type(token).ok_or_else(|| anyhow!(\"unknown block type {token:?}\")))\n        .collect()\n}\n\n/// Maps a block type name to a [`BlockType`] variant, case-insensitive.\nfn parse_block_type(s: &str) -> Option<BlockType> {\n    match s.to_lowercase().as_str() {\n        \"code\" => Some(BlockType::Code),\n        \"conversation\" => Some(BlockType::Conversation),\n        \"file_tree\" | \"filetree\" => Some(BlockType::FileTree),\n        \"tool_result\" | \"toolresult\" => Some(BlockType::ToolResult),\n        \"document\" => Some(BlockType::Document),\n        \"structured_data\" | \"structureddata\" => Some(BlockType::StructuredData),\n        \"diff\" => Some(BlockType::Diff),\n        \"annotation\" => Some(BlockType::Annotation),\n        \"embedding_ref\" | \"embeddingref\" => Some(BlockType::EmbeddingRef),\n        \"image\" => Some(BlockType::Image),\n        \"extension\" => Some(BlockType::Extension),\n        _ => None,\n    }\n}\n",
      "language": "rust",
      "path": "bcp-cli/src/cmd_decode.rs",
      "type": "code"
    },
    {
      "content": "/// Implementation of `bcp encode`.\n///\n/// Parses a JSON manifest describing a sequence of BCP blocks and serialises\n/// them into a binary `.bcp` payload using `BcpEncoder`. The manifest path is\n/// the sole positional argument; the output file is required via `-o`.\n///\n/// # Manifest format\n///\n/// ```json\n/// {\n///   \"blocks\": [\n///     {\n///       \"type\": \"code\",\n///       \"lang\": \"rust\",\n///       \"path\": \"src/main.rs\",\n///       \"content\": \"fn main() {}\",\n///       \"summary\": \"Entry point\",\n///       \"priority\": \"high\"\n///     },\n///     {\n///       \"type\": \"conversation\",\n///       \"role\": \"user\",\n///       \"content\": \"Fix the timeout bug.\"\n///     },\n///     {\n///       \"type\": \"tool_result\",\n///       \"name\": \"ripgrep\",\n///       \"status\": \"ok\",\n///       \"content\": \"src/main.rs:42: let timeout = 30;\"\n///     },\n///     {\n///       \"type\": \"document\",\n///       \"title\": \"API Reference\",\n///       \"format\": \"markdown\",\n///       \"content\": \"# API\\n...\"\n///     },\n///     {\n///       \"type\": \"structured_data\",\n///       \"format\": \"json\",\n///       \"content\": \"{\\\"key\\\": \\\"value\\\"}\"\n///     }\n///   ]\n/// }\n/// ```\n///\n/// The `content_file` key may substitute `content` for any block that accepts\n/// text — the encoder reads the file at the given path relative to the\n/// manifest file's parent directory.\n///\n/// # Supported block types\n///\n/// ```text\n/// ┌──────────────────┬──────────────────────────────────────────────────────┐\n/// │ Type             │ Required fields                                      │\n/// ├──────────────────┼──────────────────────────────────────────────────────┤\n/// │ code             │ lang, path, content (or content_file)                │\n/// │ conversation     │ role, content (or content_file)                      │\n/// │ tool_result      │ name, content (or content_file)                      │\n/// │ document         │ title, content (or content_file)                     │\n/// │ structured_data  │ format, content (or content_file)                    │\n/// └──────────────────┴──────────────────────────────────────────────────────┘\n/// ```\n///\n/// Optional fields for all blocks: `summary` (string), `priority`\n/// (`critical` | `high` | `normal` | `low` | `background`).\n///\n/// # Flags\n///\n/// ```text\n/// ┌──────────────────────┬─────────────────────────────────────────────┐\n/// │ Flag                 │ Effect                                      │\n/// ├──────────────────────┼─────────────────────────────────────────────┤\n/// │ --compress-blocks    │ zstd-compress each block body individually  │\n/// │ --compress-payload   │ zstd-compress all blocks as one stream      │\n/// │ --dedup              │ BLAKE3 dedup via in-memory content store    │\n/// └──────────────────────┴─────────────────────────────────────────────┘\n/// ```\nuse std::fs;\nuse std::path::Path;\nuse std::sync::Arc;\n\nuse anyhow::{Context, Result, anyhow};\nuse bcp_encoder::{BcpEncoder, MemoryContentStore};\nuse bcp_types::enums::{DataFormat, FormatHint, Lang, Priority, Role, Status};\n\nuse crate::EncodeArgs;\n\n// ── Manifest serde types ──────────────────────────────────────────────────────\n\n#[derive(serde::Deserialize)]\nstruct Manifest {\n    blocks: Vec<ManifestBlock>,\n}\n\n/// A single block entry in the JSON manifest.\n///\n/// The `type` field selects the variant. Optional fields (`summary`,\n/// `priority`, `content_file`) are shared across variants but only\n/// meaningful where documented.\n#[derive(serde::Deserialize)]\n#[serde(tag = \"type\", rename_all = \"snake_case\")]\nenum ManifestBlock {\n    /// Source code block. `lang` must be a recognised language name.\n    Code {\n        lang: String,\n        path: String,\n        /// Inline text content. Mutually exclusive with `content_file`.\n        content: Option<String>,\n        /// Read content from this path (relative to manifest dir).\n        content_file: Option<String>,\n        summary: Option<String>,\n        priority: Option<String>,\n    },\n    /// Conversation turn (system / user / assistant / tool).\n    Conversation {\n        role: String,\n        content: Option<String>,\n        content_file: Option<String>,\n        summary: Option<String>,\n        priority: Option<String>,\n    },\n    /// Tool invocation result.\n    ToolResult {\n        name: String,\n        /// `ok` | `error` | `timeout`. Defaults to `ok`.\n        status: Option<String>,\n        content: Option<String>,\n        content_file: Option<String>,\n    },\n    /// Free-form document (prose, references, specs).\n    Document {\n        title: String,\n        /// `markdown` | `plain` | `html`. Defaults to `markdown`.\n        format: Option<String>,\n        content: Option<String>,\n        content_file: Option<String>,\n        summary: Option<String>,\n        priority: Option<String>,\n    },\n    /// Structured / tabular data.\n    StructuredData {\n        /// `json` | `yaml` | `toml` | `csv`.\n        format: String,\n        content: Option<String>,\n        content_file: Option<String>,\n    },\n}\n\n// ── Public entry point ────────────────────────────────────────────────────────\n\n/// Run the `bcp encode` command.\n///\n/// Reads and parses the JSON manifest, builds an [`BcpEncoder`], applies\n/// compression / dedup flags, and writes the encoded payload to the output\n/// file. Prints a one-line summary (`Wrote N bytes to <path>`) on success.\n///\n/// # Errors\n///\n/// Returns an error if the manifest cannot be read or parsed, if a block\n/// references a `content_file` that does not exist, if an enum value\n/// (lang, role, priority, …) is unrecognised, or if `BcpEncoder::encode`\n/// fails (e.g. zstd compression error).\npub fn run(args: &EncodeArgs) -> Result<()> {\n    let manifest_src = fs::read_to_string(&args.input)\n        .with_context(|| format!(\"cannot read {}\", args.input.display()))?;\n\n    let manifest: Manifest = serde_json::from_str(&manifest_src)\n        .with_context(|| format!(\"failed to parse manifest {}\", args.input.display()))?;\n\n    let manifest_dir = args.input.parent().unwrap_or_else(|| Path::new(\".\"));\n\n    let mut encoder = BcpEncoder::new();\n\n    // When --dedup is requested a MemoryContentStore must be provided before\n    // any encoding; auto_dedup() alone without a store would error at encode().\n    if args.dedup {\n        let store = Arc::new(MemoryContentStore::new());\n        encoder.set_content_store(store);\n        encoder.auto_dedup();\n    }\n\n    if args.compress_blocks {\n        encoder.compress_blocks();\n    }\n    if args.compress_payload {\n        encoder.compress_payload();\n    }\n\n    for (idx, block) in manifest.blocks.iter().enumerate() {\n        apply_block(&mut encoder, block, manifest_dir)\n            .with_context(|| format!(\"block {idx}: failed to apply\"))?;\n    }\n\n    let bytes = encoder\n        .encode()\n        .with_context(|| \"BcpEncoder::encode failed\")?;\n\n    fs::write(&args.output, &bytes)\n        .with_context(|| format!(\"cannot write {}\", args.output.display()))?;\n\n    println!(\"Wrote {} bytes to {}\", bytes.len(), args.output.display());\n    Ok(())\n}\n\n// ── Block application helpers ─────────────────────────────────────────────────\n\n/// Adds a single manifest block to `encoder`, resolving content from inline\n/// text or a `content_file` path relative to `manifest_dir`.\nfn apply_block(encoder: &mut BcpEncoder, block: &ManifestBlock, manifest_dir: &Path) -> Result<()> {\n    match block {\n        ManifestBlock::Code {\n            lang,\n            path,\n            content,\n            content_file,\n            summary,\n            priority,\n        } => {\n            let bytes = resolve_content(\n                content.as_deref(),\n                content_file.as_deref(),\n                manifest_dir,\n                \"code\",\n            )?;\n            let lang_val = parse_lang(lang);\n            encoder.add_code(lang_val, path, &bytes);\n            apply_meta(encoder, summary.as_deref(), priority.as_deref())?;\n        }\n\n        ManifestBlock::Conversation {\n            role,\n            content,\n            content_file,\n            summary,\n            priority,\n        } => {\n            let bytes = resolve_content(\n                content.as_deref(),\n                content_file.as_deref(),\n                manifest_dir,\n                \"conversation\",\n            )?;\n            let role_val = parse_role(role)?;\n            encoder.add_conversation(role_val, &bytes);\n            apply_meta(encoder, summary.as_deref(), priority.as_deref())?;\n        }\n\n        ManifestBlock::ToolResult {\n            name,\n            status,\n            content,\n            content_file,\n        } => {\n            let bytes = resolve_content(\n                content.as_deref(),\n                content_file.as_deref(),\n                manifest_dir,\n                \"tool_result\",\n            )?;\n            let status_val = status.as_deref().map_or(Ok(Status::Ok), parse_status)?;\n            encoder.add_tool_result(name, status_val, &bytes);\n        }\n\n        ManifestBlock::Document {\n            title,\n            format,\n            content,\n            content_file,\n            summary,\n            priority,\n        } => {\n            let bytes = resolve_content(\n                content.as_deref(),\n                content_file.as_deref(),\n                manifest_dir,\n                \"document\",\n            )?;\n            let fmt = format\n                .as_deref()\n                .map_or(Ok(FormatHint::Markdown), parse_format_hint)?;\n            encoder.add_document(title, &bytes, fmt);\n            apply_meta(encoder, summary.as_deref(), priority.as_deref())?;\n        }\n\n        ManifestBlock::StructuredData {\n            format,\n            content,\n            content_file,\n        } => {\n            let bytes = resolve_content(\n                content.as_deref(),\n                content_file.as_deref(),\n                manifest_dir,\n                \"structured_data\",\n            )?;\n            let fmt = parse_data_format(format)?;\n            encoder.add_structured_data(fmt, &bytes);\n        }\n    }\n\n    Ok(())\n}\n\n/// Applies `summary` and `priority` metadata to the most recently added block.\nfn apply_meta(\n    encoder: &mut BcpEncoder,\n    summary: Option<&str>,\n    priority: Option<&str>,\n) -> Result<()> {\n    if let Some(s) = summary {\n        encoder.with_summary(s)?;\n    }\n    if let Some(p) = priority {\n        encoder.with_priority(parse_priority(p)?)?;\n    }\n    Ok(())\n}\n\n// ── Content resolution ────────────────────────────────────────────────────────\n\n/// Returns the UTF-8 bytes for a block's content field.\n///\n/// Prefers `content` (inline string) over `content_file` (file path).\n/// Returns an error if neither is provided or if the file cannot be read.\nfn resolve_content(\n    content: Option<&str>,\n    content_file: Option<&str>,\n    manifest_dir: &Path,\n    block_type: &str,\n) -> Result<Vec<u8>> {\n    if let Some(text) = content {\n        return Ok(text.as_bytes().to_vec());\n    }\n    if let Some(path_str) = content_file {\n        let path = manifest_dir.join(path_str);\n        return fs::read(&path)\n            .with_context(|| format!(\"cannot read content_file {}\", path.display()));\n    }\n    Err(anyhow!(\n        \"{block_type} block requires either \\\"content\\\" or \\\"content_file\\\"\"\n    ))\n}\n\n// ── Enum parsers ──────────────────────────────────────────────────────────────\n\n/// Maps a language name string to a [`Lang`] variant.\n///\n/// Unrecognised names map to [`Lang::Unknown`] rather than erroring, so\n/// arbitrary language tags in manifests produce a valid (if opaque) block.\nfn parse_lang(s: &str) -> Lang {\n    match s.to_lowercase().as_str() {\n        \"rust\" => Lang::Rust,\n        \"typescript\" | \"ts\" => Lang::TypeScript,\n        \"javascript\" | \"js\" => Lang::JavaScript,\n        \"python\" | \"py\" => Lang::Python,\n        \"go\" => Lang::Go,\n        \"java\" => Lang::Java,\n        \"c\" => Lang::C,\n        \"cpp\" | \"c++\" => Lang::Cpp,\n        \"ruby\" | \"rb\" => Lang::Ruby,\n        \"shell\" | \"sh\" | \"bash\" => Lang::Shell,\n        \"sql\" => Lang::Sql,\n        \"html\" => Lang::Html,\n        \"css\" => Lang::Css,\n        \"json\" => Lang::Json,\n        \"yaml\" | \"yml\" => Lang::Yaml,\n        \"toml\" => Lang::Toml,\n        \"markdown\" | \"md\" => Lang::Markdown,\n        _ => Lang::Unknown,\n    }\n}\n\n/// Parses a conversation role name.\n///\n/// # Errors\n///\n/// Returns an error for unrecognised role names, since an invalid role\n/// would produce a structurally incorrect block.\nfn parse_role(s: &str) -> Result<Role> {\n    match s.to_lowercase().as_str() {\n        \"system\" => Ok(Role::System),\n        \"user\" => Ok(Role::User),\n        \"assistant\" => Ok(Role::Assistant),\n        \"tool\" => Ok(Role::Tool),\n        _ => Err(anyhow!(\n            \"unknown role {s:?} — expected system|user|assistant|tool\"\n        )),\n    }\n}\n\n/// Parses a tool result status name. Defaults to `ok`.\n///\n/// # Errors\n///\n/// Returns an error for unrecognised status names.\nfn parse_status(s: &str) -> Result<Status> {\n    match s.to_lowercase().as_str() {\n        \"ok\" => Ok(Status::Ok),\n        \"error\" | \"err\" => Ok(Status::Error),\n        \"timeout\" => Ok(Status::Timeout),\n        _ => Err(anyhow!(\"unknown status {s:?} — expected ok|error|timeout\")),\n    }\n}\n\n/// Parses a block priority name.\n///\n/// # Errors\n///\n/// Returns an error for unrecognised priority names.\nfn parse_priority(s: &str) -> Result<Priority> {\n    match s.to_lowercase().as_str() {\n        \"critical\" => Ok(Priority::Critical),\n        \"high\" => Ok(Priority::High),\n        \"normal\" => Ok(Priority::Normal),\n        \"low\" => Ok(Priority::Low),\n        \"background\" => Ok(Priority::Background),\n        _ => Err(anyhow!(\n            \"unknown priority {s:?} — expected critical|high|normal|low|background\"\n        )),\n    }\n}\n\n/// Parses a document format hint name. Defaults to `markdown`.\n///\n/// # Errors\n///\n/// Returns an error for unrecognised format names.\nfn parse_format_hint(s: &str) -> Result<FormatHint> {\n    match s.to_lowercase().as_str() {\n        \"markdown\" | \"md\" => Ok(FormatHint::Markdown),\n        \"plain\" | \"text\" | \"txt\" => Ok(FormatHint::Plain),\n        \"html\" => Ok(FormatHint::Html),\n        _ => Err(anyhow!(\n            \"unknown document format {s:?} — expected markdown|plain|html\"\n        )),\n    }\n}\n\n/// Parses a structured data format name.\n///\n/// # Errors\n///\n/// Returns an error for unrecognised format names.\nfn parse_data_format(s: &str) -> Result<DataFormat> {\n    match s.to_lowercase().as_str() {\n        \"json\" => Ok(DataFormat::Json),\n        \"yaml\" | \"yml\" => Ok(DataFormat::Yaml),\n        \"toml\" => Ok(DataFormat::Toml),\n        \"csv\" => Ok(DataFormat::Csv),\n        _ => Err(anyhow!(\n            \"unknown data format {s:?} — expected json|yaml|toml|csv\"\n        )),\n    }\n}\n",
      "language": "rust",
      "path": "bcp-cli/src/cmd_encode.rs",
      "type": "code"
    },
    {
      "content": "/// Implementation of `bcp inspect`.\n///\n/// Reads a BCP file, decodes all blocks, and prints a structured summary\n/// to stdout. Optionally shows block body content (`--show-body`) or a raw\n/// hex dump (`--show-hex`). When `--block N` is given, only the block at\n/// index N is shown.\n///\n/// # Output format\n///\n/// ```text\n/// Header: BCP v1.0, flags=0x00, 4 blocks\n/// Block 0: CODE [rust] path=\"src/main.rs\" (23 bytes)\n///          Summary: \"Entry point with CLI setup\"\n/// Block 1: CONVERSATION [user] (19 bytes)\n/// Block 2: CONVERSATION [assistant] (31 bytes)\n/// Block 3: ANNOTATION target=0 kind=priority value=\"high\"\n/// ---\n/// END sentinel at offset 312\n/// ```\nuse std::fs;\n\nuse anyhow::{Context, Result};\nuse bcp_decoder::BcpDecoder;\nuse bcp_types::block::BlockContent;\nuse bcp_types::enums::AnnotationKind;\n\nuse crate::InspectArgs;\n\n/// Run the `bcp inspect` command.\n///\n/// # Errors\n///\n/// Returns an error if the file cannot be read or the BCP payload is\n/// structurally invalid (malformed header, truncated blocks, etc.).\npub fn run(args: &InspectArgs) -> Result<()> {\n    let bytes =\n        fs::read(&args.file).with_context(|| format!(\"cannot read {}\", args.file.display()))?;\n\n    let decoded = BcpDecoder::decode(&bytes)\n        .with_context(|| format!(\"failed to decode {}\", args.file.display()))?;\n\n    let header = &decoded.header;\n    println!(\n        \"Header: BCP v{}.{}, flags=0x{:02X}, {} block{}\",\n        header.version_major,\n        header.version_minor,\n        header.flags.raw(),\n        decoded.blocks.len(),\n        if decoded.blocks.len() == 1 { \"\" } else { \"s\" }\n    );\n\n    for (idx, block) in decoded.blocks.iter().enumerate() {\n        // When --block N is specified, skip all other indices.\n        if let Some(target) = args.block\n            && idx != target\n        {\n            continue;\n        }\n\n        let type_label = block_type_label(&block.content);\n        let detail = block_detail(&block.content);\n        let body_bytes = block_body_bytes(&block.content);\n\n        println!(\"Block {idx}: {type_label}{detail} ({body_bytes} bytes)\");\n\n        if let Some(ref summary) = block.summary {\n            println!(\"         Summary: {:?}\", summary.text);\n        }\n\n        if args.show_body {\n            let body = block_body_lossy(&block.content);\n            let truncated: String = body.chars().take(80).collect();\n            let ellipsis = if body.chars().count() > 80 { \"…\" } else { \"\" };\n            println!(\"         Body:    {truncated}{ellipsis}\");\n        }\n\n        if args.show_hex {\n            let raw = block_body_raw(&block.content);\n            println!(\"         Hex dump:\");\n            for (i, chunk) in raw.chunks(16).enumerate() {\n                let offset = i * 16;\n                let hex: String =\n                    chunk\n                        .iter()\n                        .fold(String::with_capacity(chunk.len() * 3), |mut s, b| {\n                            use std::fmt::Write as _;\n                            if !s.is_empty() {\n                                s.push(' ');\n                            }\n                            let _ = write!(s, \"{b:02x}\");\n                            s\n                        });\n                let ascii: String = chunk\n                    .iter()\n                    .map(|&b| if b.is_ascii_graphic() { b as char } else { '.' })\n                    .collect();\n                println!(\"           {offset:04x}  {hex:<48}  {ascii}\");\n            }\n        }\n    }\n\n    // The END sentinel occupies the last 4 bytes of a minimal payload.\n    // Show its byte offset (file length minus 4) as a convenience marker.\n    println!(\"---\");\n    println!(\"END sentinel at offset {}\", bytes.len().saturating_sub(4));\n\n    Ok(())\n}\n\n// ── Block formatting helpers ──────────────────────────────────────────────────\n\n/// Returns the uppercase type label (e.g. `\"CODE\"`, `\"CONVERSATION\"`).\nfn block_type_label(content: &BlockContent) -> &'static str {\n    match content {\n        BlockContent::Code(_) => \"CODE\",\n        BlockContent::Conversation(_) => \"CONVERSATION\",\n        BlockContent::FileTree(_) => \"FILE_TREE\",\n        BlockContent::ToolResult(_) => \"TOOL_RESULT\",\n        BlockContent::Document(_) => \"DOCUMENT\",\n        BlockContent::StructuredData(_) => \"STRUCTURED_DATA\",\n        BlockContent::Diff(_) => \"DIFF\",\n        BlockContent::Annotation(_) => \"ANNOTATION\",\n        BlockContent::EmbeddingRef(_) => \"EMBEDDING_REF\",\n        BlockContent::Image(_) => \"IMAGE\",\n        BlockContent::Extension(_) => \"EXTENSION\",\n        BlockContent::End => \"END\",\n        BlockContent::Unknown { .. } => \"UNKNOWN\",\n    }\n}\n\n/// Returns the human-readable inline detail string for a block, formatted\n/// so it reads naturally after the type label on one line.\n///\n/// Examples:\n/// - Code       → ` [rust] path=\"src/main.rs\"`\n/// - Conversation → ` [user]`\n/// - `ToolResult` → ` [ripgrep] status=ok`\n/// - Annotation → ` target=0 kind=priority value=\"high\"`\nfn block_detail(content: &BlockContent) -> String {\n    match content {\n        BlockContent::Code(c) => {\n            let lang = format!(\"{:?}\", c.lang).to_lowercase();\n            format!(\" [{lang}] path={:?}\", c.path)\n        }\n        BlockContent::Conversation(c) => {\n            let role = format!(\"{:?}\", c.role).to_lowercase();\n            format!(\" [{role}]\")\n        }\n        BlockContent::FileTree(t) => format!(\" root={:?}\", t.root_path),\n        BlockContent::ToolResult(t) => {\n            let status = format!(\"{:?}\", t.status).to_lowercase();\n            format!(\" [{}] status={status}\", t.tool_name)\n        }\n        BlockContent::Document(d) => format!(\" title={:?}\", d.title),\n        BlockContent::StructuredData(s) => {\n            let fmt = format!(\"{:?}\", s.format).to_lowercase();\n            format!(\" format={fmt}\")\n        }\n        BlockContent::Diff(d) => format!(\" path={:?}\", d.path),\n        BlockContent::Annotation(a) => {\n            let kind = annotation_kind_label(a.kind);\n            let value = format_annotation_value(a.kind, &a.value);\n            format!(\" target={} kind={kind} value={value:?}\", a.target_block_id)\n        }\n        BlockContent::EmbeddingRef(_) => \" [embedding ref]\".to_string(),\n        BlockContent::Image(i) => {\n            let media = format!(\"{:?}\", i.media_type).to_lowercase();\n            format!(\" [{media}] alt={:?}\", i.alt_text)\n        }\n        BlockContent::Extension(e) => {\n            format!(\" namespace={:?} type={:?}\", e.namespace, e.type_name)\n        }\n        BlockContent::End => String::new(),\n        BlockContent::Unknown { type_id, .. } => format!(\" [0x{type_id:02X}]\"),\n    }\n}\n\n/// Returns the byte length of the primary content field of a block.\n///\n/// For most blocks this is the main `content` bytes field. For structured\n/// types it is the inner data length. This is used in the `(N bytes)` display.\nfn block_body_bytes(content: &BlockContent) -> usize {\n    match content {\n        BlockContent::Code(c) => c.content.len(),\n        BlockContent::Conversation(c) => c.content.len(),\n        BlockContent::FileTree(t) => t.entries.len() * 8, // approximate\n        BlockContent::ToolResult(t) => t.content.len(),\n        BlockContent::Document(d) => d.content.len(),\n        BlockContent::StructuredData(s) => s.content.len(),\n        BlockContent::Diff(d) => d.hunks.iter().map(|h| h.lines.len()).sum(),\n        BlockContent::Annotation(a) => a.value.len(),\n        BlockContent::EmbeddingRef(_) => 32,\n        BlockContent::Image(i) => i.data.len(),\n        BlockContent::Extension(e) => e.content.len(),\n        BlockContent::End => 0,\n        BlockContent::Unknown { body, .. } => body.len(),\n    }\n}\n\n/// Returns the primary content bytes as UTF-8 lossy string, for `--show-body`.\nfn block_body_lossy(content: &BlockContent) -> String {\n    let bytes = block_body_raw(content);\n    String::from_utf8_lossy(bytes).into_owned()\n}\n\n/// Returns the raw bytes of the primary content field, for `--show-hex`.\nfn block_body_raw(content: &BlockContent) -> &[u8] {\n    match content {\n        BlockContent::Code(c) => &c.content,\n        BlockContent::Conversation(c) => &c.content,\n        BlockContent::ToolResult(t) => &t.content,\n        BlockContent::Document(d) => &d.content,\n        BlockContent::StructuredData(s) => &s.content,\n        BlockContent::Annotation(a) => &a.value,\n        BlockContent::Image(i) => &i.data,\n        BlockContent::Extension(e) => &e.content,\n        BlockContent::Unknown { body, .. } => body,\n        _ => b\"\",\n    }\n}\n\n/// Returns a display-friendly annotation kind name.\nfn annotation_kind_label(kind: AnnotationKind) -> &'static str {\n    match kind {\n        AnnotationKind::Priority => \"priority\",\n        AnnotationKind::Summary => \"summary\",\n        AnnotationKind::Tag => \"tag\",\n    }\n}\n\n/// Formats an annotation value for display.\n///\n/// For priority annotations, decodes the single byte to a priority name.\n/// For all others, formats as UTF-8 lossy text.\nfn format_annotation_value(kind: AnnotationKind, value: &[u8]) -> String {\n    use bcp_types::enums::Priority;\n\n    if kind == AnnotationKind::Priority\n        && let Some(&byte) = value.first()\n        && let Ok(p) = Priority::from_wire_byte(byte)\n    {\n        return format!(\"{p:?}\").to_lowercase();\n    }\n    String::from_utf8_lossy(value).into_owned()\n}\n",
      "language": "rust",
      "path": "bcp-cli/src/cmd_inspect.rs",
      "type": "code"
    },
    {
      "content": "/// Implementation of `bcp stats`.\n///\n/// Decodes a BCP file and prints a structured statistics report covering\n/// file size, block-type distribution, compression status, and heuristic\n/// token estimates for all three render modes.\n///\n/// # Example output\n///\n/// ```text\n/// File:    /tmp/context.bcp  (282 bytes)\n/// Header:  BCP v1.0, flags=0x00  (uncompressed)\n/// Blocks:  6 total\n///\n/// Type              Count   Bytes\n/// ──────────────────────────────────\n/// CODE                  1      32\n/// CONVERSATION          1      20\n/// ANNOTATION            1       1\n/// TOOL_RESULT           1      32\n/// DOCUMENT              1      38\n/// STRUCTURED_DATA       1      29\n/// ──────────────────────────────────\n/// Total                 6     152\n///\n/// Tokens (heuristic estimate):\n///   xml mode      ~84 tokens\n///   markdown mode ~71 tokens\n///   minimal mode  ~58 tokens\n/// ```\n///\n/// The token estimates use [`HeuristicEstimator`] (4 chars ≈ 1 token) on the\n/// rendered output of each mode. These are rough lower-bound estimates; actual\n/// tokenisation varies by model and content.\nuse std::collections::HashMap;\nuse std::fs;\n\nuse anyhow::{Context, Result};\nuse bcp_decoder::BcpDecoder;\nuse bcp_driver::{\n    DefaultDriver, DriverConfig, HeuristicEstimator, BcpDriver, OutputMode, TokenEstimator,\n    Verbosity,\n};\nuse bcp_types::block::BlockContent;\nuse bcp_types::block_type::BlockType;\n\nuse crate::StatsArgs;\n\n/// Run the `bcp stats` command.\n///\n/// Decodes the file, tabulates block-type distribution and content byte sums,\n/// checks header compression flags, renders the payload in each output mode,\n/// and prints a formatted summary to stdout.\n///\n/// # Errors\n///\n/// Returns an error if the file cannot be read or the BCP payload fails\n/// structural validation.\npub fn run(args: &StatsArgs) -> Result<()> {\n    let bytes =\n        fs::read(&args.file).with_context(|| format!(\"cannot read {}\", args.file.display()))?;\n\n    let file_size = bytes.len();\n\n    let decoded = BcpDecoder::decode(&bytes)\n        .with_context(|| format!(\"failed to decode {}\", args.file.display()))?;\n\n    let header = &decoded.header;\n    let compressed = header.flags.is_compressed();\n\n    // ── Block distribution ────────────────────────────────────────────────────\n\n    // Ordered list of (block_type, content_bytes) for each block (excluding END).\n    let block_stats: Vec<(BlockType, usize)> = decoded\n        .blocks\n        .iter()\n        .filter(|b| b.block_type != BlockType::End)\n        .map(|b| (b.block_type.clone(), content_size(&b.content)))\n        .collect();\n\n    // Aggregate by block type: (count, total_bytes).\n    let mut by_type: HashMap<String, (usize, usize)> = HashMap::new();\n    let mut insertion_order: Vec<String> = Vec::new();\n\n    for (bt, sz) in &block_stats {\n        let label = block_type_label(bt);\n        by_type\n            .entry(label.to_string())\n            .and_modify(|(cnt, total)| {\n                *cnt += 1;\n                *total += sz;\n            })\n            .or_insert_with(|| {\n                insertion_order.push(label.to_string());\n                (1, *sz)\n            });\n    }\n\n    let total_blocks: usize = block_stats.len();\n    let total_bytes: usize = block_stats.iter().map(|(_, sz)| sz).sum();\n\n    // ── Token estimates ───────────────────────────────────────────────────────\n\n    let estimator = HeuristicEstimator;\n    let est = |mode: OutputMode| -> u32 {\n        let config = DriverConfig {\n            mode,\n            verbosity: Verbosity::Full,\n            token_budget: None,\n            include_types: None,\n            target_model: None,\n        };\n        DefaultDriver\n            .render(&decoded.blocks, &config)\n            .ok()\n            .as_deref()\n            .map_or(0, |text| estimator.estimate(text))\n    };\n\n    let xml_tokens = est(OutputMode::Xml);\n    let md_tokens = est(OutputMode::Markdown);\n    let min_tokens = est(OutputMode::Minimal);\n\n    // ── Print report ──────────────────────────────────────────────────────────\n\n    let compression_note = if compressed {\n        \" (payload zstd-compressed)\"\n    } else {\n        \" (uncompressed)\"\n    };\n\n    println!(\"File:    {}  ({file_size} bytes)\", args.file.display());\n    println!(\n        \"Header:  BCP v{}.{}, flags=0x{:02X}{compression_note}\",\n        header.version_major,\n        header.version_minor,\n        header.flags.raw()\n    );\n    println!(\"Blocks:  {total_blocks} total\");\n    println!();\n\n    let sep = \"─\".repeat(36);\n    println!(\"{:<20}{:>6}{:>8}\", \"Type\", \"Count\", \"Bytes\");\n    println!(\"{sep}\");\n\n    for label in &insertion_order {\n        let (cnt, sz) = by_type[label];\n        println!(\"{label:<20}{cnt:>6}{sz:>8}\");\n    }\n\n    println!(\"{sep}\");\n    println!(\"{:<20}{:>6}{:>8}\", \"Total\", total_blocks, total_bytes);\n\n    println!();\n    println!(\"Tokens (heuristic estimate):\");\n    println!(\"  xml mode      ~{xml_tokens} tokens\");\n    println!(\"  markdown mode ~{md_tokens} tokens\");\n    println!(\"  minimal mode  ~{min_tokens} tokens\");\n\n    Ok(())\n}\n\n// ── Helpers ───────────────────────────────────────────────────────────────────\n\n/// Returns the primary content byte count for a block, used for the stats table.\nfn content_size(content: &BlockContent) -> usize {\n    match content {\n        BlockContent::Code(c) => c.content.len(),\n        BlockContent::Conversation(c) => c.content.len(),\n        BlockContent::FileTree(t) => t.entries.len() * 8,\n        BlockContent::ToolResult(t) => t.content.len(),\n        BlockContent::Document(d) => d.content.len(),\n        BlockContent::StructuredData(s) => s.content.len(),\n        BlockContent::Diff(d) => d.hunks.iter().map(|h| h.lines.len()).sum(),\n        BlockContent::Annotation(a) => a.value.len(),\n        BlockContent::EmbeddingRef(_) => 32,\n        BlockContent::Image(i) => i.data.len(),\n        BlockContent::Extension(e) => e.content.len(),\n        BlockContent::End => 0,\n        BlockContent::Unknown { body, .. } => body.len(),\n    }\n}\n\n/// Returns the uppercase display label for a block type, matching the format\n/// used by `bcp inspect`.\nfn block_type_label(bt: &BlockType) -> &'static str {\n    match bt {\n        BlockType::Code => \"CODE\",\n        BlockType::Conversation => \"CONVERSATION\",\n        BlockType::FileTree => \"FILE_TREE\",\n        BlockType::ToolResult => \"TOOL_RESULT\",\n        BlockType::Document => \"DOCUMENT\",\n        BlockType::StructuredData => \"STRUCTURED_DATA\",\n        BlockType::Diff => \"DIFF\",\n        BlockType::Annotation => \"ANNOTATION\",\n        BlockType::EmbeddingRef => \"EMBEDDING_REF\",\n        BlockType::Image => \"IMAGE\",\n        BlockType::Extension => \"EXTENSION\",\n        BlockType::End => \"END\",\n        BlockType::Unknown(_) => \"UNKNOWN\",\n    }\n}\n",
      "language": "rust",
      "path": "bcp-cli/src/cmd_stats.rs",
      "type": "code"
    },
    {
      "content": "/// Implementation of `bcp validate`.\n///\n/// Attempts a full structural decode of the BCP file and reports either a\n/// series of success checkmarks (`✓`) or a diagnostic failure line (`✗`).\n/// The command exits with code 0 on a valid file and code 1 on any error\n/// (the main dispatcher in `main.rs` converts `Err` to exit code 1).\n///\n/// # Success output\n///\n/// ```text\n/// ✓ Header: valid (BCP v1.0)\n/// ✓ Blocks: 4 blocks parsed successfully\n/// ✓ Sentinel: END block present\n/// ✓ Integrity: all block bodies parse without error\n/// ```\n///\n/// # Failure output\n///\n/// ```text\n/// ✗ Error: invalid header — invalid magic number: expected 0x42435000, got 0xDEADBEEF\n/// ```\n///\n/// # Validation steps\n///\n/// The validate command runs a single `BcpDecoder::decode` call, which\n/// covers all four structural layers defined in RFC §4:\n///\n/// ```text\n/// 1. Header      — magic number, version, reserved byte\n/// 2. Decompression — whole-payload zstd (if compressed flag set)\n/// 3. Block frames — block_type varint, flags byte, content_len varint, body\n/// 4. Block bodies — TLV field deserialization for each typed block\n/// ```\n///\n/// A file that passes all four steps is considered structurally valid.\n/// Semantic validity (e.g. referential integrity between annotations and\n/// their target blocks) is out of scope for the `PoC`.\nuse std::fs;\n\nuse anyhow::{Context, Result, anyhow};\nuse bcp_decoder::{DecodeError, BcpDecoder};\n\nuse crate::ValidateArgs;\n\n/// Run the `bcp validate` command.\n///\n/// Prints a validation report to stdout and returns `Ok(())` on success.\n/// On any structural error, prints a `✗` diagnostic to stdout and returns\n/// `Err`, which the main dispatcher converts to exit code 1.\n///\n/// # Errors\n///\n/// Returns an error if the file cannot be read, or if the BCP payload\n/// fails any structural validation check.\npub fn run(args: &ValidateArgs) -> Result<()> {\n    let bytes =\n        fs::read(&args.file).with_context(|| format!(\"cannot read {}\", args.file.display()))?;\n\n    match BcpDecoder::decode(&bytes) {\n        Ok(decoded) => {\n            let header = &decoded.header;\n            println!(\n                \"✓ Header: valid (BCP v{}.{})\",\n                header.version_major, header.version_minor\n            );\n            println!(\n                \"✓ Blocks: {} block{} parsed successfully\",\n                decoded.blocks.len(),\n                if decoded.blocks.len() == 1 { \"\" } else { \"s\" }\n            );\n            println!(\"✓ Sentinel: END block present\");\n            println!(\"✓ Integrity: all block bodies parse without error\");\n            Ok(())\n        }\n\n        Err(e) => {\n            let diagnostic = decode_error_diagnostic(&e);\n            println!(\"✗ Error: {diagnostic}\");\n            Err(anyhow!(\"validation failed\"))\n        }\n    }\n}\n\n// ── Error formatting ──────────────────────────────────────────────────────────\n\n/// Converts a `DecodeError` into a human-readable diagnostic string.\n///\n/// Maps each error variant to a message that mirrors the format shown in\n/// `SPEC_09` §3, giving enough context for the user to locate the problem.\n///\n/// ```text\n/// ┌──────────────────────────┬──────────────────────────────────────────┐\n/// │ DecodeError variant      │ Diagnostic message prefix                │\n/// ├──────────────────────────┼──────────────────────────────────────────┤\n/// │ InvalidHeader            │ \"invalid header — <inner error>\"         │\n/// │ MissingEndSentinel       │ \"missing END sentinel\"                   │\n/// │ TrailingData             │ \"trailing data after END ({n} bytes)\"    │\n/// │ MissingContentStore      │ \"content-addressed block with no store\"  │\n/// │ UnresolvedReference      │ \"unresolved BLAKE3 reference\"            │\n/// │ Wire / Type / Decompress │ \"<error Display>\"                        │\n/// └──────────────────────────┴──────────────────────────────────────────┘\n/// ```\nfn decode_error_diagnostic(e: &DecodeError) -> String {\n    match e {\n        DecodeError::InvalidHeader(inner) => format!(\"invalid header — {inner}\"),\n        DecodeError::MissingEndSentinel => \"missing END sentinel\".to_string(),\n        DecodeError::TrailingData { extra_bytes } => {\n            format!(\"trailing data after END ({extra_bytes} unexpected bytes)\")\n        }\n        DecodeError::MissingContentStore => {\n            \"content-addressed reference block found but no content store available\".to_string()\n        }\n        DecodeError::UnresolvedReference { hash } => {\n            let hex: String = hash\n                .iter()\n                .take(8)\n                .fold(String::with_capacity(16), |mut s, b| {\n                    use std::fmt::Write as _;\n                    let _ = write!(s, \"{b:02x}\");\n                    s\n                });\n            format!(\"unresolved BLAKE3 reference {hex}…\")\n        }\n        other => other.to_string(),\n    }\n}\n",
      "language": "rust",
      "path": "bcp-cli/src/cmd_validate.rs",
      "type": "code"
    },
    {
      "content": "/// BCP command-line tool — inspect, validate, encode, decode, and analyse\n/// `.bcp` files produced by the Bit Context Protocol stack.\n///\n/// # Command overview\n///\n/// ```text\n/// bcp <COMMAND> [OPTIONS]\n///\n/// Commands:\n///   inspect    Print a human-readable block summary of a BCP file\n///   validate   Check a BCP file for structural correctness\n///   encode     Create a BCP file from a JSON manifest\n///   decode     Render a BCP file as model-ready text\n///   stats      Print size and token-efficiency statistics\n///   help       Print help information\n///\n/// Global options:\n///   -v, --verbose    Enable verbose output\n///   --no-color       Disable coloured output (Unicode glyphs still used)\n///   -h, --help       Print help\n///   -V, --version    Print version\n/// ```\n///\n/// # Exit codes\n///\n/// | Code | Meaning                                 |\n/// |------|-----------------------------------------|\n/// | 0    | Success                                 |\n/// | 1    | Error (I/O failure, invalid file, etc.) |\n///\n/// All error details are written to stderr so stdout can be piped cleanly.\nuse std::path::PathBuf;\nuse std::process;\n\nuse clap::{Parser, Subcommand};\n\nmod cmd_decode;\nmod cmd_encode;\nmod cmd_inspect;\nmod cmd_stats;\nmod cmd_validate;\n\n// ── CLI root ──────────────────────────────────────────────────────────────────\n\n/// The BCP (Bit Context Protocol) command-line tool.\n///\n/// Inspect, validate, encode, decode, and analyse `.bcp` binary payloads.\n#[derive(Parser)]\n#[command(name = \"bcp\", version, about = \"Bit Context Protocol CLI\")]\nstruct Cli {\n    #[command(subcommand)]\n    command: Commands,\n\n    /// Enable verbose output (show extra decode/render details).\n    #[arg(short, long, global = true)]\n    verbose: bool,\n\n    /// Disable coloured output (ANSI escape codes are suppressed).\n    #[arg(long, global = true)]\n    no_color: bool,\n}\n\n// ── Sub-commands ──────────────────────────────────────────────────────────────\n\n#[derive(Subcommand)]\nenum Commands {\n    /// Print a human-readable summary of each block in a BCP file.\n    Inspect(InspectArgs),\n    /// Check a BCP file for structural correctness.\n    Validate(ValidateArgs),\n    /// Create a BCP file from a JSON manifest.\n    Encode(EncodeArgs),\n    /// Render a BCP file as model-ready text.\n    Decode(DecodeArgs),\n    /// Print size and token-efficiency statistics.\n    Stats(StatsArgs),\n}\n\n// ── Argument structs ──────────────────────────────────────────────────────────\n\n/// Arguments for `bcp inspect`.\n///\n/// Reads and decodes a BCP file, then prints a human-readable summary of\n/// every block (or a single block when `--block` is set). Useful for\n/// quickly understanding what context a payload contains without writing\n/// custom tooling.\n///\n/// ```text\n/// ┌─────────────┬────────────────────────────────────────────────────────┐\n/// │ Flag        │ Effect                                                 │\n/// ├─────────────┼────────────────────────────────────────────────────────┤\n/// │ --show-body │ Include first 80 chars of block content (UTF-8 lossy) │\n/// │ --show-hex  │ Include 16-byte-per-line hex dump of block body       │\n/// │ --block N   │ Show only the block at index N                        │\n/// └─────────────┴────────────────────────────────────────────────────────┘\n/// ```\n#[derive(clap::Args)]\npub struct InspectArgs {\n    /// Path to the `.bcp` file to inspect.\n    pub file: PathBuf,\n\n    /// Show block body content (first 80 characters, UTF-8 lossy).\n    #[arg(long)]\n    pub show_body: bool,\n\n    /// Show raw hex dump of block bodies (16 bytes per line).\n    #[arg(long)]\n    pub show_hex: bool,\n\n    /// Inspect only the block at this zero-based index.\n    #[arg(long)]\n    pub block: Option<usize>,\n}\n\n/// Arguments for `bcp validate`.\n///\n/// Attempts a full decode of the BCP file and reports either a set of\n/// success checkmarks or a diagnostic error. The process exits with code 0\n/// on success and code 1 on any structural problem.\n#[derive(clap::Args)]\npub struct ValidateArgs {\n    /// Path to the `.bcp` file to validate.\n    pub file: PathBuf,\n}\n\n/// Arguments for `bcp encode`.\n///\n/// Reads a JSON manifest describing the blocks to encode, then serialises\n/// them into a BCP binary payload. The manifest format is:\n///\n/// ```json\n/// {\n///   \"blocks\": [\n///     { \"type\": \"code\",         \"lang\": \"rust\", \"path\": \"src/main.rs\",\n///       \"content\": \"fn main() {}\", \"summary\": \"Entry point\" },\n///     { \"type\": \"conversation\", \"role\": \"user\",\n///       \"content\": \"Fix the timeout bug.\" }\n///   ]\n/// }\n/// ```\n///\n/// The `content_file` key may substitute `content` for code blocks — the\n/// encoder reads the file at the given path relative to the manifest.\n///\n/// ```text\n/// ┌──────────────────┬────────────────────────────────────────────────┐\n/// │ Flag             │ Effect                                         │\n/// ├──────────────────┼────────────────────────────────────────────────┤\n/// │ --compress-blocks  │ zstd-compress each block body individually   │\n/// │ --compress-payload │ zstd-compress everything after the header    │\n/// │ --dedup            │ BLAKE3 content-addressing + deduplication    │\n/// └──────────────────┴────────────────────────────────────────────────┘\n/// ```\n#[derive(clap::Args)]\npub struct EncodeArgs {\n    /// Path to the JSON manifest file describing the blocks to encode.\n    pub input: PathBuf,\n\n    /// Output `.bcp` file path.\n    #[arg(short, long)]\n    pub output: PathBuf,\n\n    /// Enable per-block zstd compression (each block body independently).\n    #[arg(long)]\n    pub compress_blocks: bool,\n\n    /// Enable whole-payload zstd compression (all blocks as one stream).\n    #[arg(long)]\n    pub compress_payload: bool,\n\n    /// Enable BLAKE3 content-addressed deduplication.\n    #[arg(long)]\n    pub dedup: bool,\n}\n\n/// Arguments for `bcp decode`.\n///\n/// Decodes a BCP file and renders the blocks as model-ready text on stdout\n/// (or to a file). The output format, verbosity, token budget, and block\n/// type filter are all configurable.\n///\n/// ```text\n/// ┌─────────────┬──────────────────────────────────────────────────────┐\n/// │ Flag        │ Values / default                                     │\n/// ├─────────────┼──────────────────────────────────────────────────────┤\n/// │ --mode      │ xml (default) | markdown | minimal                   │\n/// │ --verbosity │ full | summary | adaptive (default)                  │\n/// │ --budget    │ approximate token count (none = no limit)            │\n/// │ --include   │ comma-separated block types to render                │\n/// │ -o / --output │ write to file instead of stdout                   │\n/// └─────────────┴──────────────────────────────────────────────────────┘\n/// ```\n#[derive(clap::Args)]\npub struct DecodeArgs {\n    /// Path to the `.bcp` file to decode.\n    pub file: PathBuf,\n\n    /// Output format: `xml`, `markdown`, or `minimal`.\n    #[arg(long, default_value = \"xml\")]\n    pub mode: String,\n\n    /// Token budget for adaptive rendering.\n    ///\n    /// When set, the driver uses the budget engine (RFC §5.5) to prefer full\n    /// content for high-priority blocks and summaries / placeholders for\n    /// lower-priority blocks until the budget is exhausted.\n    #[arg(long)]\n    pub budget: Option<u32>,\n\n    /// Verbosity: `full`, `summary`, or `adaptive` (default).\n    #[arg(long, default_value = \"adaptive\")]\n    pub verbosity: String,\n\n    /// Comma-separated list of block types to include (e.g. `code,conversation`).\n    ///\n    /// When set, only blocks of matching types appear in the output.\n    /// Recognised names: `code`, `conversation`, `file_tree`, `tool_result`,\n    /// `document`, `structured_data`, `diff`, `annotation`, `image`, `extension`.\n    #[arg(long)]\n    pub include: Option<String>,\n\n    /// Write rendered output to this file instead of stdout.\n    #[arg(short, long)]\n    pub output: Option<PathBuf>,\n}\n\n/// Arguments for `bcp stats`.\n///\n/// Decodes a BCP file and prints size, block-type distribution, compression\n/// ratio (when applicable), and heuristic token estimates for multiple render\n/// modes. Useful for evaluating the token-efficiency impact of a payload.\n#[derive(clap::Args)]\npub struct StatsArgs {\n    /// Path to the `.bcp` file to analyse.\n    pub file: PathBuf,\n}\n\n// ── Entry point ───────────────────────────────────────────────────────────────\n\nfn main() {\n    let cli = Cli::parse();\n\n    let result = match cli.command {\n        Commands::Inspect(args) => cmd_inspect::run(&args),\n        Commands::Validate(args) => cmd_validate::run(&args),\n        Commands::Encode(args) => cmd_encode::run(&args),\n        Commands::Decode(args) => cmd_decode::run(&args),\n        Commands::Stats(args) => cmd_stats::run(&args),\n    };\n\n    if let Err(e) = result {\n        eprintln!(\"error: {e:#}\");\n        process::exit(1);\n    }\n}\n",
      "language": "rust",
      "path": "bcp-cli/src/main.rs",
      "type": "code"
    },
    {
      "content": "[package]\nname = \"bcp-decoder\"\nversion = \"0.1.0\"\nedition = \"2024\"\n\n[dependencies]\nbcp-wire = { path = \"../bcp-wire\" }\nbcp-types = { path = \"../bcp-types\" }\nthiserror = { workspace = true }\ntokio = { workspace = true }\nzstd = { workspace = true }\n\n[dev-dependencies]\nbcp-encoder = { path = \"../bcp-encoder\" }\n",
      "language": "toml",
      "path": "bcp-decoder/Cargo.toml",
      "type": "code"
    },
    {
      "content": "use bcp_types::fields::{\n    FieldWireType, decode_bytes_value, decode_field_header, decode_varint_value,\n};\n\nuse crate::error::DecodeError;\n\n/// A raw TLV field before type-specific interpretation.\n///\n/// Produced by [`BlockReader::next_field`]. The caller matches on\n/// `field_id` to decide which struct field to populate, and uses\n/// `wire_type` to interpret `data`:\n///\n///   - `Varint`: `data` contains the raw varint bytes (use\n///     [`decode_varint_value`] to extract the `u64`).\n///   - `Bytes` / `Nested`: `data` is the length-prefixed payload\n///     (the length prefix has already been consumed).\n///\n/// Unknown field IDs should be silently skipped for forward\n/// compatibility.\npub struct RawField<'a> {\n    pub field_id: u64,\n    pub wire_type: FieldWireType,\n    pub data: &'a [u8],\n}\n\n/// Cursor-based TLV field reader for block bodies.\n///\n/// `BlockReader` wraps a byte slice and provides an iterator-like\n/// interface for consuming TLV fields one at a time. It delegates\n/// to the decode functions in `bcp_types::fields` but adds a\n/// stateful cursor so the caller doesn't have to manually track\n/// offsets.\n///\n/// This is an internal implementation detail of the decoder — it is\n/// not part of the public API.\n///\n/// # Usage pattern\n///\n/// ```text\n///   let mut reader = BlockReader::new(body);\n///   while let Some(field) = reader.next_field()? {\n///       match field.field_id {\n///           1 => { /* handle field 1 */ }\n///           2 => { /* handle field 2 */ }\n///           _ => { /* skip unknown */ }\n///       }\n///   }\n/// ```\npub struct BlockReader<'a> {\n    buf: &'a [u8],\n    pos: usize,\n}\n\nimpl<'a> BlockReader<'a> {\n    /// Create a new reader over the given body bytes.\n    ///\n    /// The reader starts at position 0 and advances through the buffer\n    /// as fields are consumed via [`next_field`](Self::next_field).\n    #[must_use]\n    pub fn new(buf: &'a [u8]) -> Self {\n        Self { buf, pos: 0 }\n    }\n\n    /// Read the next TLV field from the body.\n    ///\n    /// Returns `Ok(Some(field))` if a field was successfully read, or\n    /// `Ok(None)` when the buffer is exhausted. Returns `Err` if the\n    /// field header or payload is malformed.\n    ///\n    /// For `Varint` fields, `data` points to the varint bytes in the\n    /// original buffer (the caller should use [`decode_varint_value`]\n    /// to extract the value). For `Bytes` and `Nested` fields, `data`\n    /// is the raw payload after the length prefix.\n    ///\n    /// # Errors\n    ///\n    /// Returns [`DecodeError::Type`] or [`DecodeError::Wire`] if the\n    /// field header is malformed or the payload is truncated.\n    pub fn next_field(&mut self) -> Result<Option<RawField<'a>>, DecodeError> {\n        let remaining = &self.buf[self.pos..];\n        if remaining.is_empty() {\n            return Ok(None);\n        }\n\n        let (header, header_len) = decode_field_header(remaining)?;\n        let payload_buf = &remaining[header_len..];\n\n        let (data, payload_consumed) = match header.wire_type {\n            FieldWireType::Varint => {\n                // For varints, we return the raw varint bytes so the caller\n                // can decode the value. We need to know how many bytes it\n                // consumed to advance the cursor.\n                let (_, n) = decode_varint_value(payload_buf)?;\n                (&payload_buf[..n], n)\n            }\n            FieldWireType::Bytes | FieldWireType::Nested => {\n                // For bytes/nested, decode_bytes_value returns the inner\n                // slice (after length prefix) and total bytes consumed.\n                let (inner, n) = decode_bytes_value(payload_buf)?;\n                (inner, n)\n            }\n        };\n\n        self.pos += header_len + payload_consumed;\n\n        Ok(Some(RawField {\n            field_id: header.field_id,\n            wire_type: header.wire_type,\n            data,\n        }))\n    }\n\n    /// Return the number of bytes consumed so far.\n    #[must_use]\n    pub fn position(&self) -> usize {\n        self.pos\n    }\n\n    /// Return the remaining unread bytes.\n    #[must_use]\n    pub fn remaining(&self) -> &'a [u8] {\n        &self.buf[self.pos..]\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use bcp_types::fields::{encode_bytes_field, encode_nested_field, encode_varint_field};\n\n    #[test]\n    fn empty_buffer_returns_none() {\n        let mut reader = BlockReader::new(&[]);\n        assert!(reader.next_field().unwrap().is_none());\n    }\n\n    #[test]\n    fn reads_varint_field() {\n        let mut buf = Vec::new();\n        encode_varint_field(&mut buf, 1, 42);\n\n        let mut reader = BlockReader::new(&buf);\n        let field = reader.next_field().unwrap().unwrap();\n        assert_eq!(field.field_id, 1);\n        assert_eq!(field.wire_type, FieldWireType::Varint);\n\n        // Decode the varint value from the raw bytes\n        let (value, _) = decode_varint_value(field.data).unwrap();\n        assert_eq!(value, 42);\n\n        assert!(reader.next_field().unwrap().is_none());\n    }\n\n    #[test]\n    fn reads_bytes_field() {\n        let mut buf = Vec::new();\n        encode_bytes_field(&mut buf, 2, b\"hello\");\n\n        let mut reader = BlockReader::new(&buf);\n        let field = reader.next_field().unwrap().unwrap();\n        assert_eq!(field.field_id, 2);\n        assert_eq!(field.wire_type, FieldWireType::Bytes);\n        assert_eq!(field.data, b\"hello\");\n\n        assert!(reader.next_field().unwrap().is_none());\n    }\n\n    #[test]\n    fn reads_nested_field() {\n        let mut inner = Vec::new();\n        encode_varint_field(&mut inner, 1, 99);\n\n        let mut buf = Vec::new();\n        encode_nested_field(&mut buf, 3, &inner);\n\n        let mut reader = BlockReader::new(&buf);\n        let field = reader.next_field().unwrap().unwrap();\n        assert_eq!(field.field_id, 3);\n        assert_eq!(field.wire_type, FieldWireType::Nested);\n        assert_eq!(field.data, &inner);\n    }\n\n    #[test]\n    fn reads_multiple_fields_sequentially() {\n        let mut buf = Vec::new();\n        encode_varint_field(&mut buf, 1, 7);\n        encode_bytes_field(&mut buf, 2, b\"world\");\n        encode_varint_field(&mut buf, 3, 256);\n\n        let mut reader = BlockReader::new(&buf);\n\n        let f1 = reader.next_field().unwrap().unwrap();\n        assert_eq!(f1.field_id, 1);\n\n        let f2 = reader.next_field().unwrap().unwrap();\n        assert_eq!(f2.field_id, 2);\n        assert_eq!(f2.data, b\"world\");\n\n        let f3 = reader.next_field().unwrap().unwrap();\n        assert_eq!(f3.field_id, 3);\n\n        assert!(reader.next_field().unwrap().is_none());\n        assert_eq!(reader.position(), buf.len());\n    }\n\n    #[test]\n    fn position_tracks_correctly() {\n        let mut buf = Vec::new();\n        encode_varint_field(&mut buf, 1, 42);\n        encode_bytes_field(&mut buf, 2, b\"hi\");\n\n        let mut reader = BlockReader::new(&buf);\n        assert_eq!(reader.position(), 0);\n\n        reader.next_field().unwrap();\n        let mid = reader.position();\n        assert!(mid > 0 && mid < buf.len());\n\n        reader.next_field().unwrap();\n        assert_eq!(reader.position(), buf.len());\n        assert!(reader.remaining().is_empty());\n    }\n}\n",
      "language": "rust",
      "path": "bcp-decoder/src/block_reader.rs",
      "type": "code"
    },
    {
      "content": "use bcp_types::block::{Block, BlockContent};\nuse bcp_types::block_type::BlockType;\nuse bcp_types::content_store::ContentStore;\nuse bcp_types::summary::Summary;\nuse bcp_wire::block_frame::BlockFrame;\nuse bcp_wire::header::{HEADER_SIZE, BcpHeader};\n\nuse crate::decompression::{self, MAX_BLOCK_DECOMPRESSED_SIZE, MAX_PAYLOAD_DECOMPRESSED_SIZE};\nuse crate::error::DecodeError;\n\n/// The result of decoding a BCP payload.\n///\n/// Contains the parsed file header and an ordered sequence of typed\n/// blocks. The END sentinel is consumed during decoding and is not\n/// included in the `blocks` vector.\n///\n/// ```text\n/// ┌──────────────────────────────────────────────────┐\n/// │ DecodedPayload                                   │\n/// │   header: BcpHeader  ← version, flags            │\n/// │   blocks: Vec<Block> ← ordered content blocks    │\n/// └──────────────────────────────────────────────────┘\n/// ```\npub struct DecodedPayload {\n    /// The parsed file header (magic validated, version checked).\n    pub header: BcpHeader,\n\n    /// Ordered sequence of blocks, excluding the END sentinel.\n    ///\n    /// Block ordering matches the wire order. Annotation blocks\n    /// appear at whatever position the encoder placed them, with\n    /// `target_block_id` referencing earlier blocks by index.\n    pub blocks: Vec<Block>,\n}\n\n/// Synchronous BCP decoder — parses a complete in-memory payload.\n///\n/// The decoder reads an entire BCP payload from a byte slice and\n/// produces a [`DecodedPayload`] containing the header and all typed\n/// blocks. It is the inverse of\n/// `BcpEncoder::encode` from the `bcp-encoder` crate.\n///\n/// Decoding proceeds in four steps:\n///\n///   1. **Header**: Validate and parse the 8-byte file header (magic\n///      number, version, flags, reserved byte).\n///   2. **Whole-payload decompression**: If the header's `COMPRESSED`\n///      flag (bit 0) is set, decompress all bytes after the header\n///      with zstd before parsing block frames.\n///   3. **Block frames**: Iterate block frames by reading `BlockFrame`\n///      envelopes. For each frame:\n///      - If `COMPRESSED` (bit 1): decompress the body with zstd.\n///      - If `IS_REFERENCE` (bit 2): resolve the 32-byte BLAKE3 hash\n///        against the content store to recover the original body.\n///      - Extract the summary sub-block if `HAS_SUMMARY` (bit 0) is set.\n///      - Deserialize the body into the corresponding `BlockContent`.\n///   4. **Termination**: Stop when an END sentinel (type=0xFF) is\n///      encountered. Detect and report trailing data after the sentinel.\n///\n/// Unknown block types are captured as `BlockContent::Unknown` and do\n/// not cause errors — this is the forward compatibility guarantee from\n/// RFC §3, P1 Schema Evolution.\n///\n/// # Example\n///\n/// ```rust\n/// use bcp_encoder::BcpEncoder;\n/// use bcp_decoder::BcpDecoder;\n/// use bcp_types::enums::{Lang, Role};\n///\n/// let payload = BcpEncoder::new()\n///     .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n///     .add_conversation(Role::User, b\"hello\")\n///     .encode()\n///     .unwrap();\n///\n/// let decoded = BcpDecoder::decode(&payload).unwrap();\n/// assert_eq!(decoded.blocks.len(), 2);\n/// ```\npub struct BcpDecoder;\n\nimpl BcpDecoder {\n    /// Decode a complete BCP payload from a byte slice.\n    ///\n    /// This is the standard entry point for payloads that do not contain\n    /// content-addressed (reference) blocks. If the payload contains\n    /// blocks with the `IS_REFERENCE` flag, use\n    /// [`decode_with_store`](Self::decode_with_store) instead.\n    ///\n    /// Handles whole-payload and per-block zstd decompression\n    /// transparently.\n    ///\n    /// # Errors\n    ///\n    /// - [`DecodeError::InvalidHeader`] if the magic, version, or reserved\n    ///   byte is wrong.\n    /// - [`DecodeError::Wire`] if a block frame is malformed.\n    /// - [`DecodeError::Type`] if a block body fails TLV deserialization.\n    /// - [`DecodeError::DecompressFailed`] if zstd decompression fails.\n    /// - [`DecodeError::DecompressionBomb`] if decompressed size exceeds\n    ///   the safety limit.\n    /// - [`DecodeError::MissingContentStore`] if a reference block is\n    ///   encountered (use `decode_with_store` instead).\n    /// - [`DecodeError::MissingEndSentinel`] if the payload ends without an\n    ///   END block.\n    /// - [`DecodeError::TrailingData`] if extra bytes follow the END\n    ///   sentinel.\n    pub fn decode(payload: &[u8]) -> Result<DecodedPayload, DecodeError> {\n        Self::decode_inner(payload, None)\n    }\n\n    /// Decode a payload that may contain content-addressed blocks.\n    ///\n    /// Same as [`decode`](Self::decode), but accepts a [`ContentStore`]\n    /// for resolving `IS_REFERENCE` blocks. When a block's body is a\n    /// 32-byte BLAKE3 hash, the decoder looks it up in the store to\n    /// retrieve the original body bytes.\n    ///\n    /// # Errors\n    ///\n    /// All errors from [`decode`](Self::decode), plus:\n    /// - [`DecodeError::UnresolvedReference`] if a hash is not found in\n    ///   the content store.\n    pub fn decode_with_store(\n        payload: &[u8],\n        store: &dyn ContentStore,\n    ) -> Result<DecodedPayload, DecodeError> {\n        Self::decode_inner(payload, Some(store))\n    }\n\n    /// Shared decode implementation.\n    fn decode_inner(\n        payload: &[u8],\n        store: Option<&dyn ContentStore>,\n    ) -> Result<DecodedPayload, DecodeError> {\n        // 1. Parse the 8-byte header.\n        let header = BcpHeader::read_from(payload).map_err(DecodeError::InvalidHeader)?;\n\n        // 2. Whole-payload decompression.\n        let block_data: std::borrow::Cow<'_, [u8]> = if header.flags.is_compressed() {\n            let compressed = &payload[HEADER_SIZE..];\n            let decompressed =\n                decompression::decompress(compressed, MAX_PAYLOAD_DECOMPRESSED_SIZE)?;\n            std::borrow::Cow::Owned(decompressed)\n        } else {\n            std::borrow::Cow::Borrowed(&payload[HEADER_SIZE..])\n        };\n\n        let mut cursor = 0;\n        let mut blocks = Vec::new();\n        let mut found_end = false;\n\n        // 3. Read block frames until END sentinel or EOF.\n        while cursor < block_data.len() {\n            let remaining = &block_data[cursor..];\n\n            if let Some((frame, consumed)) = BlockFrame::read_from(remaining)? {\n                let block = Self::decode_block_frame(&frame, store)?;\n                blocks.push(block);\n                cursor += consumed;\n            } else {\n                // END sentinel encountered. BlockFrame::read_from returns\n                // None for type=0xFF. Account for the END frame bytes:\n                // varint(0xFF) = [0xFF, 0x01] + flags(0x00) + content_len(0x00) = 4 bytes.\n                // But we need to calculate the actual size consumed by the\n                // END sentinel's varint encoding.\n                found_end = true;\n                cursor += Self::end_sentinel_size(remaining)?;\n                break;\n            }\n        }\n\n        // 4. Validate termination.\n        if !found_end {\n            return Err(DecodeError::MissingEndSentinel);\n        }\n\n        if cursor < block_data.len() {\n            return Err(DecodeError::TrailingData {\n                extra_bytes: block_data.len() - cursor,\n            });\n        }\n\n        Ok(DecodedPayload { header, blocks })\n    }\n\n    /// Decode a single block from a `BlockFrame`.\n    ///\n    /// Processing pipeline:\n    ///   1. If `IS_REFERENCE`: resolve the 32-byte hash via content store.\n    ///   2. If `COMPRESSED`: decompress the body with zstd.\n    ///   3. If `HAS_SUMMARY`: extract the summary from the front of the body.\n    ///   4. Deserialize the TLV body into a `BlockContent` variant.\n    fn decode_block_frame(\n        frame: &BlockFrame,\n        store: Option<&dyn ContentStore>,\n    ) -> Result<Block, DecodeError> {\n        let block_type = BlockType::from_wire_id(frame.block_type);\n\n        // Stage 1: Resolve content-addressed references.\n        let resolved_body = if frame.flags.is_reference() {\n            let store = store.ok_or(DecodeError::MissingContentStore)?;\n            if frame.body.len() != 32 {\n                return Err(DecodeError::Wire(bcp_wire::WireError::UnexpectedEof {\n                    offset: frame.body.len(),\n                }));\n            }\n            // Safe: we just checked that body.len() == 32\n            let hash: [u8; 32] = frame.body[..32].try_into().unwrap();\n            store\n                .get(&hash)\n                .ok_or(DecodeError::UnresolvedReference { hash })?\n        } else {\n            frame.body.clone()\n        };\n\n        // Stage 2: Decompress if needed.\n        let decompressed_body = if frame.flags.is_compressed() {\n            decompression::decompress(&resolved_body, MAX_BLOCK_DECOMPRESSED_SIZE)?\n        } else {\n            resolved_body\n        };\n\n        // Stage 3 & 4: Summary extraction + TLV body decode.\n        let mut body = decompressed_body.as_slice();\n        let mut summary = None;\n\n        if frame.flags.has_summary() {\n            let (sum, consumed) = Summary::decode(body)?;\n            summary = Some(sum);\n            body = &body[consumed..];\n        }\n\n        let content = BlockContent::decode_body(&block_type, body)?;\n\n        Ok(Block {\n            block_type,\n            flags: frame.flags,\n            summary,\n            content,\n        })\n    }\n\n    /// Calculate the byte size of the END sentinel in the wire format.\n    ///\n    /// The END sentinel is:\n    ///   - `block_type` = 0xFF, encoded as varint → `[0xFF, 0x01]` (2 bytes)\n    ///   - `flags` = 0x00 (1 byte)\n    ///   - `content_len` = 0, encoded as varint → `[0x00]` (1 byte)\n    ///\n    /// Total: 4 bytes. However, we compute this from the wire rather\n    /// than hardcoding, in case future encoders use a different varint\n    /// encoding width.\n    fn end_sentinel_size(buf: &[u8]) -> Result<usize, DecodeError> {\n        // Read the block_type varint (0xFF → encodes as [0xFF, 0x01])\n        let (_, type_len) = bcp_wire::varint::decode_varint(buf)?;\n        let mut size = type_len;\n\n        // flags byte\n        if size >= buf.len() {\n            return Err(DecodeError::Wire(bcp_wire::WireError::UnexpectedEof {\n                offset: size,\n            }));\n        }\n        size += 1;\n\n        // content_len varint (should be 0)\n        let rest = buf.get(size..).ok_or(DecodeError::Wire(\n            bcp_wire::WireError::UnexpectedEof { offset: size },\n        ))?;\n        if rest.is_empty() {\n            return Err(DecodeError::Wire(bcp_wire::WireError::UnexpectedEof {\n                offset: size,\n            }));\n        }\n        let (_, len_size) = bcp_wire::varint::decode_varint(rest)?;\n        size += len_size;\n\n        Ok(size)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use bcp_encoder::BcpEncoder;\n    use bcp_types::diff::DiffHunk;\n    use bcp_types::enums::{\n        AnnotationKind, DataFormat, FormatHint, Lang, MediaType, Priority, Role, Status,\n    };\n    use bcp_types::file_tree::{FileEntry, FileEntryKind};\n    use bcp_wire::block_frame::{BlockFlags, BlockFrame};\n\n    // ── Round-trip helpers ────────────────────────────────────────────────\n\n    /// Encode with `BcpEncoder`, decode with `BcpDecoder`, return blocks.\n    fn roundtrip(encoder: &BcpEncoder) -> DecodedPayload {\n        let payload = encoder.encode().unwrap();\n        BcpDecoder::decode(&payload).unwrap()\n    }\n\n    // ── Acceptance criteria tests ─────────────────────────────────────────\n\n    #[test]\n    fn decode_parses_encoder_output() {\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .encode()\n            .unwrap();\n\n        let decoded = BcpDecoder::decode(&payload).unwrap();\n        assert_eq!(decoded.blocks.len(), 1);\n        assert_eq!(decoded.header.version_major, 1);\n        assert_eq!(decoded.header.version_minor, 0);\n    }\n\n    #[test]\n    fn roundtrip_single_code_block() {\n        let decoded =\n            roundtrip(BcpEncoder::new().add_code(Lang::Rust, \"lib.rs\", b\"pub fn hello() {}\"));\n\n        assert_eq!(decoded.blocks.len(), 1);\n        let block = &decoded.blocks[0];\n        assert_eq!(block.block_type, BlockType::Code);\n        assert!(block.summary.is_none());\n\n        match &block.content {\n            BlockContent::Code(code) => {\n                assert_eq!(code.lang, Lang::Rust);\n                assert_eq!(code.path, \"lib.rs\");\n                assert_eq!(code.content, b\"pub fn hello() {}\");\n                assert!(code.line_range.is_none());\n            }\n            other => panic!(\"expected Code, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn roundtrip_multiple_block_types() {\n        let decoded = roundtrip(\n            BcpEncoder::new()\n                .add_code(Lang::Python, \"app.py\", b\"print('hi')\")\n                .add_conversation(Role::User, b\"What is this?\")\n                .add_conversation(Role::Assistant, b\"A greeting script.\")\n                .add_tool_result(\"pytest\", Status::Ok, b\"1 passed\")\n                .add_document(\"README\", b\"# Hello\", FormatHint::Markdown),\n        );\n\n        assert_eq!(decoded.blocks.len(), 5);\n\n        // Verify type ordering matches encoder order\n        let types: Vec<_> = decoded\n            .blocks\n            .iter()\n            .map(|b| b.block_type.clone())\n            .collect();\n        assert_eq!(\n            types,\n            vec![\n                BlockType::Code,\n                BlockType::Conversation,\n                BlockType::Conversation,\n                BlockType::ToolResult,\n                BlockType::Document,\n            ]\n        );\n    }\n\n    #[test]\n    fn roundtrip_with_summary() {\n        let decoded = roundtrip(\n            BcpEncoder::new()\n                .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n                .with_summary(\"Application entry point.\").unwrap(),\n        );\n\n        assert_eq!(decoded.blocks.len(), 1);\n        let block = &decoded.blocks[0];\n        assert!(block.flags.has_summary());\n        assert_eq!(\n            block.summary.as_ref().unwrap().text,\n            \"Application entry point.\"\n        );\n\n        // The content should still decode correctly\n        match &block.content {\n            BlockContent::Code(code) => {\n                assert_eq!(code.path, \"main.rs\");\n            }\n            other => panic!(\"expected Code, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn roundtrip_with_priority_annotation() {\n        let decoded = roundtrip(\n            BcpEncoder::new()\n                .add_code(Lang::Rust, \"lib.rs\", b\"// code\")\n                .with_priority(Priority::High).unwrap(),\n        );\n\n        // Encoder produces CODE + ANNOTATION blocks\n        assert_eq!(decoded.blocks.len(), 2);\n        assert_eq!(decoded.blocks[0].block_type, BlockType::Code);\n        assert_eq!(decoded.blocks[1].block_type, BlockType::Annotation);\n\n        match &decoded.blocks[1].content {\n            BlockContent::Annotation(ann) => {\n                assert_eq!(ann.target_block_id, 0);\n                assert_eq!(ann.kind, AnnotationKind::Priority);\n                assert_eq!(ann.value, vec![Priority::High.to_wire_byte()]);\n            }\n            other => panic!(\"expected Annotation, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn roundtrip_all_block_types() {\n        let decoded = roundtrip(\n            BcpEncoder::new()\n                .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n                .add_conversation(Role::User, b\"hello\")\n                .add_file_tree(\n                    \"/project\",\n                    vec![FileEntry {\n                        name: \"lib.rs\".to_string(),\n                        kind: FileEntryKind::File,\n                        size: 100,\n                        children: vec![],\n                    }],\n                )\n                .add_tool_result(\"rg\", Status::Ok, b\"3 matches\")\n                .add_document(\"README\", b\"# Title\", FormatHint::Markdown)\n                .add_structured_data(DataFormat::Json, b\"{\\\"key\\\": \\\"val\\\"}\")\n                .add_diff(\n                    \"src/lib.rs\",\n                    vec![DiffHunk {\n                        old_start: 1,\n                        new_start: 1,\n                        lines: b\"+new line\\n\".to_vec(),\n                    }],\n                )\n                .add_annotation(0, AnnotationKind::Tag, b\"important\")\n                .add_image(MediaType::Png, \"screenshot\", b\"\\x89PNG\")\n                .add_extension(\"myco\", \"custom\", b\"data\"),\n        );\n\n        assert_eq!(decoded.blocks.len(), 10);\n        let types: Vec<_> = decoded\n            .blocks\n            .iter()\n            .map(|b| b.block_type.clone())\n            .collect();\n        assert_eq!(\n            types,\n            vec![\n                BlockType::Code,\n                BlockType::Conversation,\n                BlockType::FileTree,\n                BlockType::ToolResult,\n                BlockType::Document,\n                BlockType::StructuredData,\n                BlockType::Diff,\n                BlockType::Annotation,\n                BlockType::Image,\n                BlockType::Extension,\n            ]\n        );\n    }\n\n    #[test]\n    fn roundtrip_code_with_line_range() {\n        let decoded = roundtrip(BcpEncoder::new().add_code_range(\n            Lang::Rust,\n            \"lib.rs\",\n            b\"fn foo() {}\",\n            10,\n            20,\n        ));\n\n        match &decoded.blocks[0].content {\n            BlockContent::Code(code) => {\n                assert_eq!(code.line_range, Some((10, 20)));\n            }\n            other => panic!(\"expected Code, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn roundtrip_conversation_with_tool_call_id() {\n        let decoded =\n            roundtrip(BcpEncoder::new().add_conversation_tool(Role::Tool, b\"result\", \"call_abc\"));\n\n        match &decoded.blocks[0].content {\n            BlockContent::Conversation(conv) => {\n                assert_eq!(conv.tool_call_id.as_deref(), Some(\"call_abc\"));\n            }\n            other => panic!(\"expected Conversation, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn roundtrip_preserves_all_field_values() {\n        // Comprehensive field-level round-trip for complex blocks.\n        let decoded = roundtrip(\n            BcpEncoder::new()\n                .add_file_tree(\n                    \"/project/src\",\n                    vec![\n                        FileEntry {\n                            name: \"main.rs\".to_string(),\n                            kind: FileEntryKind::File,\n                            size: 512,\n                            children: vec![],\n                        },\n                        FileEntry {\n                            name: \"lib\".to_string(),\n                            kind: FileEntryKind::Directory,\n                            size: 0,\n                            children: vec![FileEntry {\n                                name: \"utils.rs\".to_string(),\n                                kind: FileEntryKind::File,\n                                size: 128,\n                                children: vec![],\n                            }],\n                        },\n                    ],\n                )\n                .add_diff(\n                    \"Cargo.toml\",\n                    vec![\n                        DiffHunk {\n                            old_start: 5,\n                            new_start: 5,\n                            lines: b\"+tokio = \\\"1\\\"\\n\".to_vec(),\n                        },\n                        DiffHunk {\n                            old_start: 20,\n                            new_start: 21,\n                            lines: b\"-old_dep = \\\"0.1\\\"\\n+new_dep = \\\"0.2\\\"\\n\".to_vec(),\n                        },\n                    ],\n                ),\n        );\n\n        assert_eq!(decoded.blocks.len(), 2);\n\n        // Verify FileTree fields\n        match &decoded.blocks[0].content {\n            BlockContent::FileTree(tree) => {\n                assert_eq!(tree.root_path, \"/project/src\");\n                assert_eq!(tree.entries.len(), 2);\n                assert_eq!(tree.entries[0].name, \"main.rs\");\n                assert_eq!(tree.entries[0].size, 512);\n                assert_eq!(tree.entries[1].name, \"lib\");\n                assert_eq!(tree.entries[1].children.len(), 1);\n                assert_eq!(tree.entries[1].children[0].name, \"utils.rs\");\n            }\n            other => panic!(\"expected FileTree, got {other:?}\"),\n        }\n\n        // Verify Diff fields\n        match &decoded.blocks[1].content {\n            BlockContent::Diff(diff) => {\n                assert_eq!(diff.path, \"Cargo.toml\");\n                assert_eq!(diff.hunks.len(), 2);\n                assert_eq!(diff.hunks[0].old_start, 5);\n                assert_eq!(diff.hunks[1].old_start, 20);\n                assert_eq!(diff.hunks[1].new_start, 21);\n            }\n            other => panic!(\"expected Diff, got {other:?}\"),\n        }\n    }\n\n    // ── Validation tests ──────────────────────────────────────────────────\n\n    #[test]\n    fn rejects_bad_magic() {\n        let mut payload = BcpEncoder::new()\n            .add_conversation(Role::User, b\"hi\")\n            .encode()\n            .unwrap();\n\n        // Corrupt the magic bytes\n        payload[0] = b'X';\n        let result = BcpDecoder::decode(&payload);\n        assert!(matches!(result, Err(DecodeError::InvalidHeader(_))));\n    }\n\n    #[test]\n    fn rejects_truncated_header() {\n        let result = BcpDecoder::decode(&[0x4C, 0x43, 0x50, 0x00]);\n        assert!(matches!(result, Err(DecodeError::InvalidHeader(_))));\n    }\n\n    #[test]\n    fn rejects_missing_end_sentinel() {\n        let payload = BcpEncoder::new()\n            .add_conversation(Role::User, b\"hi\")\n            .encode()\n            .unwrap();\n\n        // Strip the last 4 bytes (the END sentinel)\n        let truncated = &payload[..payload.len() - 4];\n        let result = BcpDecoder::decode(truncated);\n        assert!(matches!(result, Err(DecodeError::MissingEndSentinel)));\n    }\n\n    #[test]\n    fn detects_trailing_data() {\n        let mut payload = BcpEncoder::new()\n            .add_conversation(Role::User, b\"hi\")\n            .encode()\n            .unwrap();\n\n        // Append garbage after the END sentinel\n        payload.extend_from_slice(b\"trailing garbage\");\n        let result = BcpDecoder::decode(&payload);\n        assert!(matches!(\n            result,\n            Err(DecodeError::TrailingData { extra_bytes: 16 })\n        ));\n    }\n\n    #[test]\n    fn unknown_block_type_captured_not_rejected() {\n        // Manually construct a payload with an unknown block type (0x42).\n        // We'll build: header + unknown frame + END sentinel.\n        use bcp_wire::header::HeaderFlags;\n\n        let mut payload = vec![0u8; HEADER_SIZE];\n        let header = BcpHeader::new(HeaderFlags::NONE);\n        header.write_to(&mut payload).unwrap();\n\n        // Unknown block frame: type=0x42, flags=0x00, content_len=5, body=b\"hello\"\n        let frame = BlockFrame {\n            block_type: 0x42,\n            flags: BlockFlags::NONE,\n            body: b\"hello\".to_vec(),\n        };\n        frame.write_to(&mut payload).unwrap();\n\n        // END sentinel\n        let end = BlockFrame {\n            block_type: 0xFF,\n            flags: BlockFlags::NONE,\n            body: Vec::new(),\n        };\n        end.write_to(&mut payload).unwrap();\n\n        let decoded = BcpDecoder::decode(&payload).unwrap();\n        assert_eq!(decoded.blocks.len(), 1);\n        assert_eq!(decoded.blocks[0].block_type, BlockType::Unknown(0x42));\n\n        match &decoded.blocks[0].content {\n            BlockContent::Unknown { type_id, body } => {\n                assert_eq!(*type_id, 0x42);\n                assert_eq!(body, b\"hello\");\n            }\n            other => panic!(\"expected Unknown, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn optional_fields_absent_result_in_none() {\n        let decoded = roundtrip(\n            BcpEncoder::new()\n                .add_code(Lang::Rust, \"x.rs\", b\"let x = 1;\")\n                .add_conversation(Role::User, b\"msg\"),\n        );\n\n        // Code: line_range should be None\n        match &decoded.blocks[0].content {\n            BlockContent::Code(code) => assert!(code.line_range.is_none()),\n            other => panic!(\"expected Code, got {other:?}\"),\n        }\n\n        // Conversation: tool_call_id should be None\n        match &decoded.blocks[1].content {\n            BlockContent::Conversation(conv) => assert!(conv.tool_call_id.is_none()),\n            other => panic!(\"expected Conversation, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn summary_extraction_with_body() {\n        let decoded = roundtrip(\n            BcpEncoder::new()\n                .add_document(\n                    \"Guide\",\n                    b\"# Getting Started\\n\\nWelcome!\",\n                    FormatHint::Markdown,\n                )\n                .with_summary(\"Onboarding guide for new contributors.\").unwrap(),\n        );\n\n        let block = &decoded.blocks[0];\n        assert!(block.flags.has_summary());\n        assert_eq!(\n            block.summary.as_ref().unwrap().text,\n            \"Onboarding guide for new contributors.\"\n        );\n\n        match &block.content {\n            BlockContent::Document(doc) => {\n                assert_eq!(doc.title, \"Guide\");\n                assert_eq!(doc.content, b\"# Getting Started\\n\\nWelcome!\");\n                assert_eq!(doc.format_hint, FormatHint::Markdown);\n            }\n            other => panic!(\"expected Document, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn rfc_example_roundtrip() {\n        let decoded = roundtrip(\n            BcpEncoder::new()\n                .add_code(Lang::Rust, \"src/main.rs\", b\"fn main() { todo!() }\")\n                .with_summary(\"Entry point: CLI setup and server startup.\").unwrap()\n                .with_priority(Priority::High).unwrap()\n                .add_conversation(Role::User, b\"Fix the timeout bug.\")\n                .add_conversation(Role::Assistant, b\"I'll examine the pool config...\")\n                .add_tool_result(\"ripgrep\", Status::Ok, b\"3 matches found.\"),\n        );\n\n        assert_eq!(decoded.blocks.len(), 5);\n\n        // Block 0: CODE with summary\n        assert_eq!(decoded.blocks[0].block_type, BlockType::Code);\n        assert_eq!(\n            decoded.blocks[0].summary.as_ref().unwrap().text,\n            \"Entry point: CLI setup and server startup.\"\n        );\n\n        // Block 1: ANNOTATION (priority)\n        assert_eq!(decoded.blocks[1].block_type, BlockType::Annotation);\n\n        // Block 2-3: CONVERSATION\n        assert_eq!(decoded.blocks[2].block_type, BlockType::Conversation);\n        assert_eq!(decoded.blocks[3].block_type, BlockType::Conversation);\n\n        // Block 4: TOOL_RESULT\n        assert_eq!(decoded.blocks[4].block_type, BlockType::ToolResult);\n    }\n\n    #[test]\n    fn empty_body_blocks() {\n        // Extension with empty content\n        let decoded = roundtrip(BcpEncoder::new().add_extension(\"ns\", \"type\", b\"\"));\n\n        match &decoded.blocks[0].content {\n            BlockContent::Extension(ext) => {\n                assert_eq!(ext.namespace, \"ns\");\n                assert_eq!(ext.type_name, \"type\");\n                assert!(ext.content.is_empty());\n            }\n            other => panic!(\"expected Extension, got {other:?}\"),\n        }\n    }\n\n    // ── Per-block compression roundtrip tests ───────────────────────────\n\n    #[test]\n    fn roundtrip_per_block_compression() {\n        let big_content = \"fn main() { println!(\\\"hello world\\\"); }\\n\".repeat(50);\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"main.rs\", big_content.as_bytes())\n            .with_compression().unwrap()\n            .encode()\n            .unwrap();\n\n        // Verify the block is actually compressed on the wire\n        let frame_buf = &payload[HEADER_SIZE..];\n        let (frame, _) = BlockFrame::read_from(frame_buf).unwrap().unwrap();\n        assert!(frame.flags.is_compressed());\n\n        // Decode should transparently decompress\n        let decoded = BcpDecoder::decode(&payload).unwrap();\n        assert_eq!(decoded.blocks.len(), 1);\n        match &decoded.blocks[0].content {\n            BlockContent::Code(code) => {\n                assert_eq!(code.path, \"main.rs\");\n                assert_eq!(code.content, big_content.as_bytes());\n            }\n            other => panic!(\"expected Code, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn roundtrip_per_block_compression_with_summary() {\n        let big_content = \"pub fn process() -> Result<(), Error> { Ok(()) }\\n\".repeat(50);\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"lib.rs\", big_content.as_bytes())\n            .with_summary(\"Main processing function.\").unwrap()\n            .with_compression().unwrap()\n            .encode()\n            .unwrap();\n\n        let decoded = BcpDecoder::decode(&payload).unwrap();\n        let block = &decoded.blocks[0];\n        assert!(block.flags.has_summary());\n        assert!(block.flags.is_compressed());\n        assert_eq!(\n            block.summary.as_ref().unwrap().text,\n            \"Main processing function.\"\n        );\n        match &block.content {\n            BlockContent::Code(code) => assert_eq!(code.content, big_content.as_bytes()),\n            other => panic!(\"expected Code, got {other:?}\"),\n        }\n    }\n\n    // ── Whole-payload compression roundtrip tests ───────────────────────\n\n    #[test]\n    fn roundtrip_whole_payload_compression() {\n        let big_content = \"use std::io;\\n\".repeat(100);\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"a.rs\", big_content.as_bytes())\n            .add_code(Lang::Rust, \"b.rs\", big_content.as_bytes())\n            .compress_payload()\n            .encode()\n            .unwrap();\n\n        let decoded = BcpDecoder::decode(&payload).unwrap();\n        assert_eq!(decoded.blocks.len(), 2);\n        assert!(decoded.header.flags.is_compressed());\n\n        for block in &decoded.blocks {\n            match &block.content {\n                BlockContent::Code(code) => {\n                    assert_eq!(code.content, big_content.as_bytes());\n                }\n                other => panic!(\"expected Code, got {other:?}\"),\n            }\n        }\n    }\n\n    // ── Content addressing roundtrip tests ──────────────────────────────\n\n    #[test]\n    fn roundtrip_content_addressing() {\n        use bcp_encoder::MemoryContentStore;\n        use std::sync::Arc;\n\n        let store = Arc::new(MemoryContentStore::new());\n        let payload = BcpEncoder::new()\n            .set_content_store(store.clone())\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .with_content_addressing().unwrap()\n            .encode()\n            .unwrap();\n\n        // Verify it's a reference on the wire\n        let frame_buf = &payload[HEADER_SIZE..];\n        let (frame, _) = BlockFrame::read_from(frame_buf).unwrap().unwrap();\n        assert!(frame.flags.is_reference());\n        assert_eq!(frame.body.len(), 32);\n\n        // decode() without store should fail\n        let result = BcpDecoder::decode(&payload);\n        assert!(matches!(result, Err(DecodeError::MissingContentStore)));\n\n        // decode_with_store should succeed\n        let decoded = BcpDecoder::decode_with_store(&payload, store.as_ref()).unwrap();\n        assert_eq!(decoded.blocks.len(), 1);\n        match &decoded.blocks[0].content {\n            BlockContent::Code(code) => {\n                assert_eq!(code.path, \"main.rs\");\n                assert_eq!(code.content, b\"fn main() {}\");\n            }\n            other => panic!(\"expected Code, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn roundtrip_auto_dedup() {\n        use bcp_encoder::MemoryContentStore;\n        use std::sync::Arc;\n\n        let store = Arc::new(MemoryContentStore::new());\n        let payload = BcpEncoder::new()\n            .set_content_store(store.clone())\n            .auto_dedup()\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\") // duplicate\n            .encode()\n            .unwrap();\n\n        let decoded = BcpDecoder::decode_with_store(&payload, store.as_ref()).unwrap();\n        assert_eq!(decoded.blocks.len(), 2);\n\n        // Both should decode to the same content\n        for block in &decoded.blocks {\n            match &block.content {\n                BlockContent::Code(code) => {\n                    assert_eq!(code.content, b\"fn main() {}\");\n                }\n                other => panic!(\"expected Code, got {other:?}\"),\n            }\n        }\n    }\n\n    #[test]\n    fn unresolved_reference_errors() {\n        use bcp_encoder::MemoryContentStore;\n        use std::sync::Arc;\n\n        let encode_store = Arc::new(MemoryContentStore::new());\n        let payload = BcpEncoder::new()\n            .set_content_store(encode_store)\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .with_content_addressing().unwrap()\n            .encode()\n            .unwrap();\n\n        // Decode with a fresh (empty) store — hash won't be found\n        let decode_store = MemoryContentStore::new();\n        let result = BcpDecoder::decode_with_store(&payload, &decode_store);\n        assert!(matches!(\n            result,\n            Err(DecodeError::UnresolvedReference { .. })\n        ));\n    }\n\n    // ── Combined compression + content addressing ───────────────────────\n\n    #[test]\n    fn roundtrip_refs_with_whole_payload_compression() {\n        use bcp_encoder::MemoryContentStore;\n        use std::sync::Arc;\n\n        let store = Arc::new(MemoryContentStore::new());\n        let big_content = \"fn process() -> bool { true }\\n\".repeat(50);\n        let payload = BcpEncoder::new()\n            .set_content_store(store.clone())\n            .compress_payload()\n            .add_code(Lang::Rust, \"main.rs\", big_content.as_bytes())\n            .with_content_addressing().unwrap()\n            .add_conversation(Role::User, b\"Review this code\")\n            .encode()\n            .unwrap();\n\n        let decoded = BcpDecoder::decode_with_store(&payload, store.as_ref()).unwrap();\n        assert_eq!(decoded.blocks.len(), 2);\n\n        match &decoded.blocks[0].content {\n            BlockContent::Code(code) => {\n                assert_eq!(code.content, big_content.as_bytes());\n            }\n            other => panic!(\"expected Code, got {other:?}\"),\n        }\n        match &decoded.blocks[1].content {\n            BlockContent::Conversation(conv) => {\n                assert_eq!(conv.content, b\"Review this code\");\n            }\n            other => panic!(\"expected Conversation, got {other:?}\"),\n        }\n    }\n}\n",
      "language": "rust",
      "path": "bcp-decoder/src/decoder.rs",
      "type": "code"
    },
    {
      "content": "use std::io::Cursor;\n\nuse crate::error::DecodeError;\n\n/// Maximum decompressed size for a single block body (16 MiB).\n///\n/// Matches the encoder's `MAX_BLOCK_BODY_SIZE`. If a per-block\n/// decompression exceeds this limit, the decoder returns\n/// [`DecodeError::DecompressionBomb`].\npub const MAX_BLOCK_DECOMPRESSED_SIZE: usize = 16 * 1024 * 1024;\n\n/// Maximum decompressed size for whole-payload decompression (256 MiB).\n///\n/// Whole payloads can contain many blocks, so the limit is higher\n/// than the per-block limit. If the decompressed payload exceeds\n/// this, the decoder returns [`DecodeError::DecompressionBomb`].\npub const MAX_PAYLOAD_DECOMPRESSED_SIZE: usize = 256 * 1024 * 1024;\n\n/// Decompress a zstd-compressed byte slice with a safety limit.\n///\n/// Returns the decompressed bytes, or an error if:\n/// - The input is not valid zstd data ([`DecodeError::DecompressFailed`]).\n/// - The decompressed output exceeds `max_size`\n///   ([`DecodeError::DecompressionBomb`]).\n///\n/// # Arguments\n///\n/// - `data` — the zstd-compressed input bytes.\n/// - `max_size` — upper bound on decompressed output (bomb protection).\npub fn decompress(data: &[u8], max_size: usize) -> Result<Vec<u8>, DecodeError> {\n    let decompressed = zstd::decode_all(Cursor::new(data))\n        .map_err(|e| DecodeError::DecompressFailed(e.to_string()))?;\n    if decompressed.len() > max_size {\n        return Err(DecodeError::DecompressionBomb {\n            actual: decompressed.len(),\n            limit: max_size,\n        });\n    }\n    Ok(decompressed)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn compress_test_data(data: &[u8]) -> Vec<u8> {\n        zstd::encode_all(Cursor::new(data), 3).unwrap()\n    }\n\n    #[test]\n    fn decompress_roundtrip() {\n        let original = \"fn main() { println!(\\\"hello\\\"); }\\n\".repeat(50);\n        let compressed = compress_test_data(original.as_bytes());\n        let result = decompress(&compressed, MAX_BLOCK_DECOMPRESSED_SIZE).unwrap();\n        assert_eq!(result, original.as_bytes());\n    }\n\n    #[test]\n    fn decompress_rejects_invalid_data() {\n        let garbage = b\"this is not zstd data\";\n        let result = decompress(garbage, MAX_BLOCK_DECOMPRESSED_SIZE);\n        assert!(matches!(result, Err(DecodeError::DecompressFailed(_))));\n    }\n\n    #[test]\n    fn decompress_rejects_bomb() {\n        let data = \"x\".repeat(10_000);\n        let compressed = compress_test_data(data.as_bytes());\n        let result = decompress(&compressed, 100);\n        assert!(matches!(result, Err(DecodeError::DecompressionBomb { .. })));\n    }\n\n    #[test]\n    fn constants_are_correct() {\n        assert_eq!(MAX_BLOCK_DECOMPRESSED_SIZE, 16 * 1024 * 1024);\n        assert_eq!(MAX_PAYLOAD_DECOMPRESSED_SIZE, 256 * 1024 * 1024);\n    }\n}\n",
      "language": "rust",
      "path": "bcp-decoder/src/decompression.rs",
      "type": "code"
    },
    {
      "content": "use bcp_types::error::TypeError;\nuse bcp_wire::WireError;\n\n/// Errors that can occur during BCP payload decoding.\n///\n/// The decoder validates at multiple levels: header integrity, block\n/// frame structure, TLV body fields, stream termination, decompression,\n/// and content store resolution. Each error variant captures enough\n/// context for meaningful diagnostics.\n///\n/// Error hierarchy:\n///\n/// ```text\n///   DecodeError\n///   ├── InvalidHeader(WireError)   ← magic, version, or reserved byte wrong\n///   ├── BlockTooLarge              ← single block body exceeds size limit\n///   ├── MissingField               ← required TLV field absent in block body\n///   ├── InvalidUtf8                ← string field contains non-UTF-8 bytes\n///   ├── MissingEndSentinel         ← payload ran out without END block\n///   ├── TrailingData               ← extra bytes after END sentinel\n///   ├── DecompressFailed           ← zstd decompression error\n///   ├── DecompressionBomb          ← decompressed size exceeds safety limit\n///   ├── UnresolvedReference        ← BLAKE3 hash not found in content store\n///   ├── MissingContentStore        ← IS_REFERENCE block but no store provided\n///   ├── Type(TypeError)            ← from bcp-types body deserialization\n///   ├── Wire(WireError)            ← from bcp-wire frame parsing\n///   └── Io(std::io::Error)         ← from underlying I/O reads\n/// ```\n#[derive(Debug, thiserror::Error)]\npub enum DecodeError {\n    /// The 8-byte file header failed validation.\n    ///\n    /// This wraps a [`WireError`] from `BcpHeader::read_from` — the\n    /// inner error distinguishes between bad magic, unsupported version,\n    /// and non-zero reserved byte.\n    #[error(\"invalid header: {0}\")]\n    InvalidHeader(WireError),\n\n    /// A block body exceeds the maximum allowed size.\n    #[error(\"block body too large: {size} bytes at offset {offset}\")]\n    BlockTooLarge { size: usize, offset: usize },\n\n    /// A required field was missing from a known block type's body.\n    ///\n    /// This provides richer context than the underlying\n    /// [`TypeError::MissingRequiredField`] by including the block type name\n    /// and the field's wire ID.\n    #[error(\"required field {field_name} (id={field_id}) missing in {block_type} block\")]\n    MissingField {\n        block_type: &'static str,\n        field_name: &'static str,\n        field_id: u64,\n    },\n\n    /// A string field contained invalid UTF-8 bytes.\n    #[error(\"invalid UTF-8 in field {field_name} of {block_type} block\")]\n    InvalidUtf8 {\n        block_type: &'static str,\n        field_name: &'static str,\n    },\n\n    /// The payload ended without an END sentinel block (type=0xFF).\n    ///\n    /// Every valid BCP payload must terminate with an END block. If the\n    /// byte stream is exhausted before encountering one, the payload is\n    /// considered truncated.\n    #[error(\"payload does not end with END sentinel\")]\n    MissingEndSentinel,\n\n    /// Extra bytes were found after the END sentinel.\n    ///\n    /// Per the spec, this is a warning-level condition — the payload\n    /// decoded successfully, but the trailing data may indicate\n    /// corruption or a buggy encoder. The decoder captures this as an\n    /// error variant so callers can decide how to handle it.\n    #[error(\"unexpected data after END sentinel ({extra_bytes} bytes)\")]\n    TrailingData { extra_bytes: usize },\n\n    /// Zstd decompression failed.\n    ///\n    /// Returned when a block's `COMPRESSED` flag (bit 1) or the header's\n    /// `COMPRESSED` flag (bit 0) is set and the zstd decoder cannot parse\n    /// the compressed data. Common causes: truncated input, corrupt frame,\n    /// or non-zstd data with the flag erroneously set.\n    #[error(\"zstd decompression failed: {0}\")]\n    DecompressFailed(String),\n\n    /// Decompressed data exceeds the safety limit.\n    ///\n    /// Prevents decompression bombs — payloads crafted to decompress into\n    /// vastly larger output. The `limit` is the caller-configured maximum\n    /// (default: 16 MiB per block, 256 MiB for whole-payload).\n    #[error(\"decompressed size {actual} exceeds limit {limit}\")]\n    DecompressionBomb { actual: usize, limit: usize },\n\n    /// A block has the `IS_REFERENCE` flag set but its 32-byte BLAKE3\n    /// hash was not found in the content store.\n    ///\n    /// This means the content was previously content-addressed during\n    /// encoding but the corresponding data was not provided to the\n    /// decoder's content store.\n    #[error(\"unresolved reference: BLAKE3 hash not found in content store\")]\n    UnresolvedReference { hash: [u8; 32] },\n\n    /// A block has the `IS_REFERENCE` flag but no content store was\n    /// provided to the decoder.\n    ///\n    /// Use [`BcpDecoder::decode_with_store`] instead of\n    /// [`BcpDecoder::decode`] when decoding payloads that contain\n    /// content-addressed blocks.\n    #[error(\"block has IS_REFERENCE flag but no content store was provided\")]\n    MissingContentStore,\n\n    /// A body deserialization error from `bcp-types`.\n    ///\n    /// This covers missing required fields, unknown wire types, and\n    /// invalid enum values encountered while parsing TLV fields within\n    /// a block body.\n    #[error(transparent)]\n    Type(#[from] TypeError),\n\n    /// A wire-level framing error from `bcp-wire`.\n    ///\n    /// Surfaces when a block frame's varint is malformed, the body\n    /// length exceeds the remaining bytes, or other structural issues.\n    #[error(transparent)]\n    Wire(#[from] WireError),\n\n    /// An I/O error from the underlying reader (streaming decoder).\n    #[error(transparent)]\n    Io(#[from] std::io::Error),\n}\n",
      "language": "rust",
      "path": "bcp-decoder/src/error.rs",
      "type": "code"
    },
    {
      "content": "#![warn(clippy::pedantic)]\n\n//! BCP decoder — parses binary payloads into typed blocks.\n//!\n//! Two APIs are provided:\n//!\n//! - [`BcpDecoder`] — synchronous, operates on a complete `&[u8]` slice.\n//! - [`StreamingDecoder`] — asynchronous, reads from any `AsyncRead` source\n//!   and yields blocks incrementally.\n//!\n//! **Compression and streaming**: The streaming decoder provides true\n//! incremental parsing for uncompressed and per-block-compressed payloads.\n//! However, whole-payload compression (`HeaderFlags::COMPRESSED`) forces\n//! the streaming decoder to buffer the entire payload before yielding\n//! blocks — the streaming API surface is preserved but the memory and\n//! latency benefits are lost. Prefer per-block compression when streaming\n//! matters.\n\npub mod block_reader;\npub mod decoder;\npub mod error;\npub mod streaming;\n\nmod decompression;\n\npub use decoder::{DecodedPayload, BcpDecoder};\npub use error::DecodeError;\npub use streaming::{DecoderEvent, StreamingDecoder};\n",
      "language": "rust",
      "path": "bcp-decoder/src/lib.rs",
      "type": "code"
    },
    {
      "content": "use std::sync::Arc;\n\nuse bcp_types::block::{Block, BlockContent};\nuse bcp_types::block_type::BlockType;\nuse bcp_types::content_store::ContentStore;\nuse bcp_types::summary::Summary;\nuse bcp_wire::block_frame::{BlockFlags, BlockFrame};\nuse bcp_wire::header::{HEADER_SIZE, BcpHeader};\nuse bcp_wire::varint::decode_varint;\nuse tokio::io::{AsyncRead, AsyncReadExt};\n\nuse crate::decompression::{self, MAX_BLOCK_DECOMPRESSED_SIZE, MAX_PAYLOAD_DECOMPRESSED_SIZE};\nuse crate::error::DecodeError;\n\n/// Events emitted by the streaming decoder.\n///\n/// The stream yields a `Header` event first (once the 8-byte file\n/// header has been read and validated), then a sequence of `Block`\n/// events for each decoded block, terminating when the END sentinel\n/// is encountered.\n///\n/// ```text\n///   Header(BcpHeader)\n///   Block(Block)\n///   Block(Block)\n///   Block(Block)\n///   ... (stream ends at END sentinel)\n/// ```\n#[derive(Clone, Debug)]\npub enum DecoderEvent {\n    /// The file header has been parsed and validated.\n    Header(BcpHeader),\n\n    /// A block has been fully decoded.\n    Block(Block),\n}\n\n/// Asynchronous streaming decoder — yields blocks one at a time\n/// without buffering the entire payload.\n///\n/// This is the primary API for large payloads or network streams.\n/// The decoder reads the header first, then yields blocks as they\n/// are fully received. Backpressure is handled naturally: the stream\n/// only reads the next block when the caller awaits the next item.\n///\n/// Unlike the synchronous [`BcpDecoder`](crate::BcpDecoder) which\n/// requires the entire payload in memory, `StreamingDecoder` reads\n/// incrementally from any `AsyncRead` source (files, TCP sockets,\n/// HTTP response bodies, etc.).\n///\n/// # Important: whole-payload compression disables streaming\n///\n/// **When the header's `COMPRESSED` flag is set (whole-payload\n/// compression), the streaming decoder falls back to buffering the\n/// entire payload before yielding any blocks.** This is unavoidable:\n/// zstd requires the full compressed input to decompress. The API\n/// surface remains the same (you still call `next()` in a loop), but\n/// the memory and latency characteristics become identical to\n/// [`BcpDecoder::decode`](crate::BcpDecoder::decode).\n///\n/// True incremental streaming is only achieved with **uncompressed**\n/// or **per-block compressed** payloads. If streaming is critical for\n/// your use case, prefer per-block compression\n/// ([`BcpEncoder::compress_blocks`]) over whole-payload compression\n/// ([`BcpEncoder::compress_payload`]).\n///\n/// # Content store\n///\n/// To decode payloads with `IS_REFERENCE` blocks, provide a content\n/// store via [`with_content_store`](Self::with_content_store).\n///\n/// # Example\n///\n/// ```rust,no_run\n/// use bcp_decoder::StreamingDecoder;\n/// use tokio::io::AsyncRead;\n///\n/// async fn decode_from_reader(reader: impl AsyncRead + Unpin) {\n///     let mut stream = StreamingDecoder::new(reader);\n///     while let Some(event) = stream.next().await.transpose().unwrap() {\n///         // Process each DecoderEvent...\n///     }\n/// }\n/// ```\npub struct StreamingDecoder<R> {\n    reader: R,\n    state: StreamState,\n    /// Internal read buffer. Block bodies are read into this buffer\n    /// before being parsed. The buffer is reused across blocks to\n    /// avoid repeated allocations.\n    buf: Vec<u8>,\n    /// When whole-payload compression is detected, the entire stream\n    /// is read and decompressed into this buffer. Subsequent block\n    /// reads consume from here instead of the original reader.\n    decompressed_payload: Option<Vec<u8>>,\n    /// Read cursor into `decompressed_payload`.\n    decompressed_cursor: usize,\n    /// Optional content store for resolving `IS_REFERENCE` blocks.\n    content_store: Option<Arc<dyn ContentStore>>,\n}\n\n/// Internal state machine for the streaming decoder.\n///\n/// The decoder progresses through three states:\n///\n/// ```text\n///   ReadHeader → ReadBlocks → Done\n/// ```\n///\n/// `ReadHeader` is the initial state. After the header is read, the\n/// decoder transitions to `ReadBlocks` and stays there until the END\n/// sentinel is encountered, at which point it transitions to `Done`.\n#[derive(Clone, Copy, Debug, PartialEq, Eq)]\nenum StreamState {\n    ReadHeader,\n    ReadBlocks,\n    Done,\n}\n\nimpl<R: AsyncRead + Unpin> StreamingDecoder<R> {\n    /// Create a new streaming decoder over the given async reader.\n    ///\n    /// The decoder starts in `ReadHeader` state and will read the\n    /// 8-byte file header on the first call to [`next`](Self::next).\n    #[must_use]\n    pub fn new(reader: R) -> Self {\n        Self {\n            reader,\n            state: StreamState::ReadHeader,\n            buf: Vec::with_capacity(4096),\n            decompressed_payload: None,\n            decompressed_cursor: 0,\n            content_store: None,\n        }\n    }\n\n    /// Attach a content store for resolving `IS_REFERENCE` blocks.\n    ///\n    /// When a block has the `IS_REFERENCE` flag set, its 32-byte body\n    /// is looked up in this store to retrieve the original content.\n    #[must_use]\n    pub fn with_content_store(mut self, store: Arc<dyn ContentStore>) -> Self {\n        self.content_store = Some(store);\n        self\n    }\n\n    /// Read the next event from the stream.\n    ///\n    /// Returns `Ok(Some(event))` for each decoded event, `Ok(None)`\n    /// when the stream is exhausted (END sentinel reached), or `Err`\n    /// on any decode error.\n    ///\n    /// The first call always yields `DecoderEvent::Header`. Subsequent\n    /// calls yield `DecoderEvent::Block` until the END sentinel.\n    pub async fn next(&mut self) -> Option<Result<DecoderEvent, DecodeError>> {\n        match self.state {\n            StreamState::ReadHeader => Some(self.read_header().await),\n            StreamState::ReadBlocks => self.read_next_block().await,\n            StreamState::Done => None,\n        }\n    }\n\n    /// Read and validate the 8-byte file header.\n    ///\n    /// If the header's `COMPRESSED` flag is set, the decoder reads\n    /// all remaining bytes from the stream, decompresses them with\n    /// zstd, and stores the result internally. Subsequent block reads\n    /// consume from the decompressed buffer.\n    async fn read_header(&mut self) -> Result<DecoderEvent, DecodeError> {\n        let mut header_buf = [0u8; HEADER_SIZE];\n        self.reader.read_exact(&mut header_buf).await.map_err(|_| {\n            DecodeError::InvalidHeader(bcp_wire::WireError::UnexpectedEof { offset: 0 })\n        })?;\n\n        let header = BcpHeader::read_from(&header_buf).map_err(DecodeError::InvalidHeader)?;\n\n        // Whole-payload decompression: buffer everything, decompress.\n        if header.flags.is_compressed() {\n            let mut compressed = Vec::new();\n            self.reader\n                .read_to_end(&mut compressed)\n                .await\n                .map_err(DecodeError::Io)?;\n            let decompressed =\n                decompression::decompress(&compressed, MAX_PAYLOAD_DECOMPRESSED_SIZE)?;\n            self.decompressed_payload = Some(decompressed);\n            self.decompressed_cursor = 0;\n        }\n\n        self.state = StreamState::ReadBlocks;\n        Ok(DecoderEvent::Header(header))\n    }\n\n    /// Read the next block frame from the stream.\n    ///\n    /// If a decompressed payload buffer exists (whole-payload mode),\n    /// reads from that buffer. Otherwise reads from the async reader.\n    ///\n    /// Per-block decompression and reference resolution are applied\n    /// transparently.\n    ///\n    /// Returns `None` when the END sentinel is encountered, transitioning\n    /// the state to `Done`.\n    async fn read_next_block(&mut self) -> Option<Result<DecoderEvent, DecodeError>> {\n        // If we have a decompressed payload buffer, parse from it\n        // using BlockFrame::read_from (synchronous path).\n        if let Some(ref payload) = self.decompressed_payload {\n            if self.decompressed_cursor >= payload.len() {\n                self.state = StreamState::Done;\n                return Some(Err(DecodeError::MissingEndSentinel));\n            }\n\n            let remaining = &payload[self.decompressed_cursor..];\n            match BlockFrame::read_from(remaining) {\n                Ok(Some((frame, consumed))) => {\n                    self.decompressed_cursor += consumed;\n                    Some(self.decode_frame(&frame))\n                }\n                Ok(None) => {\n                    // END sentinel — compute its size and advance cursor.\n                    // END = varint(0xFF) + flags(1 byte) + varint(0x00)\n                    match end_sentinel_size(remaining) {\n                        Ok(size) => self.decompressed_cursor += size,\n                        Err(e) => return Some(Err(e)),\n                    }\n                    self.state = StreamState::Done;\n                    None\n                }\n                Err(e) => Some(Err(DecodeError::from(e))),\n            }\n        } else {\n            self.read_next_block_from_reader().await\n        }\n    }\n\n    /// Read the next block frame from the async reader (non-buffered path).\n    async fn read_next_block_from_reader(&mut self) -> Option<Result<DecoderEvent, DecodeError>> {\n        // Read block_type varint\n        let block_type_raw = match self.read_varint().await {\n            Ok(v) => v,\n            Err(e) => return Some(Err(e)),\n        };\n\n        #[allow(clippy::cast_possible_truncation)]\n        let block_type_byte = block_type_raw as u8;\n\n        // Check for END sentinel\n        if block_type_byte == 0xFF {\n            match self.read_end_frame_tail().await {\n                Ok(()) => {}\n                Err(e) => return Some(Err(e)),\n            }\n            self.state = StreamState::Done;\n            return None;\n        }\n\n        // Read flags (single byte)\n        let mut flags_byte = [0u8; 1];\n        if let Err(e) = self.reader.read_exact(&mut flags_byte).await {\n            return Some(Err(DecodeError::Io(e)));\n        }\n        let flags = BlockFlags::from_raw(flags_byte[0]);\n\n        // Read content_len varint\n        #[allow(clippy::cast_possible_truncation)]\n        let content_len = match self.read_varint().await {\n            Ok(v) => v as usize,\n            Err(e) => return Some(Err(e)),\n        };\n\n        // Read body bytes\n        self.buf.clear();\n        self.buf.resize(content_len, 0);\n        if let Err(e) = self.reader.read_exact(&mut self.buf[..content_len]).await {\n            return Some(Err(DecodeError::Io(e)));\n        }\n\n        let frame = bcp_wire::block_frame::BlockFrame {\n            block_type: block_type_byte,\n            flags,\n            body: self.buf[..content_len].to_vec(),\n        };\n\n        Some(self.decode_frame(&frame))\n    }\n\n    /// Decode a `BlockFrame` into a `DecoderEvent::Block`.\n    ///\n    /// Handles reference resolution, decompression, summary extraction,\n    /// and body deserialization.\n    fn decode_frame(\n        &self,\n        frame: &bcp_wire::block_frame::BlockFrame,\n    ) -> Result<DecoderEvent, DecodeError> {\n        let block_type = BlockType::from_wire_id(frame.block_type);\n\n        // Stage 1: Resolve content-addressed references.\n        let resolved_body = if frame.flags.is_reference() {\n            let store = self\n                .content_store\n                .as_ref()\n                .ok_or(DecodeError::MissingContentStore)?;\n            if frame.body.len() != 32 {\n                return Err(DecodeError::Wire(bcp_wire::WireError::UnexpectedEof {\n                    offset: frame.body.len(),\n                }));\n            }\n            let hash: [u8; 32] = frame.body[..32].try_into().expect(\"length already checked\");\n            store\n                .get(&hash)\n                .ok_or(DecodeError::UnresolvedReference { hash })?\n        } else {\n            frame.body.clone()\n        };\n\n        // Stage 2: Per-block decompression.\n        let decompressed_body = if frame.flags.is_compressed() {\n            decompression::decompress(&resolved_body, MAX_BLOCK_DECOMPRESSED_SIZE)?\n        } else {\n            resolved_body\n        };\n\n        // Stage 3 & 4: Summary extraction + TLV decode.\n        let mut body = decompressed_body.as_slice();\n        let mut summary = None;\n\n        if frame.flags.has_summary() {\n            match Summary::decode(body) {\n                Ok((sum, consumed)) => {\n                    summary = Some(sum);\n                    body = &body[consumed..];\n                }\n                Err(e) => return Err(e.into()),\n            }\n        }\n\n        let content = BlockContent::decode_body(&block_type, body)?;\n\n        Ok(DecoderEvent::Block(Block {\n            block_type,\n            flags: frame.flags,\n            summary,\n            content,\n        }))\n    }\n\n    /// Read the trailing flags + `content_len` bytes of an END frame.\n    ///\n    /// The END sentinel has: `flags`=0x00 (1 byte) + `content_len`=0x00 (1 byte).\n    /// We read and discard these to fully consume the END frame.\n    async fn read_end_frame_tail(&mut self) -> Result<(), DecodeError> {\n        // flags byte\n        let mut byte = [0u8; 1];\n        self.reader\n            .read_exact(&mut byte)\n            .await\n            .map_err(DecodeError::Io)?;\n\n        // content_len varint (should be 0)\n        let _content_len = self.read_varint().await?;\n        Ok(())\n    }\n\n    /// Read a single varint from the async reader.\n    ///\n    /// Varints are read byte-by-byte: each byte's MSB indicates whether\n    /// more bytes follow (1 = more, 0 = last byte). Maximum 10 bytes\n    /// for a 64-bit value.\n    async fn read_varint(&mut self) -> Result<u64, DecodeError> {\n        let mut varint_buf = [0u8; 10];\n        let mut len = 0;\n\n        loop {\n            let mut byte = [0u8; 1];\n            self.reader\n                .read_exact(&mut byte)\n                .await\n                .map_err(DecodeError::Io)?;\n            varint_buf[len] = byte[0];\n            len += 1;\n\n            // MSB clear means this is the last byte\n            if byte[0] & 0x80 == 0 {\n                break;\n            }\n\n            if len >= 10 {\n                return Err(DecodeError::Wire(bcp_wire::WireError::VarintTooLong));\n            }\n        }\n\n        let (value, _) = decode_varint(&varint_buf[..len])?;\n        Ok(value)\n    }\n}\n\n/// Calculate the byte size of the END sentinel from a buffer slice.\n///\n/// Used by the streaming decoder when parsing from a decompressed\n/// payload buffer. The END sentinel is: `varint(0xFF)` + `flags(0x00)`\n/// + `varint(0x00)`.\nfn end_sentinel_size(buf: &[u8]) -> Result<usize, DecodeError> {\n    let (_, type_len) = decode_varint(buf)?;\n    let mut size = type_len;\n    // flags byte\n    size += 1;\n    // content_len varint\n    let rest = buf\n        .get(size..)\n        .ok_or(DecodeError::Wire(bcp_wire::WireError::UnexpectedEof {\n            offset: size,\n        }))?;\n    let (_, len_size) = decode_varint(rest)?;\n    size += len_size;\n    Ok(size)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use bcp_encoder::BcpEncoder;\n    use bcp_types::enums::{Lang, Priority, Role, Status};\n\n    /// Helper: encode a payload and decode it via the streaming decoder,\n    /// collecting all events into a Vec.\n    async fn stream_roundtrip(encoder: &BcpEncoder) -> Vec<DecoderEvent> {\n        let payload = encoder.encode().unwrap();\n        let cursor = std::io::Cursor::new(payload);\n        let reader = tokio::io::BufReader::new(cursor);\n\n        let mut decoder = StreamingDecoder::new(reader);\n        let mut events = Vec::new();\n\n        while let Some(result) = decoder.next().await {\n            events.push(result.unwrap());\n        }\n\n        events\n    }\n\n    #[tokio::test]\n    async fn streaming_produces_header_then_blocks() {\n        let mut enc = BcpEncoder::new();\n        enc.add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .add_conversation(Role::User, b\"hello\");\n        let events = stream_roundtrip(&enc).await;\n\n        assert_eq!(events.len(), 3); // Header + 2 blocks\n\n        assert!(matches!(&events[0], DecoderEvent::Header(h) if h.version_major == 1));\n        assert!(matches!(&events[1], DecoderEvent::Block(b) if b.block_type == BlockType::Code));\n        assert!(\n            matches!(&events[2], DecoderEvent::Block(b) if b.block_type == BlockType::Conversation)\n        );\n    }\n\n    #[tokio::test]\n    async fn streaming_matches_sync_decoder() {\n        let mut encoder = BcpEncoder::new();\n        encoder\n            .add_code(Lang::Rust, \"lib.rs\", b\"pub fn x() {}\")\n            .with_summary(\"Function x.\").unwrap()\n            .with_priority(Priority::High).unwrap()\n            .add_conversation(Role::User, b\"What does x do?\")\n            .add_tool_result(\"docs\", Status::Ok, b\"x is a placeholder.\");\n\n        let payload = encoder.encode().unwrap();\n\n        // Sync decode\n        let sync_decoded = crate::BcpDecoder::decode(&payload).unwrap();\n\n        // Streaming decode\n        let events = stream_roundtrip(&encoder).await;\n\n        // Extract blocks from events (skip the Header event)\n        let stream_blocks: Vec<_> = events\n            .into_iter()\n            .filter_map(|e| match e {\n                DecoderEvent::Block(b) => Some(b),\n                _ => None,\n            })\n            .collect();\n\n        // Same number of blocks\n        assert_eq!(sync_decoded.blocks.len(), stream_blocks.len());\n\n        // Same block types in same order\n        for (sync_block, stream_block) in sync_decoded.blocks.iter().zip(stream_blocks.iter()) {\n            assert_eq!(sync_block.block_type, stream_block.block_type);\n            assert_eq!(sync_block.flags, stream_block.flags);\n            assert_eq!(sync_block.summary, stream_block.summary);\n        }\n    }\n\n    #[tokio::test]\n    async fn streaming_handles_summary_blocks() {\n        let mut enc = BcpEncoder::new();\n        enc.add_code(Lang::Python, \"app.py\", b\"print('hi')\")\n            .with_summary(\"Prints a greeting.\").unwrap();\n        let events = stream_roundtrip(&enc).await;\n\n        let block = match &events[1] {\n            DecoderEvent::Block(b) => b,\n            other => panic!(\"expected Block, got {other:?}\"),\n        };\n\n        assert!(block.flags.has_summary());\n        assert_eq!(block.summary.as_ref().unwrap().text, \"Prints a greeting.\");\n    }\n\n    #[tokio::test]\n    async fn streaming_empty_body_blocks() {\n        let mut enc = BcpEncoder::new();\n        enc.add_extension(\"ns\", \"t\", b\"\");\n        let events = stream_roundtrip(&enc).await;\n\n        assert_eq!(events.len(), 2); // Header + Extension\n    }\n\n    #[tokio::test]\n    async fn streaming_terminates_at_end_sentinel() {\n        let mut enc = BcpEncoder::new();\n        enc.add_conversation(Role::User, b\"hi\");\n        let events = stream_roundtrip(&enc).await;\n\n        // After all events, decoder should return None\n        assert_eq!(events.len(), 2); // Header + 1 block\n    }\n\n    // ── Per-block compression streaming tests ───────────────────────────\n\n    #[tokio::test]\n    async fn streaming_per_block_compression_roundtrip() {\n        let big_content = \"fn main() { println!(\\\"hello world\\\"); }\\n\".repeat(50);\n        let mut enc = BcpEncoder::new();\n        enc.add_code(Lang::Rust, \"main.rs\", big_content.as_bytes())\n            .with_compression().unwrap();\n        let events = stream_roundtrip(&enc).await;\n\n        assert_eq!(events.len(), 2); // Header + 1 block\n        let block = match &events[1] {\n            DecoderEvent::Block(b) => b,\n            other => panic!(\"expected Block, got {other:?}\"),\n        };\n\n        match &block.content {\n            BlockContent::Code(code) => {\n                assert_eq!(code.content, big_content.as_bytes());\n            }\n            other => panic!(\"expected Code, got {other:?}\"),\n        }\n    }\n\n    // ── Whole-payload compression streaming tests ───────────────────────\n\n    #[tokio::test]\n    async fn streaming_whole_payload_compression_roundtrip() {\n        let big_content = \"use std::io;\\n\".repeat(100);\n        let mut enc = BcpEncoder::new();\n        enc.add_code(Lang::Rust, \"a.rs\", big_content.as_bytes())\n            .add_code(Lang::Rust, \"b.rs\", big_content.as_bytes());\n        enc.compress_payload();\n        let events = stream_roundtrip(&enc).await;\n\n        // Header + 2 blocks\n        assert_eq!(events.len(), 3);\n\n        // Verify header has COMPRESSED flag\n        match &events[0] {\n            DecoderEvent::Header(h) => assert!(h.flags.is_compressed()),\n            other => panic!(\"expected Header, got {other:?}\"),\n        }\n\n        // Both blocks should decompress correctly\n        for event in &events[1..] {\n            match event {\n                DecoderEvent::Block(block) => match &block.content {\n                    BlockContent::Code(code) => {\n                        assert_eq!(code.content, big_content.as_bytes());\n                    }\n                    other => panic!(\"expected Code, got {other:?}\"),\n                },\n                other => panic!(\"expected Block, got {other:?}\"),\n            }\n        }\n    }\n\n    // ── Content store streaming tests ───────────────────────────────────\n\n    #[tokio::test]\n    async fn streaming_content_addressing_roundtrip() {\n        let store = Arc::new(bcp_encoder::MemoryContentStore::new());\n        let mut enc = BcpEncoder::new();\n        enc.set_content_store(store.clone())\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .with_content_addressing().unwrap();\n\n        let payload = enc.encode().unwrap();\n        let cursor = std::io::Cursor::new(payload);\n        let reader = tokio::io::BufReader::new(cursor);\n\n        let mut decoder = StreamingDecoder::new(reader).with_content_store(store);\n        let mut events = Vec::new();\n        while let Some(result) = decoder.next().await {\n            events.push(result.unwrap());\n        }\n\n        assert_eq!(events.len(), 2); // Header + 1 block\n        match &events[1] {\n            DecoderEvent::Block(block) => match &block.content {\n                BlockContent::Code(code) => {\n                    assert_eq!(code.content, b\"fn main() {}\");\n                }\n                other => panic!(\"expected Code, got {other:?}\"),\n            },\n            other => panic!(\"expected Block, got {other:?}\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn streaming_matches_sync_compressed() {\n        let big_content = \"pub fn hello() -> &'static str { \\\"world\\\" }\\n\".repeat(100);\n        let mut encoder = BcpEncoder::new();\n        encoder\n            .add_code(Lang::Rust, \"lib.rs\", big_content.as_bytes())\n            .with_summary(\"Hello function.\").unwrap()\n            .add_conversation(Role::User, b\"explain\");\n        encoder.compress_payload();\n\n        let payload = encoder.encode().unwrap();\n\n        // Sync decode\n        let sync_decoded = crate::BcpDecoder::decode(&payload).unwrap();\n\n        // Streaming decode\n        let events = stream_roundtrip(&encoder).await;\n        let stream_blocks: Vec<_> = events\n            .into_iter()\n            .filter_map(|e| match e {\n                DecoderEvent::Block(b) => Some(b),\n                _ => None,\n            })\n            .collect();\n\n        assert_eq!(sync_decoded.blocks.len(), stream_blocks.len());\n        for (sync_block, stream_block) in sync_decoded.blocks.iter().zip(stream_blocks.iter()) {\n            assert_eq!(sync_block.block_type, stream_block.block_type);\n            assert_eq!(sync_block.summary, stream_block.summary);\n        }\n    }\n}\n",
      "language": "rust",
      "path": "bcp-decoder/src/streaming.rs",
      "type": "code"
    },
    {
      "content": "[package]\nname = \"bcp-driver\"\nversion = \"0.1.0\"\nedition = \"2024\"\n\n[dependencies]\nbcp-types = { path = \"../bcp-types\" }\nbcp-wire = { path = \"../bcp-wire\" }\nthiserror = { workspace = true }\n\n[dev-dependencies]\nbcp-encoder = { path = \"../bcp-encoder\" }\nbcp-decoder = { path = \"../bcp-decoder\" }\n",
      "language": "toml",
      "path": "bcp-driver/Cargo.toml",
      "type": "code"
    },
    {
      "content": "use std::collections::HashMap;\n\nuse bcp_types::BlockType;\nuse bcp_types::block::{Block, BlockContent};\nuse bcp_types::enums::{AnnotationKind, Priority};\n\nuse crate::render_xml::{content_to_string, role_display_name};\n\n/// How to render a single block under budget constraints.\n///\n/// The budget engine produces one `RenderDecision` per renderable block.\n/// Renderers use this to override the default summary-presence logic —\n/// a block with a summary may still be rendered in full if the budget\n/// allows, or a block without a summary may be reduced to a placeholder\n/// if the budget is exhausted.\n///\n/// ```text\n/// ┌─────────────┬──────────────────────────────────────────────────┐\n/// │ Variant     │ Behavior                                         │\n/// ├─────────────┼──────────────────────────────────────────────────┤\n/// │ Full        │ Render complete block content (ignore summary)   │\n/// │ Summary     │ Render summary text only                         │\n/// │ Placeholder │ Emit a compact omission notice with metadata     │\n/// │ Omit        │ Skip the block entirely (no output)              │\n/// └─────────────┴──────────────────────────────────────────────────┘\n/// ```\n///\n/// The budget engine assigns decisions based on priority (from\n/// ANNOTATION blocks) and available token budget:\n///\n/// ```text\n/// ┌────────────┬──────────────────────────────────────────────────┐\n/// │ Priority   │ Degradation path                                 │\n/// ├────────────┼──────────────────────────────────────────────────┤\n/// │ Critical   │ Full (always, even over budget)                  │\n/// │ High       │ Full → Summary → Full (forced, over budget)      │\n/// │ Normal     │ Full → Summary → Placeholder                     │\n/// │ Low        │ Summary → Placeholder                            │\n/// │ Background │ Placeholder → Omit                               │\n/// └────────────┴──────────────────────────────────────────────────┘\n/// ```\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum RenderDecision {\n    /// Render the full block content, ignoring any attached summary.\n    Full,\n\n    /// Render the summary text only. If the block has no summary,\n    /// the renderer falls back to full content.\n    Summary,\n\n    /// Render a compact placeholder showing the block's type,\n    /// description, and estimated omitted token count.\n    Placeholder {\n        /// The type of the omitted block (e.g., `Code`, `ToolResult`).\n        block_type: BlockType,\n        /// Human-readable description (e.g., file path, tool name).\n        description: String,\n        /// Estimated tokens that were omitted.\n        omitted_tokens: u32,\n    },\n\n    /// Omit the block entirely — produce no output for it.\n    Omit,\n}\n\n/// Token count estimator trait.\n///\n/// Implementations estimate how many tokens a text string will consume\n/// in an LLM's context window. The trait is pluggable: the `PoC` uses\n/// character-count heuristics, but a real tokenizer (tiktoken, etc.)\n/// can be swapped in by implementing this trait.\n///\n/// Implementations must be `Send + Sync` so the driver can be shared\n/// across threads.\npub trait TokenEstimator: Send + Sync {\n    /// Estimate the token count for the given text.\n    ///\n    /// Returns 0 for empty strings and at least 1 for non-empty strings.\n    fn estimate(&self, text: &str) -> u32;\n}\n\n/// Heuristic token estimator: `chars / 4`, minimum 1.\n///\n/// This matches the common rule-of-thumb that English prose averages\n/// roughly 4 characters per token. It systematically underestimates\n/// code (which tends toward shorter tokens due to operators and short\n/// identifiers), but is a reasonable default when no tokenizer is\n/// available.\n///\n/// ```text\n/// ┌──────────────────────┬────────┬──────────┐\n/// │ Input                │ Chars  │ Estimate │\n/// ├──────────────────────┼────────┼──────────┤\n/// │ \"\"                   │ 0      │ 0        │\n/// │ \"a\"                  │ 1      │ 1 (min)  │\n/// │ \"hello world\"        │ 11     │ 2        │\n/// │ \"fn main() {}\"       │ 13     │ 3        │\n/// └──────────────────────┴────────┴──────────┘\n/// ```\npub struct HeuristicEstimator;\n\nimpl TokenEstimator for HeuristicEstimator {\n    #[allow(clippy::cast_possible_truncation)]\n    fn estimate(&self, text: &str) -> u32 {\n        if text.is_empty() {\n            return 0;\n        }\n        let chars = text.len() as u32;\n        (chars / 4).max(1)\n    }\n}\n\n/// Code-aware token estimator.\n///\n/// Uses different character-to-token ratios depending on whether the\n/// text looks like code or prose:\n///\n/// - **Code** (>30% of non-empty lines are indented): `chars / 3`\n/// - **Prose** (everything else): `chars / 4`\n///\n/// Code tends to produce more tokens per character because of short\n/// identifiers, operators, and punctuation that each consume a token.\n///\n/// ```text\n/// ┌──────────────────────┬───────┬──────────┐\n/// │ Input type           │ Ratio │ Example  │\n/// ├──────────────────────┼───────┼──────────┤\n/// │ English prose        │ ÷ 4   │ 400ch→100│\n/// │ Source code          │ ÷ 3   │ 300ch→100│\n/// │ Empty string         │ —     │ 0        │\n/// └──────────────────────┴───────┴──────────┘\n/// ```\npub struct CodeAwareEstimator;\n\nimpl CodeAwareEstimator {\n    /// Determine whether a text block looks like code.\n    ///\n    /// Heuristic: if more than 30% of non-empty lines start with\n    /// whitespace (space or tab), treat the text as code. This catches\n    /// most indented source code while ignoring prose paragraphs.\n    fn is_code_like(text: &str) -> bool {\n        let non_empty: Vec<&str> = text.lines().filter(|l| !l.trim().is_empty()).collect();\n        if non_empty.is_empty() {\n            return false;\n        }\n        let indented = non_empty\n            .iter()\n            .filter(|l| l.starts_with(' ') || l.starts_with('\\t'))\n            .count();\n        (indented * 100 / non_empty.len()) > 30\n    }\n}\n\nimpl TokenEstimator for CodeAwareEstimator {\n    #[allow(clippy::cast_possible_truncation)]\n    fn estimate(&self, text: &str) -> u32 {\n        if text.is_empty() {\n            return 0;\n        }\n        let chars = text.len() as u32;\n        let divisor = if Self::is_code_like(text) { 3 } else { 4 };\n        (chars / divisor).max(1)\n    }\n}\n\n/// Per-block budget metadata computed during the scan pass.\n///\n/// One `BlockBudgetInfo` is created for every renderable (non-Annotation,\n/// non-End) block during the first pass of the budget engine. It captures\n/// estimated token costs and resolved priority so the allocation pass can\n/// make render decisions.\n///\n/// ```text\n/// ┌────────────────┬──────────────────────────────────────────────────┐\n/// │ Field          │ Purpose                                          │\n/// ├────────────────┼──────────────────────────────────────────────────┤\n/// │ priority       │ Resolved from ANNOTATION blocks or Normal        │\n/// │ full_tokens    │ Estimated tokens for full content rendering      │\n/// │ summary_tokens │ Estimated tokens for summary (None if no summary)│\n/// │ has_summary    │ Whether the block has an attached summary        │\n/// └────────────────┴──────────────────────────────────────────────────┘\n/// ```\npub(crate) struct BlockBudgetInfo {\n    pub priority: Priority,\n    pub full_tokens: u32,\n    pub summary_tokens: Option<u32>,\n    pub has_summary: bool,\n}\n\n// ── Priority Resolution ──────────────────────────────────────────────\n\n/// Resolve block priorities from ANNOTATION blocks.\n///\n/// Walks the full block list (including annotations and sentinels) and\n/// extracts `AnnotationKind::Priority` annotations into a map keyed by\n/// `target_block_id`. The target ID is the zero-based index of the\n/// target block in the original (unfiltered) block stream.\n///\n/// If multiple annotations target the same block, the last one wins.\n/// Non-priority annotations (`Summary`, `Tag`) are ignored — they are\n/// handled elsewhere in the pipeline.\n///\n/// Blocks without a priority annotation default to `Priority::Normal`\n/// (applied by the caller, not this function).\npub(crate) fn resolve_priorities(blocks: &[Block]) -> HashMap<u32, Priority> {\n    let mut map = HashMap::new();\n    for block in blocks {\n        if let BlockContent::Annotation(ann) = &block.content\n            && ann.kind == AnnotationKind::Priority\n            && let Some(&byte) = ann.value.first()\n            && let Ok(pri) = Priority::from_wire_byte(byte)\n        {\n            map.insert(ann.target_block_id, pri);\n        }\n    }\n    map\n}\n\n// ── Text Extraction for Estimation ──────────────────────────────────\n\n/// Extract the text content from a block for token estimation.\n///\n/// This mirrors the content extraction logic in the renderers, pulling\n/// out the UTF-8 text from each block variant. For blocks with non-UTF-8\n/// content (or variants where text extraction doesn't apply), returns a\n/// synthetic string whose length approximates the byte count.\n///\n/// The returned string is used only for token estimation — it is never\n/// rendered directly.\npub(crate) fn estimate_block_text(block: &Block) -> String {\n    match &block.content {\n        BlockContent::Code(c) => {\n            content_to_string(&c.content, 0).unwrap_or_else(|_| \"x\".repeat(c.content.len()))\n        }\n        BlockContent::Conversation(c) => {\n            content_to_string(&c.content, 0).unwrap_or_else(|_| \"x\".repeat(c.content.len()))\n        }\n        BlockContent::ToolResult(t) => {\n            content_to_string(&t.content, 0).unwrap_or_else(|_| \"x\".repeat(t.content.len()))\n        }\n        BlockContent::Document(d) => {\n            content_to_string(&d.content, 0).unwrap_or_else(|_| \"x\".repeat(d.content.len()))\n        }\n        BlockContent::StructuredData(d) => {\n            content_to_string(&d.content, 0).unwrap_or_else(|_| \"x\".repeat(d.content.len()))\n        }\n        BlockContent::Diff(d) => {\n            let mut text = String::new();\n            for hunk in &d.hunks {\n                text.push_str(&String::from_utf8_lossy(&hunk.lines));\n            }\n            text\n        }\n        BlockContent::Image(i) => {\n            content_to_string(&i.data, 0).unwrap_or_else(|_| \"x\".repeat(i.data.len()))\n        }\n        BlockContent::Extension(e) => {\n            content_to_string(&e.content, 0).unwrap_or_else(|_| \"x\".repeat(e.content.len()))\n        }\n        BlockContent::FileTree(t) => crate::render_xml::render_file_tree_entries(&t.entries, 0),\n        BlockContent::EmbeddingRef(e) => format!(\"embedding: {}\", e.model),\n        BlockContent::Unknown { body, .. } => \"x\".repeat(body.len()),\n        BlockContent::Annotation(_) | BlockContent::End => String::new(),\n    }\n}\n\n// ── Scan Pass ────────────────────────────────────────────────────────\n\n/// Scan pass: compute token estimates and resolve priorities.\n///\n/// Produces one [`BlockBudgetInfo`] per filtered block. The caller\n/// provides the priority map from [`resolve_priorities`] and the\n/// original indices mapping (filtered index → original block index)\n/// so that annotation targets resolve correctly.\n///\n/// ```text\n/// filtered[0] → original_indices[0] = 2 → priorities.get(2) → High\n/// filtered[1] → original_indices[1] = 4 → priorities.get(4) → None → Normal\n/// ```\npub(crate) fn scan_blocks(\n    filtered: &[&Block],\n    priorities: &HashMap<u32, Priority>,\n    estimator: &dyn TokenEstimator,\n    original_indices: &[usize],\n) -> Vec<BlockBudgetInfo> {\n    filtered\n        .iter()\n        .enumerate()\n        .map(|(i, block)| {\n            let orig_idx = original_indices[i];\n            #[allow(clippy::cast_possible_truncation)]\n            let priority = priorities\n                .get(&(orig_idx as u32))\n                .copied()\n                .unwrap_or(Priority::Normal);\n\n            let full_text = estimate_block_text(block);\n            let full_tokens = estimator.estimate(&full_text);\n\n            let (summary_tokens, has_summary) = if let Some(ref summary) = block.summary {\n                (Some(estimator.estimate(&summary.text)), true)\n            } else {\n                (None, false)\n            };\n\n            BlockBudgetInfo {\n                priority,\n                full_tokens,\n                summary_tokens,\n                has_summary,\n            }\n        })\n        .collect()\n}\n\n// ── Block Description ────────────────────────────────────────────────\n\n/// Extract a human-readable description for placeholder rendering.\n///\n/// Returns a string identifying the block — typically a file path for\n/// code blocks, a tool name for tool results, a title for documents,\n/// etc. Used in placeholder output like `[omitted: code src/main.rs]`.\npub(crate) fn block_description(block: &Block) -> String {\n    match &block.content {\n        BlockContent::Code(c) => c.path.clone(),\n        BlockContent::Conversation(c) => format!(\"{} turn\", role_display_name(c.role)),\n        BlockContent::FileTree(t) => format!(\"tree: {}\", t.root_path),\n        BlockContent::ToolResult(t) => t.tool_name.clone(),\n        BlockContent::Document(d) => d.title.clone(),\n        BlockContent::StructuredData(d) => {\n            format!(\n                \"{} data\",\n                crate::render_xml::data_format_display_name(d.format)\n            )\n        }\n        BlockContent::Diff(d) => d.path.clone(),\n        BlockContent::EmbeddingRef(e) => format!(\"embedding: {}\", e.model),\n        BlockContent::Image(i) => i.alt_text.clone(),\n        BlockContent::Extension(e) => format!(\"{}/{}\", e.namespace, e.type_name),\n        BlockContent::Unknown { type_id, .. } => format!(\"unknown 0x{type_id:02X}\"),\n        BlockContent::Annotation(_) | BlockContent::End => String::new(),\n    }\n}\n\n/// Map a [`BlockType`] to a short label for placeholder rendering.\n///\n/// ```text\n/// ┌───────────────┬──────────────┐\n/// │ BlockType     │ Label        │\n/// ├───────────────┼──────────────┤\n/// │ Code          │ \"code\"       │\n/// │ Conversation  │ \"conversation\"│\n/// │ FileTree      │ \"file-tree\"  │\n/// │ ToolResult    │ \"tool-result\"│\n/// │ Document      │ \"document\"   │\n/// │ StructuredData│ \"data\"       │\n/// │ Diff          │ \"diff\"       │\n/// │ Image         │ \"image\"      │\n/// │ Extension     │ \"extension\"  │\n/// │ (other)       │ \"block\"      │\n/// └───────────────┴──────────────┘\n/// ```\npub(crate) fn block_type_label(bt: &BlockType) -> &'static str {\n    match bt {\n        BlockType::Code => \"code\",\n        BlockType::Conversation => \"conversation\",\n        BlockType::FileTree => \"file-tree\",\n        BlockType::ToolResult => \"tool-result\",\n        BlockType::Document => \"document\",\n        BlockType::StructuredData => \"data\",\n        BlockType::Diff => \"diff\",\n        BlockType::Image => \"image\",\n        BlockType::Extension => \"extension\",\n        _ => \"block\",\n    }\n}\n\n// ── Allocation Pass ──────────────────────────────────────────────────\n\n/// Estimated token cost of a placeholder line.\n///\n/// Placeholders are short strings like `[omitted: code src/main.rs ~823tok]`\n/// which typically consume around 10-15 tokens. We use a conservative\n/// fixed estimate rather than measuring each one.\nconst PLACEHOLDER_TOKEN_COST: u32 = 10;\n\n/// Budget allocation pass — assign a [`RenderDecision`] to each block.\n///\n/// Takes the scanned block metadata, the total token budget, and the\n/// filtered block slice. Returns a `Vec<RenderDecision>` parallel to\n/// the filtered slice (same length, same order).\n///\n/// The algorithm:\n/// 1. Create an index list sorted by priority (ascending: Critical first).\n///    Within the same priority, original order is preserved (stable sort).\n/// 2. Walk sorted indices, greedily subtracting from remaining budget:\n///    - **Critical**: always `Full` (never degraded, even over budget).\n///    - **High**: `Full` if budget allows, else `Summary` if available,\n///      else `Full` anyway (high-priority content is too important to omit).\n///    - **Normal**: `Full` if budget allows, else `Summary` if available,\n///      else `Placeholder`.\n///    - **Low**: `Summary` if budget allows, else `Placeholder`.\n///    - **Background**: `Placeholder` if budget allows, else `Omit`.\n/// 3. Return decisions reordered to match the original block sequence.\npub(crate) fn allocate_budget(\n    infos: &[BlockBudgetInfo],\n    budget: u32,\n    filtered: &[&Block],\n) -> Vec<RenderDecision> {\n    let mut decisions = vec![RenderDecision::Omit; infos.len()];\n    let mut remaining = budget;\n\n    // Sort indices by priority (stable: preserves original order within\n    // the same priority level). Priority::Critical < Priority::High < ...\n    let mut sorted: Vec<usize> = (0..infos.len()).collect();\n    sorted.sort_by(|&a, &b| infos[a].priority.cmp(&infos[b].priority));\n\n    for idx in sorted {\n        let info = &infos[idx];\n        let block = filtered[idx];\n\n        match info.priority {\n            Priority::Critical => {\n                decisions[idx] = RenderDecision::Full;\n                remaining = remaining.saturating_sub(info.full_tokens);\n            }\n            Priority::High => {\n                if info.full_tokens <= remaining {\n                    decisions[idx] = RenderDecision::Full;\n                    remaining -= info.full_tokens;\n                } else if info.has_summary {\n                    let stok = info.summary_tokens.unwrap_or(0);\n                    if stok <= remaining {\n                        decisions[idx] = RenderDecision::Summary;\n                        remaining -= stok;\n                    } else {\n                        // High-priority: render full anyway, over budget\n                        decisions[idx] = RenderDecision::Full;\n                        remaining = 0;\n                    }\n                } else {\n                    // No summary available, render full over budget\n                    decisions[idx] = RenderDecision::Full;\n                    remaining = 0;\n                }\n            }\n            Priority::Normal => {\n                if info.full_tokens <= remaining {\n                    decisions[idx] = RenderDecision::Full;\n                    remaining -= info.full_tokens;\n                } else if info.has_summary {\n                    let stok = info.summary_tokens.unwrap_or(0);\n                    if stok <= remaining {\n                        decisions[idx] = RenderDecision::Summary;\n                        remaining -= stok;\n                    } else {\n                        decisions[idx] = make_placeholder(block, info.full_tokens);\n                    }\n                } else {\n                    decisions[idx] = make_placeholder(block, info.full_tokens);\n                }\n            }\n            Priority::Low => {\n                if info.has_summary {\n                    let stok = info.summary_tokens.unwrap_or(0);\n                    if stok <= remaining {\n                        decisions[idx] = RenderDecision::Summary;\n                        remaining -= stok;\n                    } else {\n                        decisions[idx] = make_placeholder(block, info.full_tokens);\n                    }\n                } else {\n                    decisions[idx] = make_placeholder(block, info.full_tokens);\n                }\n            }\n            Priority::Background => {\n                if PLACEHOLDER_TOKEN_COST <= remaining {\n                    decisions[idx] = make_placeholder(block, info.full_tokens);\n                    remaining = remaining.saturating_sub(PLACEHOLDER_TOKEN_COST);\n                } else {\n                    decisions[idx] = RenderDecision::Omit;\n                }\n            }\n        }\n    }\n\n    decisions\n}\n\n/// Build a `RenderDecision::Placeholder` for a block.\nfn make_placeholder(block: &Block, omitted_tokens: u32) -> RenderDecision {\n    RenderDecision::Placeholder {\n        block_type: block.block_type.clone(),\n        description: block_description(block),\n        omitted_tokens,\n    }\n}\n\n// ── Public Entry Point ───────────────────────────────────────────────\n\n/// Run the complete budget engine: resolve priorities, scan, allocate.\n///\n/// This is the main entry point called by `DefaultDriver::render()`.\n/// It ties together the three pipeline stages:\n///\n/// ```text\n/// all_blocks ──▶ resolve_priorities() ──▶ HashMap<u32, Priority>\n///                                               │\n/// filtered ──────────────────────────────▶ scan_blocks()\n///                                               │\n///                                         Vec<BlockBudgetInfo>\n///                                               │\n///                                        allocate_budget()\n///                                               │\n///                                        Vec<RenderDecision>\n/// ```\n///\n/// Returns a `Vec<RenderDecision>` parallel to `filtered` — each entry\n/// tells the renderer how to handle the corresponding block.\npub(crate) fn compute_budget_decisions(\n    all_blocks: &[Block],\n    filtered: &[&Block],\n    original_indices: &[usize],\n    budget: u32,\n    estimator: &dyn TokenEstimator,\n) -> Vec<RenderDecision> {\n    let priorities = resolve_priorities(all_blocks);\n    let infos = scan_blocks(filtered, &priorities, estimator, original_indices);\n    allocate_budget(&infos, budget, filtered)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use bcp_types::annotation::AnnotationBlock;\n    use bcp_types::code::CodeBlock;\n    use bcp_types::conversation::ConversationBlock;\n    use bcp_types::enums::{Lang, Role};\n    use bcp_types::summary::Summary;\n    use bcp_types::tool_result::ToolResultBlock;\n    use bcp_wire::block_frame::BlockFlags;\n\n    // ── Test Helpers ────────────────────────────────────────────────\n\n    fn code_block(path: &str, content: &[u8]) -> Block {\n        Block {\n            block_type: BlockType::Code,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::Code(CodeBlock {\n                lang: Lang::Rust,\n                path: path.to_string(),\n                content: content.to_vec(),\n                line_range: None,\n            }),\n        }\n    }\n\n    fn code_block_with_summary(path: &str, content: &[u8], summary: &str) -> Block {\n        Block {\n            block_type: BlockType::Code,\n            flags: BlockFlags::HAS_SUMMARY,\n            summary: Some(Summary {\n                text: summary.to_string(),\n            }),\n            content: BlockContent::Code(CodeBlock {\n                lang: Lang::Rust,\n                path: path.to_string(),\n                content: content.to_vec(),\n                line_range: None,\n            }),\n        }\n    }\n\n    fn conversation_block(role: Role, content: &[u8]) -> Block {\n        Block {\n            block_type: BlockType::Conversation,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::Conversation(ConversationBlock {\n                role,\n                content: content.to_vec(),\n                tool_call_id: None,\n            }),\n        }\n    }\n\n    fn priority_annotation(target: u32, priority: Priority) -> Block {\n        Block {\n            block_type: BlockType::Annotation,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::Annotation(AnnotationBlock {\n                target_block_id: target,\n                kind: AnnotationKind::Priority,\n                value: vec![priority.to_wire_byte()],\n            }),\n        }\n    }\n\n    fn tag_annotation(target: u32, tag: &str) -> Block {\n        Block {\n            block_type: BlockType::Annotation,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::Annotation(AnnotationBlock {\n                target_block_id: target,\n                kind: AnnotationKind::Tag,\n                value: tag.as_bytes().to_vec(),\n            }),\n        }\n    }\n\n    // ── HeuristicEstimator tests ────────────────────────────────────\n\n    #[test]\n    fn heuristic_estimator_empty() {\n        assert_eq!(HeuristicEstimator.estimate(\"\"), 0);\n    }\n\n    #[test]\n    fn heuristic_estimator_min_one() {\n        assert_eq!(HeuristicEstimator.estimate(\"a\"), 1);\n        assert_eq!(HeuristicEstimator.estimate(\"ab\"), 1);\n        assert_eq!(HeuristicEstimator.estimate(\"abc\"), 1);\n    }\n\n    #[test]\n    fn heuristic_estimator_basic() {\n        assert_eq!(HeuristicEstimator.estimate(\"hello world\"), 2);\n        let text = \"a\".repeat(100);\n        assert_eq!(HeuristicEstimator.estimate(&text), 25);\n    }\n\n    // ── CodeAwareEstimator tests ────────────────────────────────────\n\n    #[test]\n    fn code_aware_estimator_empty() {\n        assert_eq!(CodeAwareEstimator.estimate(\"\"), 0);\n    }\n\n    #[test]\n    fn code_aware_estimator_prose() {\n        let prose = \"This is a paragraph of English text.\\n\\\n                     It has no indentation at all.\\n\\\n                     Every line starts at column zero.\";\n        let expected = prose.len() as u32 / 4;\n        assert_eq!(CodeAwareEstimator.estimate(prose), expected);\n    }\n\n    #[test]\n    fn code_aware_estimator_code() {\n        let code = \"fn main() {\\n    let x = 42;\\n    println!(\\\"{x}\\\");\\n}\";\n        let expected = code.len() as u32 / 3;\n        assert_eq!(CodeAwareEstimator.estimate(code), expected);\n    }\n\n    #[test]\n    fn code_aware_estimator_min_one() {\n        assert_eq!(CodeAwareEstimator.estimate(\"x\"), 1);\n    }\n\n    #[test]\n    fn code_aware_estimator_ignores_empty_lines() {\n        let text = \"line one\\n\\n\\n    indented\\n\\n    also indented\\nflat\";\n        let expected = text.len() as u32 / 3;\n        assert_eq!(CodeAwareEstimator.estimate(text), expected);\n    }\n\n    // ── Priority Resolution tests ───────────────────────────────────\n\n    #[test]\n    fn resolve_priorities_empty() {\n        let blocks: Vec<Block> = vec![];\n        let map = resolve_priorities(&blocks);\n        assert!(map.is_empty());\n    }\n\n    #[test]\n    fn resolve_priorities_single() {\n        let blocks = vec![\n            code_block(\"main.rs\", b\"fn main() {}\"),\n            priority_annotation(0, Priority::Critical),\n        ];\n        let map = resolve_priorities(&blocks);\n        assert_eq!(map.get(&0), Some(&Priority::Critical));\n    }\n\n    #[test]\n    fn resolve_priorities_multiple_targets() {\n        let blocks = vec![\n            code_block(\"a.rs\", b\"a\"),\n            code_block(\"b.rs\", b\"b\"),\n            priority_annotation(0, Priority::High),\n            priority_annotation(1, Priority::Low),\n        ];\n        let map = resolve_priorities(&blocks);\n        assert_eq!(map.get(&0), Some(&Priority::High));\n        assert_eq!(map.get(&1), Some(&Priority::Low));\n    }\n\n    #[test]\n    fn resolve_priorities_last_wins() {\n        let blocks = vec![\n            code_block(\"main.rs\", b\"fn main() {}\"),\n            priority_annotation(0, Priority::Low),\n            priority_annotation(0, Priority::Critical),\n        ];\n        let map = resolve_priorities(&blocks);\n        assert_eq!(map.get(&0), Some(&Priority::Critical));\n    }\n\n    #[test]\n    fn resolve_priorities_ignores_non_priority() {\n        let blocks = vec![\n            code_block(\"main.rs\", b\"fn main() {}\"),\n            tag_annotation(0, \"security\"),\n        ];\n        let map = resolve_priorities(&blocks);\n        assert!(map.is_empty());\n    }\n\n    // ── Budget Allocation tests ─────────────────────────────────────\n\n    #[test]\n    fn allocate_budget_unlimited() {\n        // Large budget: all blocks should get Full\n        let blocks = vec![\n            code_block(\"a.rs\", &\"a\".repeat(100).into_bytes()),\n            code_block(\"b.rs\", &\"b\".repeat(100).into_bytes()),\n        ];\n        let filtered: Vec<&Block> = blocks.iter().collect();\n        let original_indices = vec![0, 1];\n\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            100_000,\n            &HeuristicEstimator,\n        );\n        assert_eq!(decisions.len(), 2);\n        assert_eq!(decisions[0], RenderDecision::Full);\n        assert_eq!(decisions[1], RenderDecision::Full);\n    }\n\n    #[test]\n    fn allocate_budget_critical_always_full() {\n        // Budget of 0, but Critical block still gets Full\n        let blocks = vec![\n            code_block(\"main.rs\", &\"x\".repeat(400).into_bytes()),\n            priority_annotation(0, Priority::Critical),\n        ];\n        let filtered: Vec<&Block> = blocks\n            .iter()\n            .filter(|b| b.block_type != BlockType::Annotation)\n            .collect();\n        let original_indices = vec![0];\n\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            0,\n            &HeuristicEstimator,\n        );\n        assert_eq!(decisions[0], RenderDecision::Full);\n    }\n\n    #[test]\n    fn allocate_budget_normal_degrades_to_summary() {\n        // Normal block with summary, tight budget → Summary\n        let content = \"x\".repeat(400); // ~100 tokens\n        let blocks = vec![code_block_with_summary(\n            \"main.rs\",\n            content.as_bytes(),\n            \"Entry point.\", // ~3 tokens\n        )];\n        let filtered: Vec<&Block> = blocks.iter().collect();\n        let original_indices = vec![0];\n\n        // Budget of 10: not enough for full (100 tokens) but enough for summary (3)\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            10,\n            &HeuristicEstimator,\n        );\n        assert_eq!(decisions[0], RenderDecision::Summary);\n    }\n\n    #[test]\n    fn allocate_budget_low_gets_placeholder() {\n        // Low priority block without summary, tight budget → Placeholder\n        let content = \"x\".repeat(400);\n        let blocks = vec![\n            code_block(\"main.rs\", content.as_bytes()),\n            priority_annotation(0, Priority::Low),\n        ];\n        let filtered: Vec<&Block> = blocks\n            .iter()\n            .filter(|b| b.block_type != BlockType::Annotation)\n            .collect();\n        let original_indices = vec![0];\n\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            5,\n            &HeuristicEstimator,\n        );\n        assert!(\n            matches!(decisions[0], RenderDecision::Placeholder { .. }),\n            \"Low priority without summary should be Placeholder, got {:?}\",\n            decisions[0]\n        );\n    }\n\n    #[test]\n    fn allocate_budget_background_omit() {\n        // Background block with zero budget → Omit\n        let blocks = vec![\n            code_block(\"bg.rs\", b\"background stuff\"),\n            priority_annotation(0, Priority::Background),\n        ];\n        let filtered: Vec<&Block> = blocks\n            .iter()\n            .filter(|b| b.block_type != BlockType::Annotation)\n            .collect();\n        let original_indices = vec![0];\n\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            0,\n            &HeuristicEstimator,\n        );\n        assert_eq!(decisions[0], RenderDecision::Omit);\n    }\n\n    #[test]\n    fn allocate_budget_respects_priority_ordering() {\n        // Critical block consumes budget before Normal block\n        let big_content = \"x\".repeat(400); // ~100 tokens each\n        let blocks = vec![\n            // Block 0: Normal priority (default)\n            code_block_with_summary(\"normal.rs\", big_content.as_bytes(), \"Normal summary.\"),\n            // Block 1: Critical priority\n            code_block(\"critical.rs\", big_content.as_bytes()),\n            priority_annotation(1, Priority::Critical),\n        ];\n        let filtered: Vec<&Block> = blocks\n            .iter()\n            .filter(|b| b.block_type != BlockType::Annotation)\n            .collect();\n        let original_indices = vec![0, 1];\n\n        // Budget = 120: enough for one full (100) + one summary (4), not two fulls\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            120,\n            &HeuristicEstimator,\n        );\n        // Critical block should be Full (processed first due to priority)\n        assert_eq!(\n            decisions[1],\n            RenderDecision::Full,\n            \"Critical should be Full\"\n        );\n        // Normal block should be Summary (budget was consumed by Critical)\n        assert_eq!(\n            decisions[0],\n            RenderDecision::Summary,\n            \"Normal should degrade to Summary\"\n        );\n    }\n\n    #[test]\n    fn no_annotations_all_normal() {\n        // Without annotations, all blocks default to Normal\n        let content = \"x\".repeat(400);\n        let blocks = vec![\n            code_block_with_summary(\"a.rs\", content.as_bytes(), \"Summary A.\"),\n            code_block_with_summary(\"b.rs\", content.as_bytes(), \"Summary B.\"),\n        ];\n        let filtered: Vec<&Block> = blocks.iter().collect();\n        let original_indices = vec![0, 1];\n\n        // Budget enough for one full + one summary\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            120,\n            &HeuristicEstimator,\n        );\n        // First Normal block should get Full (has budget), second should degrade\n        assert_eq!(decisions[0], RenderDecision::Full);\n        assert_eq!(decisions[1], RenderDecision::Summary);\n    }\n\n    #[test]\n    fn block_description_code() {\n        let block = code_block(\"src/main.rs\", b\"fn main() {}\");\n        assert_eq!(block_description(&block), \"src/main.rs\");\n    }\n\n    #[test]\n    fn block_description_conversation() {\n        let block = conversation_block(Role::User, b\"Hello\");\n        assert_eq!(block_description(&block), \"user turn\");\n    }\n\n    #[test]\n    fn zero_budget_all_critical() {\n        // Zero budget, all Critical → all Full (Critical is never degraded)\n        let blocks = vec![\n            code_block(\"a.rs\", &\"a\".repeat(400).into_bytes()),\n            priority_annotation(0, Priority::Critical),\n            code_block(\"b.rs\", &\"b\".repeat(400).into_bytes()),\n            priority_annotation(2, Priority::Critical),\n        ];\n        let filtered: Vec<&Block> = blocks\n            .iter()\n            .filter(|b| b.block_type != BlockType::Annotation)\n            .collect();\n        let original_indices = vec![0, 2];\n\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            0,\n            &HeuristicEstimator,\n        );\n        assert_eq!(decisions[0], RenderDecision::Full);\n        assert_eq!(decisions[1], RenderDecision::Full);\n    }\n\n    #[test]\n    fn mixed_priorities_five_blocks() {\n        // 5 blocks with mixed priorities and tight budget\n        let content = \"x\".repeat(400); // ~100 tokens each\n        let blocks = vec![\n            // 0: Background\n            code_block(\"bg.rs\", content.as_bytes()),\n            priority_annotation(0, Priority::Background),\n            // 2: Critical\n            code_block(\"crit.rs\", content.as_bytes()),\n            priority_annotation(2, Priority::Critical),\n            // 4: Normal with summary\n            code_block_with_summary(\"normal.rs\", content.as_bytes(), \"Normal summary.\"),\n            // 5: Low\n            code_block(\"low.rs\", content.as_bytes()),\n            priority_annotation(5, Priority::Low),\n            // 7: High with summary\n            code_block_with_summary(\"high.rs\", content.as_bytes(), \"High summary.\"),\n            priority_annotation(7, Priority::High),\n        ];\n        let filtered: Vec<&Block> = blocks\n            .iter()\n            .filter(|b| b.block_type != BlockType::Annotation)\n            .collect();\n        let original_indices = vec![0, 2, 4, 5, 7];\n\n        // Budget = 150: enough for Critical (100) + some leftovers\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            150,\n            &HeuristicEstimator,\n        );\n\n        // Critical → Full (always)\n        assert_eq!(\n            decisions[1],\n            RenderDecision::Full,\n            \"Critical should be Full\"\n        );\n        // High → Summary or Full depending on remaining budget\n        assert!(\n            matches!(decisions[4], RenderDecision::Full | RenderDecision::Summary),\n            \"High should be Full or Summary, got {:?}\",\n            decisions[4]\n        );\n        // Background → Placeholder or Omit\n        assert!(\n            matches!(\n                decisions[0],\n                RenderDecision::Placeholder { .. } | RenderDecision::Omit\n            ),\n            \"Background should be Placeholder or Omit, got {:?}\",\n            decisions[0]\n        );\n    }\n\n    #[test]\n    fn block_without_summary_at_normal_gets_placeholder() {\n        // Normal block without summary, tight budget → Placeholder (not Summary)\n        let content = \"x\".repeat(400); // ~100 tokens\n        let blocks = vec![code_block(\"nosummary.rs\", content.as_bytes())];\n        let filtered: Vec<&Block> = blocks.iter().collect();\n        let original_indices = vec![0];\n\n        let decisions = compute_budget_decisions(\n            &blocks,\n            &filtered,\n            &original_indices,\n            5, // way too small\n            &HeuristicEstimator,\n        );\n        assert!(\n            matches!(decisions[0], RenderDecision::Placeholder { .. }),\n            \"Normal without summary should be Placeholder, got {:?}\",\n            decisions[0]\n        );\n    }\n\n    #[test]\n    fn block_description_tool_result() {\n        let block = Block {\n            block_type: BlockType::ToolResult,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::ToolResult(ToolResultBlock {\n                tool_name: \"ripgrep\".to_string(),\n                status: bcp_types::enums::Status::Ok,\n                content: b\"results\".to_vec(),\n                schema_hint: None,\n            }),\n        };\n        assert_eq!(block_description(&block), \"ripgrep\");\n    }\n}\n",
      "language": "rust",
      "path": "bcp-driver/src/budget.rs",
      "type": "code"
    },
    {
      "content": "use bcp_types::BlockType;\n\n/// Configuration for the BCP driver.\n///\n/// Controls how decoded blocks are rendered into model-ready text.\n/// The driver uses these settings to select the output format, apply\n/// model-specific tuning, filter which block types appear in the\n/// rendered output, and manage token budget constraints.\n///\n/// ```text\n/// ┌────────────────┬────────────────────────────────────────────────────┐\n/// │ Field          │ Purpose                                            │\n/// ├────────────────┼────────────────────────────────────────────────────┤\n/// │ mode           │ Selects XML, Markdown, or Minimal output format    │\n/// │ target_model   │ Hints for model-specific formatting adjustments    │\n/// │ include_types  │ Optional allowlist — only render matching blocks   │\n/// │ token_budget   │ Approximate token limit for rendered output        │\n/// │ verbosity      │ Full / Summary / Adaptive rendering mode           │\n/// └────────────────┴────────────────────────────────────────────────────┘\n/// ```\n///\n/// When `include_types` is `None`, all block types are rendered (except\n/// `Annotation`, which is metadata-only and never produces visible output).\n/// When `Some(vec)`, only blocks whose `BlockType` is in the list are\n/// rendered; all others are silently skipped.\n///\n/// When `token_budget` is `Some(n)`, the driver uses a two-pass algorithm\n/// (RFC §5.5) to fit blocks within the budget: high-priority blocks get\n/// full content, while lower-priority blocks degrade to summaries or\n/// placeholders. When `None`, all blocks render with full content.\npub struct DriverConfig {\n    /// Output format mode. Determines the textual structure of the\n    /// rendered output.\n    pub mode: OutputMode,\n\n    /// Model family hint. Affects minor formatting choices (e.g., XML\n    /// attribute ordering for Claude compatibility, markdown header\n    /// depth for GPT).\n    pub target_model: Option<ModelFamily>,\n\n    /// Block type filter. When set, only blocks of these types are\n    /// rendered; all others are silently skipped.\n    pub include_types: Option<Vec<BlockType>>,\n\n    /// Approximate token budget. When set, the driver uses summaries\n    /// and placeholders for low-priority blocks to fit within this\n    /// limit. When `None`, all blocks render with full content.\n    pub token_budget: Option<u32>,\n\n    /// Verbosity mode for budget-aware rendering. Controls whether the\n    /// driver renders full content, summaries, or auto-selects per\n    /// block based on budget and priority.\n    pub verbosity: Verbosity,\n}\n\nimpl Default for DriverConfig {\n    /// Default configuration: XML mode, no model hint, no type filter,\n    /// no token budget, adaptive verbosity.\n    ///\n    /// XML mode is the default because it produces the most semantically\n    /// structured output — Claude-family models parse it natively, and\n    /// other models handle it well too. Adaptive verbosity means the\n    /// driver will use the budget engine when a `token_budget` is set,\n    /// and render everything in full otherwise.\n    fn default() -> Self {\n        Self {\n            mode: OutputMode::Xml,\n            target_model: None,\n            include_types: None,\n            token_budget: None,\n            verbosity: Verbosity::default(),\n        }\n    }\n}\n\n/// Verbosity modes for budget-aware rendering (RFC §5.5).\n///\n/// Controls how the driver decides between full content and summary\n/// rendering for each block. This interacts with `token_budget`:\n///\n/// ```text\n/// ┌──────────┬────────────────────────────────────────────────────────┐\n/// │ Mode     │ Behavior                                               │\n/// ├──────────┼────────────────────────────────────────────────────────┤\n/// │ Full     │ Always render full content. Ignore budget entirely.    │\n/// │ Summary  │ Always render summaries where available, full content  │\n/// │          │ otherwise. Budget is ignored.                          │\n/// │ Adaptive │ Auto-select per block based on budget + priority.      │\n/// │          │ Without a budget set, behaves like Full.               │\n/// └──────────┴────────────────────────────────────────────────────────┘\n/// ```\n#[derive(Clone, Copy, Debug, Default, PartialEq, Eq)]\npub enum Verbosity {\n    /// Always render full content, ignoring any token budget.\n    Full,\n    /// Always render summaries where available. Blocks without\n    /// summaries fall back to full content. Budget is ignored.\n    Summary,\n    /// Auto-select per block based on budget and priority. This is the\n    /// default mode. Without a `token_budget`, behaves like `Full`.\n    #[default]\n    Adaptive,\n}\n\n/// Output format modes per RFC §5.4.\n///\n/// Each mode represents a different tradeoff between semantic structure,\n/// model compatibility, and token efficiency. The driver dispatches to\n/// a different renderer based on this setting.\n///\n/// ```text\n/// ┌──────────┬─────────────────────────────────────────────────────────┐\n/// │ Mode     │ Description                                             │\n/// ├──────────┼─────────────────────────────────────────────────────────┤\n/// │ Xml      │ <code lang=\"rust\" path=\"...\">content</code>            │\n/// │          │ Optimized for Claude-family models. Wraps all output    │\n/// │          │ in <context>...</context>.                              │\n/// ├──────────┼─────────────────────────────────────────────────────────┤\n/// │ Markdown │ ```rust\\n// src/main.rs\\ncontent\\n```                   │\n/// │          │ Compatible with all models, more tokens.                │\n/// ├──────────┼─────────────────────────────────────────────────────────┤\n/// │ Minimal  │ --- src/main.rs [rust] ---\\ncontent                     │\n/// │          │ Maximum token efficiency, fewest structural tokens.     │\n/// └──────────┴─────────────────────────────────────────────────────────┘\n/// ```\n#[derive(Clone, Copy, Debug, PartialEq, Eq)]\npub enum OutputMode {\n    Xml,\n    Markdown,\n    Minimal,\n}\n\n/// Model family hints for output tuning.\n///\n/// The driver may adjust minor formatting choices based on the target\n/// model. For example, Claude models have strong XML comprehension, so\n/// the XML renderer can use Claude-optimized attribute ordering. GPT\n/// models handle markdown well, so the markdown renderer can use\n/// GPT-friendly header conventions.\n///\n/// This is a hint, not a hard requirement — the output is valid regardless\n/// of the target model setting.\n///\n/// ```text\n/// ┌─────────┬────────────────────────────────────────────────────┐\n/// │ Family  │ Notes                                              │\n/// ├─────────┼────────────────────────────────────────────────────┤\n/// │ Claude  │ Strong XML parsing, prefers semantic tags          │\n/// │ Gpt     │ Strong markdown parsing, prefers fenced blocks     │\n/// │ Gemini  │ Handles both XML and markdown well                 │\n/// │ Generic │ No model-specific tuning applied                   │\n/// └─────────┴────────────────────────────────────────────────────┘\n/// ```\n#[derive(Clone, Copy, Debug, PartialEq, Eq)]\npub enum ModelFamily {\n    Claude,\n    Gpt,\n    Gemini,\n    Generic,\n}\n",
      "language": "rust",
      "path": "bcp-driver/src/config.rs",
      "type": "code"
    },
    {
      "content": "use bcp_types::BlockType;\nuse bcp_types::block::Block;\n\nuse crate::budget::{CodeAwareEstimator, RenderDecision, compute_budget_decisions};\nuse crate::config::{DriverConfig, OutputMode, Verbosity};\nuse crate::error::DriverError;\nuse crate::render_markdown::MarkdownRenderer;\nuse crate::render_minimal::MinimalRenderer;\nuse crate::render_xml::XmlRenderer;\n\n/// Core driver interface — renders decoded blocks into model-ready text.\n///\n/// This is the primary trait consumers use to convert a `Vec<Block>` (from\n/// `bcp-decoder`) into a string that can be injected into an LLM's context\n/// window. The driver is not a simple serializer — it is an opinionated\n/// renderer that makes decisions about how to present context to maximize\n/// model comprehension within a token budget.\n///\n/// The trait takes an immutable block slice and a configuration reference,\n/// returning either the rendered text or a `DriverError`. Implementations\n/// are expected to be stateless — all configuration comes through\n/// `DriverConfig`.\n///\n/// ```text\n/// Vec<Block> ──▶ BcpDriver::render() ──▶ model-ready String\n///                        │\n///                  DriverConfig\n///                  (mode, verbosity, token_budget, ...)\n/// ```\npub trait BcpDriver {\n    /// Render a complete set of decoded blocks into model-ready text.\n    ///\n    /// # Errors\n    ///\n    /// Returns `DriverError::EmptyInput` if `blocks` is empty (after\n    /// filtering by `config.include_types`, if set).\n    fn render(&self, blocks: &[Block], config: &DriverConfig) -> Result<String, DriverError>;\n}\n\n/// Default driver implementation — filtering, budget allocation, and\n/// renderer dispatch.\n///\n/// This is the standard entry point for rendering. It handles:\n///\n/// 1. **Block filtering** — removes Annotation/End blocks and applies\n///    `config.include_types` to skip non-matching blocks.\n/// 2. **Budget decisions** — based on `config.verbosity` and\n///    `config.token_budget`, computes a [`RenderDecision`] per block\n///    (Full, Summary, Placeholder, or Omit).\n/// 3. **Renderer dispatch** — selects `XmlRenderer`, `MarkdownRenderer`,\n///    or `MinimalRenderer` based on `config.mode`, using the\n///    decision-aware rendering path.\n///\n/// ```text\n/// ┌─────────────┐     ┌───────────────┐     ┌──────────────────┐\n/// │ &[Block]    │────▶│ filter +      │────▶│ XmlRenderer      │\n/// │             │     │ budget engine │     │ MarkdownRenderer │\n/// │             │     │ + dispatch    │     │ MinimalRenderer  │\n/// └─────────────┘     └───────────────┘     └──────────────────┘\n///                           │                       │\n///                     DriverConfig            String output\n///                     (mode, verbosity,\n///                      token_budget, ...)\n/// ```\npub struct DefaultDriver;\n\nimpl BcpDriver for DefaultDriver {\n    /// Render decoded blocks into model-ready text.\n    ///\n    /// The rendering pipeline:\n    ///\n    /// 1. Filter: remove Annotation/End blocks, apply `include_types`.\n    /// 2. Decide: compute per-block [`RenderDecision`] based on verbosity\n    ///    and token budget.\n    /// 3. Render: dispatch to the appropriate renderer with decisions.\n    ///\n    /// # Errors\n    ///\n    /// - `DriverError::EmptyInput` if no renderable blocks remain after filtering.\n    /// - `DriverError::InvalidContent` if a block contains non-UTF-8 bytes.\n    fn render(&self, blocks: &[Block], config: &DriverConfig) -> Result<String, DriverError> {\n        // Step 1: Filter blocks, tracking original indices for annotation mapping\n        let mut filtered: Vec<&Block> = Vec::new();\n        let mut original_indices: Vec<usize> = Vec::new();\n\n        for (i, b) in blocks.iter().enumerate() {\n            if b.block_type == BlockType::Annotation || b.block_type == BlockType::End {\n                continue;\n            }\n            if let Some(ref types) = config.include_types\n                && !types.contains(&b.block_type)\n            {\n                continue;\n            }\n            filtered.push(b);\n            original_indices.push(i);\n        }\n\n        if filtered.is_empty() {\n            return Err(DriverError::EmptyInput);\n        }\n\n        // Step 2: Compute render decisions\n        let decisions = match (config.token_budget, config.verbosity) {\n            // Summary mode (with or without budget): summaries where available\n            (_, Verbosity::Summary) => filtered\n                .iter()\n                .map(|b| {\n                    if b.summary.is_some() {\n                        RenderDecision::Summary\n                    } else {\n                        RenderDecision::Full\n                    }\n                })\n                .collect(),\n            // Budget + Adaptive: run the full budget engine\n            (Some(budget), Verbosity::Adaptive) => compute_budget_decisions(\n                blocks,\n                &filtered,\n                &original_indices,\n                budget,\n                &CodeAwareEstimator,\n            ),\n            // All other cases: render everything in full\n            // (no budget, or Full verbosity regardless of budget)\n            _ => vec![RenderDecision::Full; filtered.len()],\n        };\n\n        // Step 3: Build (block, decision) pairs and dispatch to renderer\n        let items: Vec<(&Block, &RenderDecision)> =\n            filtered.iter().copied().zip(decisions.iter()).collect();\n\n        match config.mode {\n            OutputMode::Xml => XmlRenderer::render_all_with_decisions(&items),\n            OutputMode::Markdown => MarkdownRenderer::render_all_with_decisions(&items),\n            OutputMode::Minimal => MinimalRenderer::render_all_with_decisions(&items),\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use bcp_types::annotation::AnnotationBlock;\n    use bcp_types::block::BlockContent;\n    use bcp_types::code::CodeBlock;\n    use bcp_types::conversation::ConversationBlock;\n    use bcp_types::enums::{AnnotationKind, Lang, Role, Status};\n    use bcp_types::file_tree::{FileEntry, FileEntryKind, FileTreeBlock};\n    use bcp_types::summary::Summary;\n    use bcp_types::tool_result::ToolResultBlock;\n    use bcp_wire::block_frame::BlockFlags;\n\n    fn code_block(lang: Lang, path: &str, content: &[u8]) -> Block {\n        Block {\n            block_type: BlockType::Code,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::Code(CodeBlock {\n                lang,\n                path: path.to_string(),\n                content: content.to_vec(),\n                line_range: None,\n            }),\n        }\n    }\n\n    fn conversation_block(role: Role, content: &[u8]) -> Block {\n        Block {\n            block_type: BlockType::Conversation,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::Conversation(ConversationBlock {\n                role,\n                content: content.to_vec(),\n                tool_call_id: None,\n            }),\n        }\n    }\n\n    #[test]\n    fn empty_input_returns_error() {\n        let driver = DefaultDriver;\n        let config = DriverConfig::default();\n        let result = driver.render(&[], &config);\n        assert!(matches!(result, Err(DriverError::EmptyInput)));\n    }\n\n    #[test]\n    fn annotation_blocks_filtered_out() {\n        let driver = DefaultDriver;\n        let config = DriverConfig::default();\n        let blocks = vec![Block {\n            block_type: BlockType::Annotation,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::Annotation(AnnotationBlock {\n                target_block_id: 0,\n                kind: AnnotationKind::Priority,\n                value: vec![0x01],\n            }),\n        }];\n        let result = driver.render(&blocks, &config);\n        assert!(matches!(result, Err(DriverError::EmptyInput)));\n    }\n\n    #[test]\n    fn include_types_filter() {\n        let driver = DefaultDriver;\n        let config = DriverConfig {\n            mode: OutputMode::Minimal,\n            include_types: Some(vec![BlockType::Code]),\n            ..DriverConfig::default()\n        };\n        let blocks = vec![\n            code_block(Lang::Rust, \"main.rs\", b\"fn main() {}\"),\n            conversation_block(Role::User, b\"Hello\"),\n        ];\n        let result = driver.render(&blocks, &config).unwrap();\n        assert!(result.contains(\"main.rs\"));\n        assert!(!result.contains(\"Hello\"));\n    }\n\n    #[test]\n    fn include_types_filter_empty_result() {\n        let driver = DefaultDriver;\n        let config = DriverConfig {\n            mode: OutputMode::Xml,\n            include_types: Some(vec![BlockType::Diff]),\n            ..DriverConfig::default()\n        };\n        let blocks = vec![code_block(Lang::Rust, \"main.rs\", b\"fn main() {}\")];\n        let result = driver.render(&blocks, &config);\n        assert!(matches!(result, Err(DriverError::EmptyInput)));\n    }\n\n    #[test]\n    fn xml_mode_wraps_in_context() {\n        let driver = DefaultDriver;\n        let config = DriverConfig {\n            mode: OutputMode::Xml,\n            ..DriverConfig::default()\n        };\n        let blocks = vec![code_block(Lang::Rust, \"main.rs\", b\"fn main() {}\")];\n        let result = driver.render(&blocks, &config).unwrap();\n        assert!(result.starts_with(\"<context>\"));\n        assert!(result.ends_with(\"</context>\"));\n    }\n\n    #[test]\n    fn markdown_mode_no_context_wrapper() {\n        let driver = DefaultDriver;\n        let config = DriverConfig {\n            mode: OutputMode::Markdown,\n            ..DriverConfig::default()\n        };\n        let blocks = vec![code_block(Lang::Rust, \"main.rs\", b\"fn main() {}\")];\n        let result = driver.render(&blocks, &config).unwrap();\n        assert!(!result.contains(\"<context>\"));\n        assert!(result.contains(\"## main.rs\"));\n    }\n\n    #[test]\n    fn minimal_mode_uses_dashes() {\n        let driver = DefaultDriver;\n        let config = DriverConfig {\n            mode: OutputMode::Minimal,\n            ..DriverConfig::default()\n        };\n        let blocks = vec![code_block(Lang::Rust, \"main.rs\", b\"fn main() {}\")];\n        let result = driver.render(&blocks, &config).unwrap();\n        assert!(result.contains(\"--- main.rs [rust] ---\"));\n    }\n\n    #[test]\n    fn multiple_blocks_rendered() {\n        let driver = DefaultDriver;\n        let config = DriverConfig {\n            mode: OutputMode::Xml,\n            ..DriverConfig::default()\n        };\n        let blocks = vec![\n            code_block(Lang::Rust, \"src/main.rs\", b\"fn main() {}\"),\n            conversation_block(Role::User, b\"Fix the bug.\"),\n            conversation_block(Role::Assistant, b\"Looking into it.\"),\n        ];\n        let result = driver.render(&blocks, &config).unwrap();\n        assert!(result.contains(\"<code lang=\\\"rust\\\"\"));\n        assert!(result.contains(\"<turn role=\\\"user\\\">Fix the bug.</turn>\"));\n        assert!(result.contains(\"<turn role=\\\"assistant\\\">Looking into it.</turn>\"));\n    }\n\n    #[test]\n    fn file_tree_rendering_xml() {\n        let driver = DefaultDriver;\n        let config = DriverConfig {\n            mode: OutputMode::Xml,\n            ..DriverConfig::default()\n        };\n        let blocks = vec![Block {\n            block_type: BlockType::FileTree,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::FileTree(FileTreeBlock {\n                root_path: \"src/\".to_string(),\n                entries: vec![\n                    FileEntry {\n                        name: \"main.rs\".to_string(),\n                        kind: FileEntryKind::File,\n                        size: 1024,\n                        children: vec![],\n                    },\n                    FileEntry {\n                        name: \"utils\".to_string(),\n                        kind: FileEntryKind::Directory,\n                        size: 0,\n                        children: vec![FileEntry {\n                            name: \"helpers.rs\".to_string(),\n                            kind: FileEntryKind::File,\n                            size: 256,\n                            children: vec![],\n                        }],\n                    },\n                ],\n            }),\n        }];\n        let result = driver.render(&blocks, &config).unwrap();\n        assert!(result.contains(\"<tree root=\\\"src/\\\">\"));\n        assert!(result.contains(\"main.rs (1024 bytes)\"));\n        assert!(result.contains(\"utils/\"));\n        assert!(result.contains(\"  helpers.rs (256 bytes)\"));\n    }\n\n    #[test]\n    fn tool_result_rendering_all_modes() {\n        let driver = DefaultDriver;\n        let blocks = vec![Block {\n            block_type: BlockType::ToolResult,\n            flags: BlockFlags::NONE,\n            summary: None,\n            content: BlockContent::ToolResult(ToolResultBlock {\n                tool_name: \"ripgrep\".to_string(),\n                status: Status::Ok,\n                content: b\"3 matches found.\".to_vec(),\n                schema_hint: None,\n            }),\n        }];\n\n        let xml = driver\n            .render(\n                &blocks,\n                &DriverConfig {\n                    mode: OutputMode::Xml,\n                    ..DriverConfig::default()\n                },\n            )\n            .unwrap();\n        assert!(xml.contains(\"<tool name=\\\"ripgrep\\\" status=\\\"ok\\\">\"));\n\n        let md = driver\n            .render(\n                &blocks,\n                &DriverConfig {\n                    mode: OutputMode::Markdown,\n                    ..DriverConfig::default()\n                },\n            )\n            .unwrap();\n        assert!(md.contains(\"### Tool: ripgrep (ok)\"));\n\n        let min = driver\n            .render(\n                &blocks,\n                &DriverConfig {\n                    mode: OutputMode::Minimal,\n                    ..DriverConfig::default()\n                },\n            )\n            .unwrap();\n        assert!(min.contains(\"--- ripgrep [ok] ---\"));\n    }\n\n    #[test]\n    fn summary_replaces_content_with_summary_verbosity() {\n        let driver = DefaultDriver;\n        let blocks = vec![Block {\n            block_type: BlockType::Code,\n            flags: BlockFlags::NONE,\n            summary: Some(Summary {\n                text: \"Entry point with CLI parsing.\".to_string(),\n            }),\n            content: BlockContent::Code(CodeBlock {\n                lang: Lang::Rust,\n                path: \"src/main.rs\".to_string(),\n                content: b\"fn main() { /* very long implementation */ }\".to_vec(),\n                line_range: None,\n            }),\n        }];\n\n        for mode in [OutputMode::Xml, OutputMode::Markdown, OutputMode::Minimal] {\n            let config = DriverConfig {\n                mode,\n                verbosity: Verbosity::Summary,\n                ..DriverConfig::default()\n            };\n            let result = driver.render(&blocks, &config).unwrap();\n            assert!(\n                result.contains(\"Entry point with CLI parsing.\"),\n                \"mode {mode:?} should contain summary\"\n            );\n            assert!(\n                !result.contains(\"very long implementation\"),\n                \"mode {mode:?} should not contain full content\"\n            );\n        }\n    }\n\n    #[test]\n    fn adaptive_without_budget_renders_full_content() {\n        let driver = DefaultDriver;\n        let blocks = vec![Block {\n            block_type: BlockType::Code,\n            flags: BlockFlags::NONE,\n            summary: Some(Summary {\n                text: \"Entry point with CLI parsing.\".to_string(),\n            }),\n            content: BlockContent::Code(CodeBlock {\n                lang: Lang::Rust,\n                path: \"src/main.rs\".to_string(),\n                content: b\"fn main() { /* very long implementation */ }\".to_vec(),\n                line_range: None,\n            }),\n        }];\n\n        // Adaptive without budget → Full rendering (summary ignored)\n        let config = DriverConfig {\n            mode: OutputMode::Xml,\n            ..DriverConfig::default()\n        };\n        let result = driver.render(&blocks, &config).unwrap();\n        assert!(\n            result.contains(\"very long implementation\"),\n            \"Adaptive without budget should render full content\"\n        );\n    }\n\n    #[test]\n    fn end_blocks_filtered_out() {\n        let driver = DefaultDriver;\n        let config = DriverConfig::default();\n        let blocks = vec![\n            code_block(Lang::Rust, \"main.rs\", b\"fn main() {}\"),\n            Block {\n                block_type: BlockType::End,\n                flags: BlockFlags::NONE,\n                summary: None,\n                content: BlockContent::End,\n            },\n        ];\n        let result = driver.render(&blocks, &config).unwrap();\n        assert!(result.contains(\"fn main()\"));\n    }\n}\n",
      "language": "rust",
      "path": "bcp-driver/src/driver.rs",
      "type": "code"
    },
    {
      "content": "use bcp_types::BlockType;\n\n/// Errors that can occur during block rendering.\n///\n/// The driver validates its input before rendering. These errors represent\n/// conditions that prevent a successful render — empty input, unsupported\n/// block types, or invalid content within blocks.\n///\n/// ```text\n/// ┌──────────────────────┬──────────────────────────────────────────────┐\n/// │ Variant              │ Cause                                        │\n/// ├──────────────────────┼──────────────────────────────────────────────┤\n/// │ EmptyInput           │ No blocks provided to render                 │\n/// │ UnsupportedBlockType │ Block type cannot be rendered in this mode   │\n/// │ InvalidContent       │ Block body contains invalid UTF-8            │\n/// └──────────────────────┴──────────────────────────────────────────────┘\n/// ```\n#[derive(Debug, thiserror::Error)]\npub enum DriverError {\n    #[error(\"no blocks to render\")]\n    EmptyInput,\n\n    #[error(\"unsupported block type for rendering: {block_type:?}\")]\n    UnsupportedBlockType { block_type: BlockType },\n\n    #[error(\"invalid UTF-8 in block content at index {block_index}\")]\n    InvalidContent { block_index: usize },\n}\n",
      "language": "rust",
      "path": "bcp-driver/src/error.rs",
      "type": "code"
    },
    {
      "content": "#![warn(clippy::pedantic)]\n\npub mod budget;\npub mod config;\npub mod driver;\npub mod error;\npub mod render_markdown;\npub mod render_minimal;\npub mod render_xml;\n\nmod placeholder;\n\npub use budget::{CodeAwareEstimator, HeuristicEstimator, RenderDecision, TokenEstimator};\npub use config::{DriverConfig, ModelFamily, OutputMode, Verbosity};\npub use driver::{DefaultDriver, BcpDriver};\npub use error::DriverError;\n",
      "language": "rust",
      "path": "bcp-driver/src/lib.rs",
      "type": "code"
    },
    {
      "content": "I've loaded 30 source files from ./crates. Can you analyze the architecture?",
      "role": "user",
      "type": "conversation"
    },
    {
      "content": "I'll review the 30 files. Let me start by understanding the module structure and key types.",
      "role": "assistant",
      "type": "conversation"
    },
    {
      "content": "Found 30 source files in ./crates",
      "status": "ok",
      "tool_name": "ripgrep",
      "type": "tool_result"
    }
  ],
  "description": "Dir scan of ./crates (30 files)"
}