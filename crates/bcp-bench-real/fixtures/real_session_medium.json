{
  "blocks": [
    {
      "content": "/// TLV field serializer for block bodies.\n///\n/// `BlockWriter` accumulates tag-length-value encoded fields into an\n/// internal byte buffer. It mirrors the field encoding convention from\n/// `bcp_types::fields` but wraps it in a stateful builder that tracks\n/// the buffer and provides a clean `finish()` hand-off.\n///\n/// This is an internal implementation detail of the encoder — it is\n/// not part of the public API. Each block type's serialization calls\n/// into `BlockWriter` to produce the body bytes that get framed by\n/// [`BlockFrame`](bcp_wire::block_frame::BlockFrame).\n///\n/// Wire format per field:\n///\n/// ```text\n/// ┌─────────────────┬──────────────────┬────────────────────────┐\n/// │ field_id (varint)│ wire_type (varint)│ payload (varies)      │\n/// ├─────────────────┼──────────────────┼────────────────────────┤\n/// │                 │ 0 (Varint)       │ value (varint)         │\n/// │                 │ 1 (Bytes)        │ length (varint) + data │\n/// │                 │ 2 (Nested)       │ length (varint) + data │\n/// └─────────────────┴──────────────────┴────────────────────────┘\n/// ```\npub struct BlockWriter {\n    buf: Vec<u8>,\n}\n\nimpl BlockWriter {\n    /// Create a new writer with an empty buffer.\n    #[must_use]\n    pub fn new() -> Self {\n        Self { buf: Vec::new() }\n    }\n\n    /// Create a new writer with a pre-allocated buffer capacity.\n    ///\n    /// Use this when you can estimate the final body size to avoid\n    /// intermediate reallocations.\n    #[must_use]\n    pub fn with_capacity(capacity: usize) -> Self {\n        Self {\n            buf: Vec::with_capacity(capacity),\n        }\n    }\n\n    /// Write a varint field (wire type 0).\n    ///\n    /// Encodes: `field_id (varint) | 0 (varint) | value (varint)`\n    pub fn write_varint_field(&mut self, field_id: u64, value: u64) {\n        bcp_types::fields::encode_varint_field(&mut self.buf, field_id, value);\n    }\n\n    /// Write a bytes field (wire type 1).\n    ///\n    /// Encodes: `field_id (varint) | 1 (varint) | length (varint) | data [length]`\n    ///\n    /// Strings are encoded as bytes fields with UTF-8 content — there is\n    /// no distinct string wire type.\n    pub fn write_bytes_field(&mut self, field_id: u64, value: &[u8]) {\n        bcp_types::fields::encode_bytes_field(&mut self.buf, field_id, value);\n    }\n\n    /// Write a nested field (wire type 2).\n    ///\n    /// Encodes: `field_id (varint) | 2 (varint) | length (varint) | nested [length]`\n    ///\n    /// The `nested` bytes are themselves a sequence of TLV-encoded fields,\n    /// pre-serialized by the caller. This enables recursive structures like\n    /// `FileEntry` children and `DiffHunk` sequences.\n    pub fn write_nested_field(&mut self, field_id: u64, nested: &[u8]) {\n        bcp_types::fields::encode_nested_field(&mut self.buf, field_id, nested);\n    }\n\n    /// Consume the writer and return the accumulated bytes.\n    ///\n    /// After calling `finish()`, the writer is consumed. The returned\n    /// `Vec<u8>` is the complete TLV-encoded body ready to be wrapped\n    /// in a `BlockFrame`.\n    #[must_use]\n    pub fn finish(self) -> Vec<u8> {\n        self.buf\n    }\n}\n\nimpl Default for BlockWriter {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn empty_writer_produces_empty_bytes() {\n        let writer = BlockWriter::new();\n        assert!(writer.finish().is_empty());\n    }\n\n    #[test]\n    fn single_varint_field() {\n        let mut writer = BlockWriter::new();\n        writer.write_varint_field(1, 42);\n        let bytes = writer.finish();\n        assert!(!bytes.is_empty());\n\n        // Verify via bcp-types decode path\n        let (header, n) = bcp_types::fields::decode_field_header(&bytes).unwrap();\n        assert_eq!(header.field_id, 1);\n        assert_eq!(header.wire_type, bcp_types::fields::FieldWireType::Varint);\n        let (val, m) = bcp_types::fields::decode_varint_value(&bytes[n..]).unwrap();\n        assert_eq!(val, 42);\n        assert_eq!(n + m, bytes.len());\n    }\n\n    #[test]\n    fn single_bytes_field() {\n        let mut writer = BlockWriter::new();\n        writer.write_bytes_field(2, b\"hello\");\n        let bytes = writer.finish();\n\n        let (header, n) = bcp_types::fields::decode_field_header(&bytes).unwrap();\n        assert_eq!(header.field_id, 2);\n        assert_eq!(header.wire_type, bcp_types::fields::FieldWireType::Bytes);\n        let (data, m) = bcp_types::fields::decode_bytes_value(&bytes[n..]).unwrap();\n        assert_eq!(data, b\"hello\");\n        assert_eq!(n + m, bytes.len());\n    }\n\n    #[test]\n    fn nested_field_roundtrip() {\n        let mut inner = BlockWriter::new();\n        inner.write_varint_field(1, 99);\n        let inner_bytes = inner.finish();\n\n        let mut outer = BlockWriter::new();\n        outer.write_nested_field(3, &inner_bytes);\n        let bytes = outer.finish();\n\n        let (header, n) = bcp_types::fields::decode_field_header(&bytes).unwrap();\n        assert_eq!(header.field_id, 3);\n        assert_eq!(header.wire_type, bcp_types::fields::FieldWireType::Nested);\n        let (nested, m) = bcp_types::fields::decode_bytes_value(&bytes[n..]).unwrap();\n        assert_eq!(n + m, bytes.len());\n\n        // Decode the nested content\n        let (inner_header, k) = bcp_types::fields::decode_field_header(nested).unwrap();\n        assert_eq!(inner_header.field_id, 1);\n        let (val, _) = bcp_types::fields::decode_varint_value(&nested[k..]).unwrap();\n        assert_eq!(val, 99);\n    }\n\n    #[test]\n    fn multiple_fields_sequential() {\n        let mut writer = BlockWriter::new();\n        writer.write_varint_field(1, 7);\n        writer.write_bytes_field(2, b\"world\");\n        writer.write_varint_field(3, 256);\n        let bytes = writer.finish();\n\n        // Should be decodable as 3 sequential fields\n        let mut cursor = 0;\n\n        let (h, n) = bcp_types::fields::decode_field_header(&bytes[cursor..]).unwrap();\n        cursor += n;\n        assert_eq!(h.field_id, 1);\n        let (v, n) = bcp_types::fields::decode_varint_value(&bytes[cursor..]).unwrap();\n        cursor += n;\n        assert_eq!(v, 7);\n\n        let (h, n) = bcp_types::fields::decode_field_header(&bytes[cursor..]).unwrap();\n        cursor += n;\n        assert_eq!(h.field_id, 2);\n        let (data, n) = bcp_types::fields::decode_bytes_value(&bytes[cursor..]).unwrap();\n        cursor += n;\n        assert_eq!(data, b\"world\");\n\n        let (h, n) = bcp_types::fields::decode_field_header(&bytes[cursor..]).unwrap();\n        cursor += n;\n        assert_eq!(h.field_id, 3);\n        let (v, n) = bcp_types::fields::decode_varint_value(&bytes[cursor..]).unwrap();\n        cursor += n;\n        assert_eq!(v, 256);\n\n        assert_eq!(cursor, bytes.len());\n    }\n}\n",
      "language": "rust",
      "path": "block_writer.rs",
      "type": "code"
    },
    {
      "content": "use std::io::Cursor;\n\nuse crate::error::CompressionError;\n\n/// Minimum block body size (in bytes) before per-block compression\n/// is attempted.\n///\n/// Blocks smaller than this threshold are always stored uncompressed\n/// because zstd framing overhead (~13 bytes for the frame header)\n/// outweighs any savings on very small inputs.\n///\n/// Default: 256 bytes.\npub const COMPRESSION_THRESHOLD: usize = 256;\n\n/// Default zstd compression level (1–22 scale).\n///\n/// Level 3 provides a good balance of speed and compression ratio\n/// for typical code/text context blocks (RFC §4.6). Higher levels\n/// yield diminishing returns for the latency cost.\nconst DEFAULT_COMPRESSION_LEVEL: i32 = 3;\n\n/// Compress a byte slice with zstd.\n///\n/// Returns `Some(compressed)` if compression reduced the size, or\n/// `None` if the compressed output is >= the input size. This\n/// ensures compression is never harmful — the caller should store\n/// the block uncompressed when `None` is returned.\n///\n/// Uses the default compression level (3).\n///\n/// # Example\n///\n/// ```rust\n/// use bcp_encoder::compression::compress;\n///\n/// let data = \"fn main() { }\\n\".repeat(100);\n/// match compress(data.as_bytes()) {\n///     Some(compressed) => assert!(compressed.len() < data.len()),\n///     None => { /* data was incompressible */ }\n/// }\n/// ```\npub fn compress(data: &[u8]) -> Option<Vec<u8>> {\n    let compressed = zstd::encode_all(Cursor::new(data), DEFAULT_COMPRESSION_LEVEL).ok()?;\n    if compressed.len() < data.len() {\n        Some(compressed)\n    } else {\n        None\n    }\n}\n\n/// Decompress a zstd-compressed byte slice.\n///\n/// The `max_size` parameter provides an upper bound on the\n/// decompressed output to prevent decompression bombs — if the\n/// decompressed data exceeds this limit, an error is returned\n/// without completing decompression.\n///\n/// # Errors\n///\n/// - [`CompressionError::DecompressFailed`] if zstd cannot decode\n///   the input (invalid frame, truncated data, etc.).\n/// - [`CompressionError::DecompressionBomb`] if the decompressed\n///   size exceeds `max_size`.\npub fn decompress(data: &[u8], max_size: usize) -> Result<Vec<u8>, CompressionError> {\n    let decompressed = zstd::decode_all(Cursor::new(data))\n        .map_err(|e| CompressionError::DecompressFailed(e.to_string()))?;\n    if decompressed.len() > max_size {\n        return Err(CompressionError::DecompressionBomb {\n            actual: decompressed.len(),\n            limit: max_size,\n        });\n    }\n    Ok(decompressed)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn compress_returns_none_for_small_incompressible_data() {\n        let data = b\"abc123\";\n        assert!(compress(data).is_none());\n    }\n\n    #[test]\n    fn compress_reduces_repetitive_data() {\n        let data = \"fn main() { }\\n\".repeat(100);\n        let result = compress(data.as_bytes());\n        assert!(result.is_some());\n        let compressed = result.unwrap();\n        assert!(compressed.len() < data.len());\n    }\n\n    #[test]\n    fn compress_decompress_roundtrip() {\n        let data = \"pub fn hello() -> &'static str { \\\"world\\\" }\\n\".repeat(50);\n        let compressed = compress(data.as_bytes()).expect(\"should compress\");\n        let decompressed = decompress(&compressed, 1024 * 1024).expect(\"should decompress\");\n        assert_eq!(decompressed, data.as_bytes());\n    }\n\n    #[test]\n    fn decompress_rejects_bomb() {\n        let data = \"x\".repeat(10_000);\n        let compressed = compress(data.as_bytes()).expect(\"should compress\");\n        let result = decompress(&compressed, 100);\n        assert!(matches!(\n            result,\n            Err(CompressionError::DecompressionBomb { .. })\n        ));\n    }\n\n    #[test]\n    fn decompress_rejects_invalid_data() {\n        let garbage = b\"this is not zstd data\";\n        let result = decompress(garbage, 1024 * 1024);\n        assert!(matches!(result, Err(CompressionError::DecompressFailed(_))));\n    }\n\n    #[test]\n    fn compression_threshold_is_256() {\n        assert_eq!(COMPRESSION_THRESHOLD, 256);\n    }\n}\n",
      "language": "rust",
      "path": "compression.rs",
      "type": "code"
    },
    {
      "content": "use std::collections::HashMap;\nuse std::sync::RwLock;\n\nuse bcp_types::content_store::ContentStore;\n\n/// In-memory content store backed by a `HashMap`.\n///\n/// Suitable for the PoC and testing. Not persisted across runs.\n/// Uses [`RwLock`] for interior mutability so that the\n/// [`ContentStore`] trait methods (which take `&self`) can mutate\n/// the internal map safely across threads.\n///\n/// # Concurrency\n///\n/// Read operations (`get`, `contains`) acquire a read lock.\n/// Write operations (`put`) acquire a write lock. Multiple\n/// concurrent readers are allowed; writers are exclusive.\n///\n/// # Example\n///\n/// ```rust\n/// use bcp_encoder::MemoryContentStore;\n/// use bcp_types::ContentStore;\n///\n/// let store = MemoryContentStore::new();\n/// let data = b\"fn main() {}\";\n/// let hash = store.put(data);\n/// assert_eq!(store.get(&hash).unwrap(), data);\n/// assert!(store.contains(&hash));\n/// assert_eq!(store.len(), 1);\n/// ```\npub struct MemoryContentStore {\n    store: RwLock<HashMap<[u8; 32], Vec<u8>>>,\n}\n\nimpl MemoryContentStore {\n    /// Create an empty in-memory content store.\n    #[must_use]\n    pub fn new() -> Self {\n        Self {\n            store: RwLock::new(HashMap::new()),\n        }\n    }\n\n    /// Return the number of unique entries in the store.\n    #[must_use]\n    pub fn len(&self) -> usize {\n        self.store\n            .read()\n            .expect(\"content store lock poisoned\")\n            .len()\n    }\n\n    /// Return `true` if the store contains no entries.\n    #[must_use]\n    pub fn is_empty(&self) -> bool {\n        self.len() == 0\n    }\n\n    /// Return the total bytes stored across all entries.\n    ///\n    /// This counts only the content bytes, not the 32-byte hash\n    /// keys or `HashMap` overhead.\n    #[must_use]\n    pub fn total_bytes(&self) -> usize {\n        self.store\n            .read()\n            .expect(\"content store lock poisoned\")\n            .values()\n            .map(Vec::len)\n            .sum()\n    }\n}\n\nimpl Default for MemoryContentStore {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl ContentStore for MemoryContentStore {\n    fn get(&self, hash: &[u8; 32]) -> Option<Vec<u8>> {\n        self.store\n            .read()\n            .expect(\"content store lock poisoned\")\n            .get(hash)\n            .cloned()\n    }\n\n    fn put(&self, content: &[u8]) -> [u8; 32] {\n        let hash: [u8; 32] = blake3::hash(content).into();\n        let mut store = self.store.write().expect(\"content store lock poisoned\");\n        store.entry(hash).or_insert_with(|| content.to_vec());\n        hash\n    }\n\n    fn contains(&self, hash: &[u8; 32]) -> bool {\n        self.store\n            .read()\n            .expect(\"content store lock poisoned\")\n            .contains_key(hash)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn put_get_roundtrip() {\n        let store = MemoryContentStore::new();\n        let data = b\"fn main() { println!(\\\"hello\\\"); }\";\n        let hash = store.put(data);\n        let retrieved = store.get(&hash).expect(\"should find stored content\");\n        assert_eq!(retrieved, data);\n    }\n\n    #[test]\n    fn put_returns_deterministic_hash() {\n        let store = MemoryContentStore::new();\n        let data = b\"deterministic content\";\n        let hash1 = store.put(data);\n        let hash2 = store.put(data);\n        assert_eq!(hash1, hash2);\n    }\n\n    #[test]\n    fn dedup_stores_only_once() {\n        let store = MemoryContentStore::new();\n        let data = b\"duplicate content\";\n        store.put(data);\n        store.put(data);\n        assert_eq!(store.len(), 1);\n    }\n\n    #[test]\n    fn contains_returns_true_for_stored_hash() {\n        let store = MemoryContentStore::new();\n        let data = b\"some content\";\n        let hash = store.put(data);\n        assert!(store.contains(&hash));\n    }\n\n    #[test]\n    fn contains_returns_false_for_unknown_hash() {\n        let store = MemoryContentStore::new();\n        let fake_hash = [0u8; 32];\n        assert!(!store.contains(&fake_hash));\n    }\n\n    #[test]\n    fn get_returns_none_for_unknown_hash() {\n        let store = MemoryContentStore::new();\n        let fake_hash = [0xFF; 32];\n        assert!(store.get(&fake_hash).is_none());\n    }\n\n    #[test]\n    fn len_and_total_bytes() {\n        let store = MemoryContentStore::new();\n        assert_eq!(store.len(), 0);\n        assert!(store.is_empty());\n        assert_eq!(store.total_bytes(), 0);\n\n        store.put(b\"hello\"); // 5 bytes\n        store.put(b\"world\"); // 5 bytes\n        assert_eq!(store.len(), 2);\n        assert!(!store.is_empty());\n        assert_eq!(store.total_bytes(), 10);\n    }\n\n    #[test]\n    fn blake3_hash_is_32_bytes() {\n        let store = MemoryContentStore::new();\n        let hash = store.put(b\"test\");\n        assert_eq!(hash.len(), 32);\n    }\n\n    #[test]\n    fn different_content_produces_different_hashes() {\n        let store = MemoryContentStore::new();\n        let hash1 = store.put(b\"content A\");\n        let hash2 = store.put(b\"content B\");\n        assert_ne!(hash1, hash2);\n    }\n}\n",
      "language": "rust",
      "path": "content_store.rs",
      "type": "code"
    },
    {
      "content": "use std::sync::Arc;\n\nuse bcp_types::BlockContent;\nuse bcp_types::annotation::AnnotationBlock;\nuse bcp_types::code::CodeBlock;\nuse bcp_types::content_store::ContentStore;\nuse bcp_types::conversation::ConversationBlock;\nuse bcp_types::diff::{DiffBlock, DiffHunk};\nuse bcp_types::embedding_ref::EmbeddingRefBlock;\nuse bcp_types::document::DocumentBlock;\nuse bcp_types::enums::{\n    AnnotationKind, DataFormat, FormatHint, Lang, MediaType, Priority, Role, Status,\n};\nuse bcp_types::extension::ExtensionBlock;\nuse bcp_types::file_tree::{FileEntry, FileTreeBlock};\nuse bcp_types::image::ImageBlock;\nuse bcp_types::structured_data::StructuredDataBlock;\nuse bcp_types::summary::Summary;\nuse bcp_types::tool_result::ToolResultBlock;\nuse bcp_wire::block_frame::{BlockFlags, BlockFrame, block_type};\nuse bcp_wire::header::{HEADER_SIZE, HeaderFlags, BcpHeader};\n\nuse crate::compression::{self, COMPRESSION_THRESHOLD};\nuse crate::error::EncodeError;\n\n/// Maximum block body size (16 MiB). Blocks exceeding this limit produce\n/// an [`EncodeError::BlockTooLarge`] during `.encode()`.\nconst MAX_BLOCK_BODY_SIZE: usize = 16 * 1024 * 1024;\n\n/// BCP encoder — constructs a binary payload from structured blocks.\n///\n/// The encoder is the tool-facing API that allows agents, MCP servers,\n/// and other producers to build BCP payloads. It follows the builder\n/// pattern defined in RFC §5.6: methods like [`add_code`](Self::add_code),\n/// [`add_conversation`](Self::add_conversation), etc. append typed blocks\n/// to an internal list, and chainable modifiers like\n/// [`with_summary`](Self::with_summary) and\n/// [`with_priority`](Self::with_priority) annotate the most recently\n/// added block.\n///\n/// # Compression (RFC §4.6)\n///\n/// Two compression modes are supported, both opt-in:\n///\n/// - **Per-block**: call [`with_compression`](Self::with_compression) after\n///   adding a block, or [`compress_blocks`](Self::compress_blocks) to\n///   enable compression for all subsequent blocks. Each block body is\n///   independently zstd-compressed if it exceeds\n///   [`COMPRESSION_THRESHOLD`](crate::compression::COMPRESSION_THRESHOLD)\n///   bytes and compression yields a size reduction. The block's\n///   `COMPRESSED` flag (bit 1) is set when compression is applied.\n///\n/// - **Whole-payload**: call [`compress_payload`](Self::compress_payload)\n///   to zstd-compress all bytes after the 8-byte header. When enabled,\n///   per-block compression is skipped (whole-payload subsumes it). The\n///   header's `COMPRESSED` flag (bit 0) is set.\n///\n/// # Content Addressing (RFC §4.7)\n///\n/// When a [`ContentStore`] is configured via\n/// [`set_content_store`](Self::set_content_store), blocks can be stored\n/// by their BLAKE3 hash rather than inline:\n///\n/// - **Per-block**: call [`with_content_addressing`](Self::with_content_addressing)\n///   after adding a block. The body is hashed, stored in the content store,\n///   and replaced with the 32-byte hash on the wire. The block's\n///   `IS_REFERENCE` flag (bit 2) is set.\n///\n/// - **Auto-dedup**: call [`auto_dedup`](Self::auto_dedup) to automatically\n///   content-address any block whose body has been seen before. First\n///   occurrence is stored inline and registered in the store; subsequent\n///   identical blocks become references.\n///\n/// Content addressing runs before compression — a 32-byte hash reference\n/// is always below the compression threshold, so reference blocks are\n/// never compressed.\n///\n/// # Usage\n///\n/// ```rust\n/// use bcp_encoder::BcpEncoder;\n/// use bcp_types::enums::{Lang, Role, Status, Priority};\n///\n/// # fn main() -> Result<(), Box<dyn std::error::Error>> {\n/// let payload = BcpEncoder::new()\n///     .add_code(Lang::Rust, \"src/main.rs\", b\"fn main() {}\")\n///     .with_summary(\"Entry point: CLI setup and server startup.\")?\n///     .with_priority(Priority::High)?\n///     .add_conversation(Role::User, b\"Fix the timeout bug.\")\n///     .add_conversation(Role::Assistant, b\"I'll examine the pool config...\")\n///     .add_tool_result(\"ripgrep\", Status::Ok, b\"3 matches found.\")\n///     .encode()?;\n/// # Ok(())\n/// # }\n/// ```\n///\n/// # Output layout\n///\n/// The `.encode()` method serializes all accumulated blocks into a\n/// self-contained byte sequence:\n///\n/// ```text\n/// ┌──────────────┬──────────────────────────────────────────┐\n/// │ [8 bytes]    │ File header (magic, version, flags, rsv) │\n/// │ [N bytes]    │ Block 0 frame (type + flags + len + body)│\n/// │ [N bytes]    │ Block 1 frame ...                        │\n/// │ ...          │                                          │\n/// │ [2-3 bytes]  │ END sentinel (type=0xFF, flags=0, len=0) │\n/// └──────────────┴──────────────────────────────────────────┘\n/// ```\n///\n/// When whole-payload compression is enabled, the layout becomes:\n///\n/// ```text\n/// ┌──────────────┬──────────────────────────────────────────┐\n/// │ [8 bytes]    │ Header (flags bit 0 = COMPRESSED)        │\n/// │ [N bytes]    │ zstd(Block 0 + Block 1 + ... + END)      │\n/// └──────────────┴──────────────────────────────────────────┘\n/// ```\n///\n/// The payload is ready for storage or transmission — no further\n/// framing is required.\npub struct BcpEncoder {\n    blocks: Vec<PendingBlock>,\n    flags: HeaderFlags,\n    /// When `true`, the entire payload after the header is zstd-compressed.\n    compress_payload: bool,\n    /// When `true`, all blocks are individually compressed (unless\n    /// `compress_payload` is also set, which takes precedence).\n    compress_all_blocks: bool,\n    /// Content store for BLAKE3 content-addressed deduplication.\n    /// Required when any block has `content_address = true` or\n    /// when `auto_dedup` is enabled.\n    content_store: Option<Arc<dyn ContentStore>>,\n    /// When `true`, automatically content-address any block whose body\n    /// has been seen before (hash already exists in the store).\n    auto_dedup: bool,\n}\n\n/// Internal representation of a block awaiting serialization.\n///\n/// Captures the block type tag, the typed content (which knows how to\n/// serialize its own TLV body via [`BlockContent::encode_body`]), an\n/// optional summary to prepend, and per-block compression / content\n/// addressing flags.\n///\n/// `PendingBlock` is never exposed publicly. The encoder builds these\n/// internally as the caller chains `.add_*()` and `.with_*()` methods,\n/// then consumes them during `.encode()`.\nstruct PendingBlock {\n    block_type: u8,\n    content: BlockContent,\n    summary: Option<String>,\n    /// When `true`, this block's body should be zstd-compressed if it\n    /// exceeds [`COMPRESSION_THRESHOLD`] and compression yields savings.\n    compress: bool,\n    /// When `true`, this block's body should be replaced with its\n    /// 32-byte BLAKE3 hash and stored in the content store.\n    content_address: bool,\n}\n\nimpl BcpEncoder {\n    /// Create a new encoder with default settings (version 1.0, no flags).\n    ///\n    /// The encoder starts with an empty block list, no compression, and\n    /// no content store. At least one block must be added before calling\n    /// `.encode()`, otherwise it returns [`EncodeError::EmptyPayload`].\n    #[must_use]\n    pub fn new() -> Self {\n        Self {\n            blocks: Vec::new(),\n            flags: HeaderFlags::NONE,\n            compress_payload: false,\n            compress_all_blocks: false,\n            content_store: None,\n            auto_dedup: false,\n        }\n    }\n\n    // ── Block addition methods ──────────────────────────────────────────\n    //\n    // Each method constructs the appropriate `BlockContent` variant from\n    // `bcp-types`, wraps it in a `PendingBlock`, pushes it onto the\n    // internal list, and returns `&mut Self` for chaining.\n\n    /// Add a CODE block.\n    ///\n    /// Encodes a source code file or fragment. The `lang` enum identifies\n    /// the programming language (used by the decoder for syntax-aware\n    /// rendering), `path` is the file path (UTF-8), and `content` is the\n    /// raw source bytes.\n    ///\n    /// For partial files, use [`add_code_range`](Self::add_code_range)\n    /// to include line range metadata.\n    pub fn add_code(&mut self, lang: Lang, path: &str, content: &[u8]) -> &mut Self {\n        self.push_block(\n            block_type::CODE,\n            BlockContent::Code(CodeBlock {\n                lang,\n                path: path.to_string(),\n                content: content.to_vec(),\n                line_range: None,\n            }),\n        )\n    }\n\n    /// Add a CODE block with a line range.\n    ///\n    /// Same as [`add_code`](Self::add_code) but includes `line_start` and\n    /// `line_end` metadata (1-based, inclusive). The decoder can use this\n    /// to display line numbers or to correlate with diagnostics.\n    pub fn add_code_range(\n        &mut self,\n        lang: Lang,\n        path: &str,\n        content: &[u8],\n        line_start: u32,\n        line_end: u32,\n    ) -> &mut Self {\n        self.push_block(\n            block_type::CODE,\n            BlockContent::Code(CodeBlock {\n                lang,\n                path: path.to_string(),\n                content: content.to_vec(),\n                line_range: Some((line_start, line_end)),\n            }),\n        )\n    }\n\n    /// Add a CONVERSATION block.\n    ///\n    /// Represents a single chat turn. The `role` identifies the speaker\n    /// (system, user, assistant, or tool) and `content` is the message\n    /// body as raw bytes.\n    pub fn add_conversation(&mut self, role: Role, content: &[u8]) -> &mut Self {\n        self.push_block(\n            block_type::CONVERSATION,\n            BlockContent::Conversation(ConversationBlock {\n                role,\n                content: content.to_vec(),\n                tool_call_id: None,\n            }),\n        )\n    }\n\n    /// Add a CONVERSATION block with a tool call ID.\n    ///\n    /// Used for tool-role messages that reference a specific tool\n    /// invocation. The `tool_call_id` links this response back to the\n    /// tool call that produced it.\n    pub fn add_conversation_tool(\n        &mut self,\n        role: Role,\n        content: &[u8],\n        tool_call_id: &str,\n    ) -> &mut Self {\n        self.push_block(\n            block_type::CONVERSATION,\n            BlockContent::Conversation(ConversationBlock {\n                role,\n                content: content.to_vec(),\n                tool_call_id: Some(tool_call_id.to_string()),\n            }),\n        )\n    }\n\n    /// Add a `FILE_TREE` block.\n    ///\n    /// Represents a directory structure rooted at `root`. Each entry\n    /// contains a name, kind (file or directory), size, and optional\n    /// nested children for recursive directory trees.\n    pub fn add_file_tree(&mut self, root: &str, entries: Vec<FileEntry>) -> &mut Self {\n        self.push_block(\n            block_type::FILE_TREE,\n            BlockContent::FileTree(FileTreeBlock {\n                root_path: root.to_string(),\n                entries,\n            }),\n        )\n    }\n\n    /// Add a `TOOL_RESULT` block.\n    ///\n    /// Captures the output of an external tool invocation (e.g. ripgrep,\n    /// LSP diagnostics, test runner). The `status` indicates whether the\n    /// tool succeeded, failed, or timed out.\n    pub fn add_tool_result(&mut self, name: &str, status: Status, content: &[u8]) -> &mut Self {\n        self.push_block(\n            block_type::TOOL_RESULT,\n            BlockContent::ToolResult(ToolResultBlock {\n                tool_name: name.to_string(),\n                status,\n                content: content.to_vec(),\n                schema_hint: None,\n            }),\n        )\n    }\n\n    /// Add a DOCUMENT block.\n    ///\n    /// Represents prose content — README files, documentation, wiki pages.\n    /// The `format_hint` tells the decoder how to render the body\n    /// (markdown, plain text, or HTML).\n    pub fn add_document(\n        &mut self,\n        title: &str,\n        content: &[u8],\n        format_hint: FormatHint,\n    ) -> &mut Self {\n        self.push_block(\n            block_type::DOCUMENT,\n            BlockContent::Document(DocumentBlock {\n                title: title.to_string(),\n                content: content.to_vec(),\n                format_hint,\n            }),\n        )\n    }\n\n    /// Add a `STRUCTURED_DATA` block.\n    ///\n    /// Encodes tabular or structured content — JSON configs, YAML\n    /// manifests, TOML files, CSV data. The `format` identifies the\n    /// serialization format so the decoder can syntax-highlight or\n    /// parse appropriately.\n    pub fn add_structured_data(&mut self, format: DataFormat, content: &[u8]) -> &mut Self {\n        self.push_block(\n            block_type::STRUCTURED_DATA,\n            BlockContent::StructuredData(StructuredDataBlock {\n                format,\n                content: content.to_vec(),\n                schema: None,\n            }),\n        )\n    }\n\n    /// Add a DIFF block.\n    ///\n    /// Represents code changes for a single file — from git diffs, editor\n    /// changes, or patch files. Each hunk captures a contiguous range of\n    /// modifications in unified diff format.\n    pub fn add_diff(&mut self, path: &str, hunks: Vec<DiffHunk>) -> &mut Self {\n        self.push_block(\n            block_type::DIFF,\n            BlockContent::Diff(DiffBlock {\n                path: path.to_string(),\n                hunks,\n            }),\n        )\n    }\n\n    /// Add an ANNOTATION block.\n    ///\n    /// Annotations are metadata overlays that target another block by its\n    /// zero-based index in the stream. The `kind` determines how the\n    /// `value` payload is interpreted (priority level, summary text, or\n    /// tag label).\n    ///\n    /// For the common case of attaching a priority to the most recent\n    /// block, prefer [`with_priority`](Self::with_priority).\n    pub fn add_annotation(\n        &mut self,\n        target_block_id: u32,\n        kind: AnnotationKind,\n        value: &[u8],\n    ) -> &mut Self {\n        self.push_block(\n            block_type::ANNOTATION,\n            BlockContent::Annotation(AnnotationBlock {\n                target_block_id,\n                kind,\n                value: value.to_vec(),\n            }),\n        )\n    }\n\n    /// Add an EMBEDDING_REF block.\n    ///\n    /// Points to a pre-computed vector embedding stored externally (e.g.\n    /// in a vector database). The `vector_id` is an opaque byte identifier\n    /// for the vector in the external store, `source_hash` is the BLAKE3\n    /// hash of the content that was embedded (32 bytes), and `model` is\n    /// the name of the embedding model (e.g. `\"text-embedding-3-small\"`).\n    ///\n    /// # Wire type\n    ///\n    /// Block type `0x09` (`EMBEDDING_REF`). See RFC §4.4.\n    pub fn add_embedding_ref(\n        &mut self,\n        vector_id: &[u8],\n        source_hash: &[u8],\n        model: &str,\n    ) -> &mut Self {\n        self.push_block(\n            block_type::EMBEDDING_REF,\n            BlockContent::EmbeddingRef(EmbeddingRefBlock {\n                vector_id: vector_id.to_vec(),\n                source_hash: source_hash.to_vec(),\n                model: model.to_string(),\n            }),\n        )\n    }\n\n    /// Add an IMAGE block.\n    ///\n    /// Encodes an image as inline binary data. The `media_type` identifies\n    /// the image format (PNG, JPEG, etc.), `alt_text` provides a textual\n    /// description for accessibility, and `data` is the raw image bytes.\n    pub fn add_image(&mut self, media_type: MediaType, alt_text: &str, data: &[u8]) -> &mut Self {\n        self.push_block(\n            block_type::IMAGE,\n            BlockContent::Image(ImageBlock {\n                media_type,\n                alt_text: alt_text.to_string(),\n                data: data.to_vec(),\n            }),\n        )\n    }\n\n    /// Add an EXTENSION block.\n    ///\n    /// User-defined block type for custom payloads. The `namespace` and\n    /// `type_name` together form a unique identifier for the extension\n    /// type, preventing collisions across different tools and vendors.\n    pub fn add_extension(&mut self, namespace: &str, type_name: &str, content: &[u8]) -> &mut Self {\n        self.push_block(\n            block_type::EXTENSION,\n            BlockContent::Extension(ExtensionBlock {\n                namespace: namespace.to_string(),\n                type_name: type_name.to_string(),\n                content: content.to_vec(),\n            }),\n        )\n    }\n\n    // ── Modifier methods ────────────────────────────────────────────────\n    //\n    // Modifiers act on the most recently added block. They set metadata\n    // that affects how the block is serialized (summary prefix, flags)\n    // or append related blocks (priority annotation).\n\n    /// Attach a summary to the most recently added block.\n    ///\n    /// Sets the `HAS_SUMMARY` flag on the block and prepends the summary\n    /// sub-block to the body during serialization. The summary is a\n    /// compact UTF-8 description that the token budget engine can use as\n    /// a stand-in when the full block content would exceed the budget.\n    ///\n    /// # Errors\n    ///\n    /// Returns [`EncodeError::NoBlockTarget`] if no blocks have been\n    /// added yet. Use this immediately after an `.add_*()` call.\n    pub fn with_summary(&mut self, summary: &str) -> Result<&mut Self, EncodeError> {\n        let block = self\n            .blocks\n            .last_mut()\n            .ok_or(EncodeError::NoBlockTarget { method: \"with_summary\" })?;\n        block.summary = Some(summary.to_string());\n        Ok(self)\n    }\n\n    /// Attach a priority annotation to the most recently added block.\n    ///\n    /// This is a convenience method that appends an ANNOTATION block\n    /// with `kind=Priority` targeting the last added block's index.\n    /// The annotation's value is the priority byte (e.g. `0x02` for\n    /// `Priority::High`).\n    ///\n    /// # Errors\n    ///\n    /// Returns [`EncodeError::NoBlockTarget`] if no blocks have been\n    /// added yet.\n    pub fn with_priority(&mut self, priority: Priority) -> Result<&mut Self, EncodeError> {\n        let target_index = self\n            .blocks\n            .len()\n            .checked_sub(1)\n            .ok_or(EncodeError::NoBlockTarget { method: \"with_priority\" })?;\n\n        #[allow(clippy::cast_possible_truncation)]\n        let target_id = target_index as u32;\n\n        self.push_block(\n            block_type::ANNOTATION,\n            BlockContent::Annotation(AnnotationBlock {\n                target_block_id: target_id,\n                kind: AnnotationKind::Priority,\n                value: vec![priority.to_wire_byte()],\n            }),\n        );\n        Ok(self)\n    }\n\n    // ── Compression modifiers ────────────────────────────────────────────\n    //\n    // These methods control per-block and whole-payload zstd compression.\n    // Per-block compression is skipped when whole-payload compression is\n    // enabled — the outer zstd frame subsumes individual block compression.\n\n    /// Enable zstd compression for the most recently added block.\n    ///\n    /// During `.encode()`, the block body is compressed with zstd if it\n    /// exceeds [`COMPRESSION_THRESHOLD`] bytes and compression yields a\n    /// size reduction. If compression doesn't help (output >= input), the\n    /// body is stored uncompressed and the `COMPRESSED` flag is not set.\n    ///\n    /// Has no effect if [`compress_payload`](Self::compress_payload) is\n    /// also enabled — whole-payload compression takes precedence.\n    ///\n    /// # Errors\n    ///\n    /// Returns [`EncodeError::NoBlockTarget`] if no blocks have been\n    /// added yet.\n    pub fn with_compression(&mut self) -> Result<&mut Self, EncodeError> {\n        let block = self\n            .blocks\n            .last_mut()\n            .ok_or(EncodeError::NoBlockTarget { method: \"with_compression\" })?;\n        block.compress = true;\n        Ok(self)\n    }\n\n    /// Enable zstd compression for all blocks added so far and all\n    /// future blocks.\n    ///\n    /// Equivalent to calling [`with_compression`](Self::with_compression)\n    /// on every block. Individual blocks still respect the size threshold\n    /// and no-savings guard.\n    pub fn compress_blocks(&mut self) -> &mut Self {\n        self.compress_all_blocks = true;\n        for block in &mut self.blocks {\n            block.compress = true;\n        }\n        self\n    }\n\n    /// Enable whole-payload zstd compression.\n    ///\n    /// When set, the entire block stream (all frames + END sentinel) is\n    /// compressed as a single zstd frame. The 8-byte header is written\n    /// uncompressed with `HeaderFlags::COMPRESSED` set so the decoder\n    /// can detect compression before reading further.\n    ///\n    /// When whole-payload compression is enabled, per-block compression\n    /// is skipped — compressing within a compressed stream adds overhead\n    /// without benefit.\n    ///\n    /// If compression doesn't reduce the total size, the payload is\n    /// stored uncompressed and the header flag is not set.\n    ///\n    /// **Tradeoff**: Whole-payload compression disables incremental\n    /// streaming in `StreamingDecoder` — the decoder must buffer and\n    /// decompress the entire payload before yielding any blocks. If\n    /// streaming is important, use [`compress_blocks`](Self::compress_blocks)\n    /// instead.\n    pub fn compress_payload(&mut self) -> &mut Self {\n        self.compress_payload = true;\n        self\n    }\n\n    // ── Content addressing modifiers ────────────────────────────────────\n    //\n    // These methods control BLAKE3 content-addressed deduplication.\n    // A content store must be configured before blocks can be\n    // content-addressed.\n\n    /// Set the content store used for BLAKE3 content addressing.\n    ///\n    /// The store is shared via `Arc` so the same store can be passed to\n    /// both the encoder and decoder for roundtrip workflows. The encoder\n    /// calls `store.put()` for each content-addressed block; the decoder\n    /// calls `store.get()` to resolve references.\n    ///\n    /// Must be called before `.encode()` if any block has content\n    /// addressing enabled or if [`auto_dedup`](Self::auto_dedup) is set.\n    pub fn set_content_store(&mut self, store: Arc<dyn ContentStore>) -> &mut Self {\n        self.content_store = Some(store);\n        self\n    }\n\n    /// Enable content addressing for the most recently added block.\n    ///\n    /// During `.encode()`, the block body is hashed with BLAKE3,\n    /// stored in the content store, and replaced with the 32-byte hash\n    /// on the wire. The block's `IS_REFERENCE` flag (bit 2) is set.\n    ///\n    /// Requires a content store — call\n    /// [`set_content_store`](Self::set_content_store) before `.encode()`.\n    ///\n    /// Content addressing runs before compression. Since a 32-byte\n    /// hash reference is always below [`COMPRESSION_THRESHOLD`],\n    /// reference blocks are never per-block compressed.\n    ///\n    /// # Errors\n    ///\n    /// Returns [`EncodeError::NoBlockTarget`] if no blocks have been\n    /// added yet.\n    pub fn with_content_addressing(&mut self) -> Result<&mut Self, EncodeError> {\n        let block = self\n            .blocks\n            .last_mut()\n            .ok_or(EncodeError::NoBlockTarget { method: \"with_content_addressing\" })?;\n        block.content_address = true;\n        Ok(self)\n    }\n\n    /// Enable automatic deduplication across all blocks.\n    ///\n    /// When set, the encoder hashes every block body with BLAKE3 during\n    /// `.encode()`. If the hash already exists in the content store\n    /// (i.e. a previous block in this or a prior encoding had the same\n    /// content), the block is automatically replaced with a hash\n    /// reference. First-occurrence blocks are stored inline and\n    /// registered in the store for future dedup.\n    ///\n    /// Requires a content store — call\n    /// [`set_content_store`](Self::set_content_store) before `.encode()`.\n    pub fn auto_dedup(&mut self) -> &mut Self {\n        self.auto_dedup = true;\n        self\n    }\n\n    // ── Encode ──────────────────────────────────────────────────────────\n\n    /// Serialize all accumulated blocks into a complete BCP payload.\n    ///\n    /// The encode pipeline processes each `PendingBlock` through up to\n    /// three stages:\n    ///\n    ///   1. **Serialize** — calls [`BlockContent::encode_body`] to get\n    ///      the TLV-encoded body bytes. If a summary is present, it is\n    ///      prepended and the `HAS_SUMMARY` flag is set.\n    ///\n    ///   2. **Content address** (optional) — if the block has\n    ///      `content_address = true` or auto-dedup detects a duplicate,\n    ///      the body is hashed with BLAKE3, stored in the content store,\n    ///      and replaced with the 32-byte hash. The `IS_REFERENCE` flag\n    ///      (bit 2) is set.\n    ///\n    ///   3. **Per-block compress** (optional) — if compression is enabled\n    ///      for this block, whole-payload compression is NOT active, and\n    ///      the body is not a reference, the body is zstd-compressed if\n    ///      it exceeds [`COMPRESSION_THRESHOLD`] and compression yields\n    ///      savings. The `COMPRESSED` flag (bit 1) is set.\n    ///\n    /// After all blocks, the END sentinel is appended. If whole-payload\n    /// compression is enabled, everything after the 8-byte header is\n    /// compressed as a single zstd frame and the header's `COMPRESSED`\n    /// flag is set.\n    ///\n    /// # Errors\n    ///\n    /// - [`EncodeError::EmptyPayload`] if no blocks have been added.\n    /// - [`EncodeError::BlockTooLarge`] if any block body exceeds 16 MiB.\n    /// - [`EncodeError::MissingContentStore`] if content addressing is\n    ///   requested but no store has been configured.\n    /// - [`EncodeError::Wire`] if the underlying wire serialization fails.\n    /// - [`EncodeError::Io`] if writing to the output buffer fails.\n    pub fn encode(&self) -> Result<Vec<u8>, EncodeError> {\n        if self.blocks.is_empty() {\n            return Err(EncodeError::EmptyPayload);\n        }\n\n        // Validate: if any block needs content addressing or auto_dedup\n        // is enabled, a store must be present.\n        let needs_store = self.auto_dedup || self.blocks.iter().any(|b| b.content_address);\n        if needs_store && self.content_store.is_none() {\n            return Err(EncodeError::MissingContentStore);\n        }\n\n        // Pre-allocate: 8 bytes header + estimated block data + END sentinel.\n        let estimated_size = HEADER_SIZE + self.blocks.len() * 256 + 3;\n        let mut output = Vec::with_capacity(estimated_size);\n\n        // 1. Write a placeholder header (flags may be updated for whole-payload).\n        output.resize(HEADER_SIZE, 0);\n\n        // 2. Serialize each pending block through the encode pipeline.\n        for pending in &self.blocks {\n            let mut body = Self::serialize_block_body(pending)?;\n            let mut flags_raw = 0u8;\n\n            if pending.summary.is_some() {\n                flags_raw |= BlockFlags::HAS_SUMMARY.raw();\n            }\n\n            // Stage 2: Content addressing (runs before compression).\n            let is_reference = self.apply_content_addressing(pending, &mut body)?;\n            if is_reference {\n                flags_raw |= BlockFlags::IS_REFERENCE.raw();\n            }\n\n            // Stage 3: Per-block compression (skipped for references and\n            // when whole-payload compression is active).\n            if !is_reference && !self.compress_payload {\n                let should_compress = pending.compress || self.compress_all_blocks;\n                if should_compress && body.len() >= COMPRESSION_THRESHOLD {\n                    if let Some(compressed) = compression::compress(&body) {\n                        body = compressed;\n                        flags_raw |= BlockFlags::COMPRESSED.raw();\n                    }\n                }\n            }\n\n            let frame = BlockFrame {\n                block_type: pending.block_type,\n                flags: BlockFlags::from_raw(flags_raw),\n                body,\n            };\n            frame.write_to(&mut output)?;\n        }\n\n        // 3. Write the END sentinel.\n        let end_frame = BlockFrame {\n            block_type: block_type::END,\n            flags: BlockFlags::NONE,\n            body: Vec::new(),\n        };\n        end_frame.write_to(&mut output)?;\n\n        // 4. Whole-payload compression: compress everything after the header.\n        let header_flags = if self.compress_payload {\n            let block_data = &output[HEADER_SIZE..];\n            match compression::compress(block_data) {\n                Some(compressed) => {\n                    output.truncate(HEADER_SIZE);\n                    output.extend_from_slice(&compressed);\n                    HeaderFlags::from_raw(self.flags.raw() | HeaderFlags::COMPRESSED.raw())\n                }\n                None => self.flags,\n            }\n        } else {\n            self.flags\n        };\n\n        // 5. Write the final header with correct flags.\n        let header = BcpHeader::new(header_flags);\n        header.write_to(&mut output[..HEADER_SIZE])?;\n\n        Ok(output)\n    }\n\n    // ── Internal helpers ────────────────────────────────────────────────\n\n    /// Push a new `PendingBlock` onto the internal list.\n    ///\n    /// If `compress_all_blocks` is set, the new block inherits\n    /// `compress = true` automatically.\n    ///\n    /// Returns `&mut Self` so callers can chain additional methods.\n    fn push_block(&mut self, block_type: u8, content: BlockContent) -> &mut Self {\n        self.blocks.push(PendingBlock {\n            block_type,\n            content,\n            summary: None,\n            compress: self.compress_all_blocks,\n            content_address: false,\n        });\n        self\n    }\n\n    /// Apply content addressing to a block body if requested.\n    ///\n    /// Returns `true` if the body was replaced with a 32-byte hash\n    /// reference, `false` if the body is unchanged (inline).\n    ///\n    /// Two paths trigger content addressing:\n    /// 1. `pending.content_address == true` — always replace with hash.\n    /// 2. `self.auto_dedup == true` — replace only if the hash already\n    ///    exists in the store (i.e. a duplicate). First occurrence is\n    ///    stored inline and registered for future dedup.\n    fn apply_content_addressing(\n        &self,\n        pending: &PendingBlock,\n        body: &mut Vec<u8>,\n    ) -> Result<bool, EncodeError> {\n        let store = match &self.content_store {\n            Some(s) => s,\n            None => return Ok(false),\n        };\n\n        if pending.content_address {\n            // Explicit content addressing: always replace with hash.\n            let hash = store.put(body);\n            *body = hash.to_vec();\n            return Ok(true);\n        }\n\n        if self.auto_dedup {\n            // Auto-dedup: check if this body was seen before.\n            let hash: [u8; 32] = blake3::hash(body).into();\n            if store.contains(&hash) {\n                // Duplicate — replace with reference.\n                *body = hash.to_vec();\n                return Ok(true);\n            }\n            // First occurrence — store for future dedup, keep inline.\n            store.put(body);\n        }\n\n        Ok(false)\n    }\n\n    /// Serialize a `PendingBlock` into its final body bytes.\n    ///\n    /// If the block has a summary, the summary is encoded first (as a\n    /// length-prefixed UTF-8 string) followed by the TLV body fields.\n    /// This matches the wire convention: when `HAS_SUMMARY` is set, the\n    /// summary occupies the front of the body, before any TLV fields.\n    fn serialize_block_body(pending: &PendingBlock) -> Result<Vec<u8>, EncodeError> {\n        let tlv_body = pending.content.encode_body();\n        let mut body = Vec::new();\n\n        if let Some(ref summary_text) = pending.summary {\n            let summary = Summary {\n                text: summary_text.clone(),\n            };\n            summary.encode(&mut body);\n        }\n\n        body.extend_from_slice(&tlv_body);\n\n        if body.len() > MAX_BLOCK_BODY_SIZE {\n            return Err(EncodeError::BlockTooLarge {\n                size: body.len(),\n                limit: MAX_BLOCK_BODY_SIZE,\n            });\n        }\n\n        Ok(body)\n    }\n}\n\nimpl Default for BcpEncoder {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use bcp_types::file_tree::FileEntryKind;\n    use bcp_wire::header::BCP_MAGIC;\n\n    // ── Helper ──────────────────────────────────────────────────────────\n\n    /// Verify that a payload starts with the BCP magic number.\n    fn assert_starts_with_magic(payload: &[u8]) {\n        assert!(payload.len() >= HEADER_SIZE, \"payload too short for header\");\n        assert_eq!(&payload[..4], &BCP_MAGIC, \"missing BCP magic\");\n    }\n\n    /// Verify that a payload ends with a valid END sentinel.\n    ///\n    /// The END sentinel is: block_type=0xFF as varint (2 bytes: 0xFF 0x01),\n    /// flags=0x00, content_len=0 as varint (1 byte: 0x00).\n    fn assert_ends_with_end_sentinel(payload: &[u8]) {\n        // The END block type 0xFF encodes as varint [0xFF, 0x01],\n        // followed by flags byte 0x00, followed by content_len varint 0x00.\n        let tail = &payload[payload.len() - 4..];\n        assert_eq!(tail, &[0xFF, 0x01, 0x00, 0x00], \"missing END sentinel\");\n    }\n\n    // ── Acceptance criteria tests ───────────────────────────────────────\n\n    #[test]\n    fn encode_single_code_block_produces_valid_magic() {\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"src/main.rs\", b\"fn main() {}\")\n            .encode()\n            .unwrap();\n\n        assert_starts_with_magic(&payload);\n    }\n\n    #[test]\n    fn builder_methods_are_chainable() {\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"src/lib.rs\", b\"pub fn hello() {}\")\n            .with_summary(\"Hello function.\").unwrap()\n            .add_conversation(Role::User, b\"What does this do?\")\n            .encode()\n            .unwrap();\n\n        assert_starts_with_magic(&payload);\n        assert_ends_with_end_sentinel(&payload);\n    }\n\n    #[test]\n    fn with_summary_sets_has_summary_flag() {\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Python, \"main.py\", b\"print('hi')\")\n            .with_summary(\"Prints a greeting.\").unwrap()\n            .encode()\n            .unwrap();\n\n        // Parse: skip the 8-byte header, read the first block frame.\n        let frame_buf = &payload[HEADER_SIZE..];\n        let (frame, _) = BlockFrame::read_from(frame_buf).unwrap().unwrap();\n        assert!(\n            frame.flags.has_summary(),\n            \"HAS_SUMMARY flag should be set on the code block\"\n        );\n    }\n\n    #[test]\n    fn with_priority_appends_annotation_block() {\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"lib.rs\", b\"// code\")\n            .with_priority(Priority::High).unwrap()\n            .encode()\n            .unwrap();\n\n        // Parse: header + first block (CODE) + second block (ANNOTATION) + END\n        let mut cursor = HEADER_SIZE;\n\n        // Block 0: CODE\n        let (frame0, n) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\n        assert_eq!(frame0.block_type, block_type::CODE);\n        cursor += n;\n\n        // Block 1: ANNOTATION (priority)\n        let (frame1, _) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\n        assert_eq!(frame1.block_type, block_type::ANNOTATION);\n\n        // Decode the annotation body and verify it targets block 0\n        let annotation = AnnotationBlock::decode_body(&frame1.body).unwrap();\n        assert_eq!(annotation.target_block_id, 0);\n        assert_eq!(annotation.kind, AnnotationKind::Priority);\n        assert_eq!(annotation.value, vec![Priority::High.to_wire_byte()]);\n    }\n\n    #[test]\n    fn empty_encoder_returns_empty_payload_error() {\n        let result = BcpEncoder::new().encode();\n        assert!(matches!(result, Err(EncodeError::EmptyPayload)));\n    }\n\n    #[test]\n    fn payload_ends_with_end_sentinel() {\n        let payload = BcpEncoder::new()\n            .add_conversation(Role::User, b\"hello\")\n            .encode()\n            .unwrap();\n\n        assert_ends_with_end_sentinel(&payload);\n    }\n\n    #[test]\n    fn all_eleven_block_types_encode_without_error() {\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .add_conversation(Role::User, b\"hello\")\n            .add_file_tree(\n                \"/project\",\n                vec![FileEntry {\n                    name: \"lib.rs\".to_string(),\n                    kind: FileEntryKind::File,\n                    size: 100,\n                    children: vec![],\n                }],\n            )\n            .add_tool_result(\"rg\", Status::Ok, b\"found 3 matches\")\n            .add_document(\"README\", b\"# Title\", FormatHint::Markdown)\n            .add_structured_data(DataFormat::Json, b\"{\\\"key\\\": \\\"value\\\"}\")\n            .add_diff(\n                \"src/lib.rs\",\n                vec![DiffHunk {\n                    old_start: 1,\n                    new_start: 1,\n                    lines: b\"+new line\\n\".to_vec(),\n                }],\n            )\n            .add_annotation(0, AnnotationKind::Tag, b\"important\")\n            .add_embedding_ref(b\"vec-001\", &[0xAB; 32], \"text-embedding-3-small\")\n            .add_image(MediaType::Png, \"screenshot\", b\"\\x89PNG\\r\\n\")\n            .add_extension(\"myco\", \"custom_block\", b\"custom data\")\n            .encode()\n            .unwrap();\n\n        assert_starts_with_magic(&payload);\n        assert_ends_with_end_sentinel(&payload);\n\n        // Verify we can walk all 12 content blocks (11 semantic + 1 annotation)\n        let mut cursor = HEADER_SIZE;\n        let mut block_count = 0;\n        loop {\n            match BlockFrame::read_from(&payload[cursor..]).unwrap() {\n                Some((_, n)) => {\n                    cursor += n;\n                    block_count += 1;\n                }\n                None => break, // END sentinel\n            }\n        }\n        assert_eq!(block_count, 11, \"expected 11 content blocks\");\n    }\n\n    #[test]\n    fn payload_byte_length_matches_calculation() {\n        let mut enc = BcpEncoder::new();\n        enc.add_code(Lang::Rust, \"x.rs\", b\"let x = 1;\");\n        enc.add_conversation(Role::User, b\"hi\");\n\n        let payload = enc.encode().unwrap();\n\n        // Calculate expected size manually:\n        // Header: 8 bytes\n        let mut expected = HEADER_SIZE;\n\n        // Walk actual frames to verify\n        let mut cursor = HEADER_SIZE;\n        loop {\n            let remaining = &payload[cursor..];\n            // Try to read a frame (including END which returns None)\n            let start = cursor;\n            match BlockFrame::read_from(remaining).unwrap() {\n                Some((_, n)) => {\n                    cursor += n;\n                    expected += n;\n                }\n                None => {\n                    // END sentinel was consumed — count those bytes too\n                    let end_bytes = payload.len() - start;\n                    expected += end_bytes;\n                    break;\n                }\n            }\n        }\n\n        assert_eq!(\n            payload.len(),\n            expected,\n            \"payload length should match header + frames + END\"\n        );\n    }\n\n    #[test]\n    fn optional_fields_omitted_when_none() {\n        // CODE block without line_range\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"x.rs\", b\"code\")\n            .encode()\n            .unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\n            .unwrap()\n            .unwrap();\n\n        // Decode the body and verify line_range is None\n        let code = CodeBlock::decode_body(&frame.body).unwrap();\n        assert!(code.line_range.is_none());\n\n        // CONVERSATION block without tool_call_id\n        let payload = BcpEncoder::new()\n            .add_conversation(Role::User, b\"msg\")\n            .encode()\n            .unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\n            .unwrap()\n            .unwrap();\n\n        let conv = ConversationBlock::decode_body(&frame.body).unwrap();\n        assert!(conv.tool_call_id.is_none());\n    }\n\n    #[test]\n    fn code_range_includes_line_numbers() {\n        let payload = BcpEncoder::new()\n            .add_code_range(Lang::Rust, \"src/lib.rs\", b\"fn foo() {}\", 10, 20)\n            .encode()\n            .unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\n            .unwrap()\n            .unwrap();\n\n        let code = CodeBlock::decode_body(&frame.body).unwrap();\n        assert_eq!(code.line_range, Some((10, 20)));\n    }\n\n    #[test]\n    fn conversation_tool_includes_tool_call_id() {\n        let payload = BcpEncoder::new()\n            .add_conversation_tool(Role::Tool, b\"result\", \"call_123\")\n            .encode()\n            .unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\n            .unwrap()\n            .unwrap();\n\n        let conv = ConversationBlock::decode_body(&frame.body).unwrap();\n        assert_eq!(conv.tool_call_id.as_deref(), Some(\"call_123\"));\n    }\n\n    #[test]\n    fn summary_is_decodable_from_block_body() {\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .with_summary(\"Entry point for the application.\").unwrap()\n            .encode()\n            .unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\n            .unwrap()\n            .unwrap();\n\n        assert!(frame.flags.has_summary());\n\n        // Decode summary from the front of the body\n        let (summary, consumed) = Summary::decode(&frame.body).unwrap();\n        assert_eq!(summary.text, \"Entry point for the application.\");\n\n        // Remaining bytes should decode as a valid CodeBlock\n        let code = CodeBlock::decode_body(&frame.body[consumed..]).unwrap();\n        assert_eq!(code.path, \"main.rs\");\n        assert_eq!(code.content, b\"fn main() {}\");\n    }\n\n    #[test]\n    fn rfc_example_encodes_successfully() {\n        // Reproduces the example from RFC §12.1 / SPEC_03 §1\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"src/main.rs\", b\"fn main() { todo!() }\")\n            .with_summary(\"Entry point: CLI setup and server startup.\").unwrap()\n            .with_priority(Priority::High).unwrap()\n            .add_conversation(Role::User, b\"Fix the timeout bug.\")\n            .add_conversation(Role::Assistant, b\"I'll examine the pool config...\")\n            .add_tool_result(\"ripgrep\", Status::Ok, b\"3 matches found.\")\n            .encode()\n            .unwrap();\n\n        assert_starts_with_magic(&payload);\n        assert_ends_with_end_sentinel(&payload);\n\n        // Walk all frames to verify structure\n        let mut cursor = HEADER_SIZE;\n        let mut types = Vec::new();\n        loop {\n            match BlockFrame::read_from(&payload[cursor..]).unwrap() {\n                Some((frame, n)) => {\n                    types.push(frame.block_type);\n                    cursor += n;\n                }\n                None => break,\n            }\n        }\n\n        assert_eq!(\n            types,\n            vec![\n                block_type::CODE,\n                block_type::ANNOTATION, // from with_priority\n                block_type::CONVERSATION,\n                block_type::CONVERSATION,\n                block_type::TOOL_RESULT,\n            ]\n        );\n    }\n\n    #[test]\n    fn default_impl_matches_new() {\n        let from_new = BcpEncoder::new();\n        let from_default = BcpEncoder::default();\n        assert!(from_new.blocks.is_empty());\n        assert!(from_default.blocks.is_empty());\n    }\n\n    // ── Per-block compression tests ─────────────────────────────────────\n\n    #[test]\n    fn per_block_compression_sets_compressed_flag() {\n        // Create a large, compressible block (exceeds COMPRESSION_THRESHOLD)\n        let big_content = \"fn main() { println!(\\\"hello world\\\"); }\\n\".repeat(50);\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"main.rs\", big_content.as_bytes())\n            .with_compression().unwrap()\n            .encode()\n            .unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\n            .unwrap()\n            .unwrap();\n        assert!(\n            frame.flags.is_compressed(),\n            \"COMPRESSED flag should be set on a large compressible block\"\n        );\n        assert!(\n            frame.body.len() < big_content.len(),\n            \"compressed body should be smaller than original\"\n        );\n    }\n\n    #[test]\n    fn small_block_not_compressed_even_when_requested() {\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"x.rs\", b\"let x = 1;\")\n            .with_compression().unwrap()\n            .encode()\n            .unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\n            .unwrap()\n            .unwrap();\n        assert!(\n            !frame.flags.is_compressed(),\n            \"small blocks should not be compressed (below threshold)\"\n        );\n    }\n\n    #[test]\n    fn compress_blocks_applies_to_all() {\n        let big_content = \"use std::io;\\n\".repeat(100);\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"a.rs\", big_content.as_bytes())\n            .add_code(Lang::Rust, \"b.rs\", big_content.as_bytes())\n            .compress_blocks()\n            .encode()\n            .unwrap();\n\n        let mut cursor = HEADER_SIZE;\n        for _ in 0..2 {\n            let (frame, n) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\n            assert!(\n                frame.flags.is_compressed(),\n                \"all blocks should be compressed with compress_blocks()\"\n            );\n            cursor += n;\n        }\n    }\n\n    // ── Whole-payload compression tests ─────────────────────────────────\n\n    #[test]\n    fn whole_payload_compression_sets_header_flag() {\n        let big_content = \"pub fn hello() -> &'static str { \\\"world\\\" }\\n\".repeat(100);\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"main.rs\", big_content.as_bytes())\n            .compress_payload()\n            .encode()\n            .unwrap();\n\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\n        assert!(\n            header.flags.is_compressed(),\n            \"header COMPRESSED flag should be set for whole-payload compression\"\n        );\n    }\n\n    #[test]\n    fn whole_payload_skips_per_block_compression() {\n        // When whole-payload compression is active, individual block\n        // COMPRESSED flags should NOT be set.\n        let big_content = \"pub fn hello() -> &'static str { \\\"world\\\" }\\n\".repeat(100);\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"main.rs\", big_content.as_bytes())\n            .with_compression().unwrap()\n            .compress_payload()\n            .encode()\n            .unwrap();\n\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\n        assert!(header.flags.is_compressed());\n\n        // Decompress the payload to check individual blocks\n        let decompressed =\n            crate::compression::decompress(&payload[HEADER_SIZE..], 16 * 1024 * 1024).unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&decompressed).unwrap().unwrap();\n        assert!(\n            !frame.flags.is_compressed(),\n            \"per-block COMPRESSED flag should not be set when whole-payload is active\"\n        );\n    }\n\n    #[test]\n    fn whole_payload_no_savings_stays_uncompressed() {\n        // Tiny payload — zstd overhead exceeds savings\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"x.rs\", b\"x\")\n            .compress_payload()\n            .encode()\n            .unwrap();\n\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\n        assert!(\n            !header.flags.is_compressed(),\n            \"header COMPRESSED flag should NOT be set when compression yields no savings\"\n        );\n    }\n\n    // ── Content addressing tests ────────────────────────────────────────\n\n    #[test]\n    fn content_addressing_sets_reference_flag() {\n        let store = Arc::new(crate::MemoryContentStore::new());\n        let payload = BcpEncoder::new()\n            .set_content_store(store.clone())\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .with_content_addressing().unwrap()\n            .encode()\n            .unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\n            .unwrap()\n            .unwrap();\n        assert!(\n            frame.flags.is_reference(),\n            \"IS_REFERENCE flag should be set on content-addressed block\"\n        );\n        assert_eq!(\n            frame.body.len(),\n            32,\n            \"reference block body should be exactly 32 bytes (BLAKE3 hash)\"\n        );\n\n        // The hash should resolve in the store\n        let hash: [u8; 32] = frame.body.try_into().unwrap();\n        assert!(store.contains(&hash));\n    }\n\n    #[test]\n    fn content_addressing_without_store_errors() {\n        let result = BcpEncoder::new()\n            .add_code(Lang::Rust, \"main.rs\", b\"fn main() {}\")\n            .with_content_addressing().unwrap()\n            .encode();\n\n        assert!(\n            matches!(result, Err(EncodeError::MissingContentStore)),\n            \"should error when content addressing is requested without a store\"\n        );\n    }\n\n    #[test]\n    fn auto_dedup_detects_duplicate_blocks() {\n        let store = Arc::new(crate::MemoryContentStore::new());\n        // Both blocks must have identical serialized TLV bodies (same\n        // path + content + lang) for auto-dedup to detect a duplicate.\n        let content = b\"fn main() {}\";\n\n        let payload = BcpEncoder::new()\n            .set_content_store(store.clone())\n            .auto_dedup()\n            .add_code(Lang::Rust, \"main.rs\", content)\n            .add_code(Lang::Rust, \"main.rs\", content) // identical TLV body\n            .encode()\n            .unwrap();\n\n        let mut cursor = HEADER_SIZE;\n\n        // First block: inline (first occurrence)\n        let (frame0, n) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\n        assert!(\n            !frame0.flags.is_reference(),\n            \"first occurrence should be stored inline\"\n        );\n        cursor += n;\n\n        // Second block: reference (duplicate)\n        let (frame1, _) = BlockFrame::read_from(&payload[cursor..]).unwrap().unwrap();\n        assert!(\n            frame1.flags.is_reference(),\n            \"duplicate should become a hash reference\"\n        );\n        assert_eq!(frame1.body.len(), 32);\n    }\n\n    #[test]\n    fn auto_dedup_without_store_errors() {\n        let result = BcpEncoder::new()\n            .auto_dedup()\n            .add_code(Lang::Rust, \"x.rs\", b\"code\")\n            .encode();\n\n        assert!(matches!(result, Err(EncodeError::MissingContentStore)));\n    }\n\n    #[test]\n    fn reference_block_not_per_block_compressed() {\n        // Content-addressed blocks produce 32-byte bodies which are\n        // below the compression threshold — verify no COMPRESSED flag.\n        let store = Arc::new(crate::MemoryContentStore::new());\n        let big_content = \"fn main() { println!(\\\"hello\\\"); }\\n\".repeat(50);\n        let payload = BcpEncoder::new()\n            .set_content_store(store)\n            .add_code(Lang::Rust, \"main.rs\", big_content.as_bytes())\n            .with_content_addressing().unwrap()\n            .with_compression().unwrap()\n            .encode()\n            .unwrap();\n\n        let (frame, _) = BlockFrame::read_from(&payload[HEADER_SIZE..])\n            .unwrap()\n            .unwrap();\n        assert!(frame.flags.is_reference());\n        assert!(\n            !frame.flags.is_compressed(),\n            \"reference blocks should not be per-block compressed\"\n        );\n    }\n\n    #[test]\n    fn content_addressing_with_whole_payload_compression() {\n        // Reference blocks CAN be wrapped in whole-payload compression.\n        let store = Arc::new(crate::MemoryContentStore::new());\n        // Same path + content = identical TLV body = single store entry\n        let content = \"fn main() { println!(\\\"hello\\\"); }\\n\".repeat(50);\n\n        let payload = BcpEncoder::new()\n            .set_content_store(store.clone())\n            .compress_payload()\n            .add_code(Lang::Rust, \"main.rs\", content.as_bytes())\n            .with_content_addressing().unwrap()\n            .add_code(Lang::Rust, \"main.rs\", content.as_bytes())\n            .with_content_addressing().unwrap()\n            .encode()\n            .unwrap();\n\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\n        // The payload might or might not be compressed (two 32-byte hashes\n        // plus framing may not compress well), but if it is, verify it's valid.\n        if header.flags.is_compressed() {\n            let decompressed =\n                crate::compression::decompress(&payload[HEADER_SIZE..], 16 * 1024 * 1024).unwrap();\n\n            let (frame, _) = BlockFrame::read_from(&decompressed).unwrap().unwrap();\n            assert!(frame.flags.is_reference());\n            assert_eq!(frame.body.len(), 32);\n        }\n\n        // Both blocks have identical TLV bodies → single store entry\n        assert_eq!(\n            store.len(),\n            1,\n            \"identical blocks should produce one store entry\"\n        );\n    }\n\n    // ── Phase 4: Cross-cutting tests ────────────────────────────────────\n\n    #[test]\n    fn compression_ratio_benchmark() {\n        // A realistic 50-line Rust file should compress by >= 20%.\n        let rust_code = r#\"use std::collections::HashMap;\nuse std::sync::Arc;\n\npub struct Config {\n    pub name: String,\n    pub values: HashMap<String, String>,\n    pub timeout: u64,\n}\n\nimpl Config {\n    pub fn new(name: &str) -> Self {\n        Self {\n            name: name.to_string(),\n            values: HashMap::new(),\n            timeout: 30,\n        }\n    }\n\n    pub fn set(&mut self, key: &str, value: &str) {\n        self.values.insert(key.to_string(), value.to_string());\n    }\n\n    pub fn get(&self, key: &str) -> Option<&String> {\n        self.values.get(key)\n    }\n\n    pub fn timeout(&self) -> u64 {\n        self.timeout\n    }\n}\n\nimpl Default for Config {\n    fn default() -> Self {\n        Self::new(\"default\")\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_new_config() {\n        let config = Config::new(\"test\");\n        assert_eq!(config.name, \"test\");\n        assert!(config.values.is_empty());\n        assert_eq!(config.timeout(), 30);\n    }\n\n    #[test]\n    fn test_set_and_get() {\n        let mut config = Config::new(\"test\");\n        config.set(\"key\", \"value\");\n        assert_eq!(config.get(\"key\"), Some(&\"value\".to_string()));\n    }\n}\n\"#;\n\n        let uncompressed_payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"config.rs\", rust_code.as_bytes())\n            .encode()\n            .unwrap();\n\n        let compressed_payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"config.rs\", rust_code.as_bytes())\n            .with_compression().unwrap()\n            .encode()\n            .unwrap();\n\n        let savings_pct =\n            100.0 * (1.0 - compressed_payload.len() as f64 / uncompressed_payload.len() as f64);\n\n        assert!(\n            savings_pct >= 20.0,\n            \"expected >= 20% compression savings on a 50-line Rust file, got {savings_pct:.1}%\"\n        );\n    }\n\n    #[test]\n    fn whole_payload_wins_over_per_block() {\n        // When both per-block and whole-payload compression are requested,\n        // only the header COMPRESSED flag should be set; individual blocks\n        // should NOT have their COMPRESSED flags set.\n        let big_content = \"pub fn process() -> Result<(), Error> { Ok(()) }\\n\".repeat(50);\n        let payload = BcpEncoder::new()\n            .add_code(Lang::Rust, \"a.rs\", big_content.as_bytes())\n            .with_compression().unwrap()\n            .add_code(Lang::Rust, \"b.rs\", big_content.as_bytes())\n            .with_compression().unwrap()\n            .compress_payload()\n            .encode()\n            .unwrap();\n\n        let header = BcpHeader::read_from(&payload[..HEADER_SIZE]).unwrap();\n        assert!(\n            header.flags.is_compressed(),\n            \"header should have COMPRESSED flag\"\n        );\n\n        // Decompress payload to inspect individual blocks\n        let decompressed =\n            crate::compression::decompress(&payload[HEADER_SIZE..], 16 * 1024 * 1024).unwrap();\n\n        let mut cursor = 0;\n        while let Some((frame, n)) = BlockFrame::read_from(&decompressed[cursor..]).unwrap() {\n            assert!(\n                !frame.flags.is_compressed(),\n                \"individual blocks should NOT be compressed when whole-payload is active\"\n            );\n            cursor += n;\n        }\n    }\n\n    #[test]\n    fn full_pipeline_encode_decode_roundtrip() {\n        // Exercises all features together: multiple block types,\n        // summaries, priorities, per-block compression, content\n        // addressing, and auto-dedup.\n        let store = Arc::new(crate::MemoryContentStore::new());\n        let big_code = \"fn compute() -> i64 { 42 }\\n\".repeat(50);\n\n        let payload = BcpEncoder::new()\n            .set_content_store(store.clone())\n            .auto_dedup()\n            .add_code(Lang::Rust, \"lib.rs\", big_code.as_bytes())\n            .with_summary(\"Core computation module.\").unwrap()\n            .with_compression().unwrap()\n            .add_code(Lang::Rust, \"lib.rs\", big_code.as_bytes()) // auto-dedup\n            .add_conversation(Role::User, b\"Review this code\")\n            .add_tool_result(\"clippy\", Status::Ok, b\"No warnings\")\n            .encode()\n            .unwrap();\n\n        // Decode with the same store\n        let decoded = bcp_decoder::BcpDecoder::decode_with_store(&payload, store.as_ref()).unwrap();\n\n        assert_eq!(decoded.blocks.len(), 4);\n        assert_eq!(decoded.blocks[0].block_type, bcp_types::BlockType::Code);\n        assert_eq!(\n            decoded.blocks[0].summary.as_ref().unwrap().text,\n            \"Core computation module.\"\n        );\n        assert_eq!(decoded.blocks[1].block_type, bcp_types::BlockType::Code);\n        assert_eq!(\n            decoded.blocks[2].block_type,\n            bcp_types::BlockType::Conversation\n        );\n        assert_eq!(\n            decoded.blocks[3].block_type,\n            bcp_types::BlockType::ToolResult\n        );\n\n        // Both code blocks should have the same content\n        for block in &decoded.blocks[..2] {\n            match &block.content {\n                BlockContent::Code(code) => {\n                    assert_eq!(code.content, big_code.as_bytes());\n                }\n                other => panic!(\"expected Code, got {other:?}\"),\n            }\n        }\n    }\n}\n",
      "language": "rust",
      "path": "encoder.rs",
      "type": "code"
    },
    {
      "content": "use bcp_wire::WireError;\n\n/// Errors specific to zstd compression and decompression.\n///\n/// These are surfaced when per-block or whole-payload compression\n/// is enabled and the zstd codec encounters an issue, or when\n/// decompressed output exceeds safety limits.\n///\n/// ```text\n///   CompressionError\n///   ├── CompressFailed      ← zstd encoder returned an error\n///   ├── DecompressFailed    ← zstd decoder returned an error\n///   └── DecompressionBomb   ← decompressed size exceeds safety limit\n/// ```\n#[derive(Debug, thiserror::Error)]\npub enum CompressionError {\n    #[error(\"zstd compression failed: {0}\")]\n    CompressFailed(String),\n\n    #[error(\"zstd decompression failed: {0}\")]\n    DecompressFailed(String),\n\n    #[error(\"decompressed size {actual} exceeds limit {limit}\")]\n    DecompressionBomb { actual: usize, limit: usize },\n}\n\n/// Errors that can occur during BCP payload encoding.\n///\n/// The encoder validates structural constraints (non-empty payload,\n/// block size limits, summary targeting) and propagates lower-level\n/// wire, I/O, and compression errors from the serialization layer.\n///\n/// Error hierarchy:\n///\n/// ```text\n///   EncodeError\n///   ├── EmptyPayload         ← no blocks were added before .encode()\n///   ├── BlockTooLarge        ← single block body exceeds size limit\n///   ├── NoBlockTarget        ← modifier called with no preceding block\n///   ├── MissingContentStore  ← content addressing enabled without a store\n///   ├── Compression(…)       ← from zstd compress/decompress\n///   ├── Wire(WireError)      ← from bcp-wire serialization\n///   └── Io(std::io::Error)   ← from underlying I/O writes\n/// ```\n#[derive(Debug, thiserror::Error)]\npub enum EncodeError {\n    #[error(\"no blocks have been added to the encoder\")]\n    EmptyPayload,\n\n    #[error(\"block body exceeds maximum size ({size} bytes, limit {limit})\")]\n    BlockTooLarge { size: usize, limit: usize },\n\n    #[error(\"{method} called but no blocks have been added yet\")]\n    NoBlockTarget { method: &'static str },\n\n    #[error(\"content addressing requires a content store (call set_content_store first)\")]\n    MissingContentStore,\n\n    #[error(transparent)]\n    Compression(#[from] CompressionError),\n\n    #[error(transparent)]\n    Wire(#[from] WireError),\n\n    #[error(transparent)]\n    Io(#[from] std::io::Error),\n}\n",
      "language": "rust",
      "path": "error.rs",
      "type": "code"
    },
    {
      "content": "I've loaded 5 source files from ./crates/bcp-encoder/src. Can you analyze the architecture?",
      "role": "user",
      "type": "conversation"
    },
    {
      "content": "I'll review the 5 files. Let me start by understanding the module structure and key types.",
      "role": "assistant",
      "type": "conversation"
    },
    {
      "content": "Found 5 source files in ./crates/bcp-encoder/src",
      "status": "ok",
      "tool_name": "ripgrep",
      "type": "tool_result"
    }
  ],
  "description": "Dir scan of ./crates/bcp-encoder/src (5 files)"
}